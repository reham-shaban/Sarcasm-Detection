{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBjD9s-bbJBB",
    "outputId": "fef5b7c8-0a60-469d-8c39-8e665803332f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('clean_data/features.npy')\n",
    "labels = np.load('clean_data/labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCcQlfFIQ8hv"
   },
   "source": [
    "## Spilt Train, Cross Validation and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "tEuJwRv5Q9ao"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33195, 33)\n",
      "(11065, 33)\n",
      "(11065, 33)\n",
      "(33195,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test_full, y_train, y_test_full = train_test_split(features, labels, train_size=0.6, random_state=1)\n",
    "x_test, x_cv, y_test, y_cv = train_test_split(x_test_full, y_test_full, train_size=0.5, random_state=1)\n",
    "print(x_train.shape)\n",
    "print(x_cv.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propogation, R  Back Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z): \n",
    "    return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y,vocabulary_size=21361, nodes_in_layer1=60, nodes_in_layer2=30, nodes_in_layer3=1, l_rate=0.1):\n",
    "        embedding_dim =50\n",
    "        self.x=x\n",
    "        vocab_size = vocabulary_size +1 # Add 1 for the out-of-vocabulary token\n",
    "        self.embedding_weights = np.random.randn(vocab_size, embedding_dim)\n",
    "        # define x, y\n",
    "        self.inputs_in_layer0 =self.embedding(self.x)\n",
    "        self.y = y.reshape(-1, 1)  # reshape y to be a column vector\n",
    "        \n",
    "        self.l_rate = l_rate  # learning rate\n",
    "        \n",
    "        # define and set the number of neurons in each layer\n",
    "        self.nodes_in_layer1 = nodes_in_layer1\n",
    "        self.nodes_in_layer2 = nodes_in_layer2\n",
    "        self.nodes_in_layer3 = nodes_in_layer3\n",
    "        \n",
    "        # initialize the weights (theta) matrices\n",
    "        self.thetas_layer0 = np.random.rand(self.inputs_in_layer0.shape[1] + 1, self.nodes_in_layer1) \n",
    "        self.thetas_layer1 = np.random.rand(self.nodes_in_layer1 + 1, self.nodes_in_layer2) \n",
    "        self.thetas_layer2 = np.random.rand(self.nodes_in_layer2 + 1, self.nodes_in_layer3)  \n",
    "    def feedforward(self):      \n",
    "        #compute all the nodes (a1, a2, a3, a4) in layer1\n",
    "        n = self.inputs_in_layer0.shape[0]\n",
    "\n",
    "        self.Z1 = self.thetas_layer0[0] + np.dot(self.inputs_in_layer0, self.thetas_layer0[1:])\n",
    "        self.layer1 = sigmoid(self.Z1)  #values of a1, a2, a3, a4 in layer 1\n",
    "        \n",
    "        #compute all the nodes (a1, a2, a3) in layer2\n",
    "        self.Z2 = self.thetas_layer1[0] + np.dot(self.layer1, self.thetas_layer1[1:])\n",
    "        self.layer2 = sigmoid(self.Z2)  #values of a1, a2, a3 in layer 2\n",
    "        \n",
    "        #compute all the nodes (a1) in layer3\n",
    "        self.Z3 = self.thetas_layer2[0] + np.dot(self.layer2, self.thetas_layer2[1:])\n",
    "        self.layer3 = sigmoid(self.Z3)  #output layer      \n",
    "        \n",
    "        return self.layer3\n",
    "    \n",
    "    def cost_func(self):\n",
    "        self.n = self.inputs_in_layer0.shape[0] #number of training examples\n",
    "        self.cost = (1/self.n) * np.sum(-self.y * np.log(self.layer3) - (1 - self.y) * np.log(1 - self.layer3)) #cross entropy\n",
    "        return self.cost \n",
    "    def embedding(self, x):\n",
    "            self.embedded_input = self.embedding_weights[x]\n",
    "            pooled_embeddings = np.mean(self.embedded_input, axis=1)\n",
    "            return pooled_embeddings\n",
    "    def calculate_accuracy(self ):\n",
    "        actual_output=self.y\n",
    "        predicted_output=self.layer3\n",
    "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
    "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
    "\n",
    "        # Compare predicted classes with actual classes\n",
    "        correct_predictions = (predicted_classes == actual_output).sum()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = correct_predictions / len(actual_output)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def Rbackprop(self):\n",
    "        # Define RProp parameters\n",
    "        delta0 = 0.01  # Initial update value\n",
    "        delta_max = 20  # Maximum update value\n",
    "        delta_min = 1e-6  # Minimum update value\n",
    "        eta_plus = 1.5  # Increase factor\n",
    "        eta_minus = 0.5  # Decrease factor\n",
    "\n",
    "        # dervative of E with respect to theta and bias in layer2\n",
    "        self.dE_dlayer3 = (1 / self.n) * (self.layer3 - self.y) / (self.layer3 * (1 - self.layer3))\n",
    "        self.dE_dZ3 = np.multiply(self.dE_dlayer3, sigmoid(self.Z3) * (1 - sigmoid(self.Z3)))\n",
    "        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n",
    "        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n",
    "\n",
    "        # dervative of E with respect to theta and bias in layer1\n",
    "        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n",
    "        self.dE_dZ2 = np.multiply(self.dE_dlayer2, sigmoid(self.Z2) * (1 - sigmoid(self.Z2)))\n",
    "        # self.dE_dZ1 = np.multiply(self.dE_dlayer2, dervative_relu(self.Z2))\n",
    "        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n",
    "        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n",
    "\n",
    "        # dervative of E with respect to theta and bias in layer0\n",
    "        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n",
    "        self.dE_dZ1 = np.multiply(self.dE_dlayer1, sigmoid(self.Z1) * (1 - sigmoid(self.Z1)))\n",
    "        # self.dE_dZ1 = np.multiply(self.dE_dlayer1, dervative_relu(self.Z1))\n",
    "        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n",
    "        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n",
    "\n",
    "        # Initialize RProp update values\n",
    "        if not hasattr(self, 'prev_dE_dtheta2'):\n",
    "            self.prev_dE_dtheta2 = np.zeros_like(self.dE_dtheta2)\n",
    "            self.delta_theta2 = np.full_like(self.dE_dtheta2, delta0)\n",
    "        else:\n",
    "            self.delta_theta2 = np.where(self.dE_dtheta2 * self.prev_dE_dtheta2 > 0,\n",
    "                                        np.minimum(self.delta_theta2 * eta_plus, delta_max),\n",
    "                                        np.maximum(self.delta_theta2 * eta_minus, delta_min))\n",
    "        self.prev_dE_dtheta2 = self.dE_dtheta2\n",
    "\n",
    "        if not hasattr(self, 'prev_dE_dtheta1'):\n",
    "            self.prev_dE_dtheta1 = np.zeros_like(self.dE_dtheta1)\n",
    "            self.delta_theta1 = np.full_like(self.dE_dtheta1, delta0)\n",
    "        else:\n",
    "            self.delta_theta1 = np.where(self.dE_dtheta1 * self.prev_dE_dtheta1 > 0,\n",
    "                                        np.minimum(self.delta_theta1 * eta_plus, delta_max),\n",
    "                                        np.maximum(self.delta_theta1 * eta_minus, delta_min))\n",
    "        self.prev_dE_dtheta1 = self.dE_dtheta1\n",
    "\n",
    "        if not hasattr(self, 'prev_dE_dtheta0'):\n",
    "            self.prev_dE_dtheta0 = np.zeros_like(self.dE_dtheta0)\n",
    "            self.delta_theta0 = np.full_like(self.dE_dtheta0, delta0)\n",
    "        else:\n",
    "            self.delta_theta0 = np.where(self.dE_dtheta0 * self.prev_dE_dtheta0 > 0,\n",
    "                                        np.minimum(self.delta_theta0 * eta_plus, delta_max),\n",
    "                                        np.maximum(self.delta_theta0 * eta_minus, delta_min))\n",
    "        self.prev_dE_dtheta0 = self.dE_dtheta0\n",
    "\n",
    "        # Updating theta using RProp in layers 2, 1, and 0\n",
    "        self.thetas_layer2[1:] -= np.sign(self.dE_dtheta2) * self.delta_theta2\n",
    "        self.thetas_layer1[1:] -= np.sign(self.dE_dtheta1) * self.delta_theta1\n",
    "        self.thetas_layer0[1:] -= np.sign(self.dE_dtheta0) * self.delta_theta0\n",
    "\n",
    "        # Updating bias using RProp in layers 2, 1, and 0\n",
    "        self.thetas_layer2[0] -= np.sign(self.dE_dbias2) * self.delta_theta2[0]\n",
    "        self.thetas_layer1[0] -= np.sign(self.dE_dbias1) * self.delta_theta1[0]\n",
    "        self.thetas_layer0[0] -= np.sign(self.dE_dbias0) * self.delta_theta0[0]\n",
    "        return self \n",
    "    \n",
    "    def backprop(self):\n",
    "        #dervative of E with respect to theta and bias in layer2\n",
    "        self.dE_dlayer3 = (1/self.n) * (self.layer3-self.y)/(self.layer3*(1-self.layer3))\n",
    "        self.dE_dZ3 = np.multiply(self.dE_dlayer3, (sigmoid(self.Z3)* (1-sigmoid(self.Z3))))\n",
    "        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n",
    "        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n",
    "        \n",
    "        #dervative of E with respect to theta and bias in layer1\n",
    "        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n",
    "        self.dE_dZ2 = np.multiply(self.dE_dlayer2, sigmoid(self.Z2)* (1-sigmoid(self.Z2)))\n",
    "        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n",
    "        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n",
    "        \n",
    "        #dervative of E with respect to theta and bias in layer0\n",
    "        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n",
    "        self.dE_dZ1 = np.multiply(self.dE_dlayer1, sigmoid(self.Z1)* (1-sigmoid(self.Z1)))\n",
    "        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n",
    "        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n",
    "        #updating theta using gradient descent in layers 2, 1, and 0\n",
    "        self.thetas_layer2[1:] = self.thetas_layer2[1:] - self.l_rate * self.dE_dtheta2\n",
    "        self.thetas_layer1[1:] = self.thetas_layer1[1:] - self.l_rate * self.dE_dtheta1\n",
    "        self.thetas_layer0[1:] = self.thetas_layer0[1:] - self.l_rate * self.dE_dtheta0\n",
    "        # self.de_wegihts = np.dot(self.embedded_input.T, self.dE_dZ1)\n",
    "        # self.embedding_weights = -self.embedding_weights - self.l_rate * self.de_wegihts\n",
    "\n",
    "        \n",
    "        #updating bias using gradient descent in layers 2, 1, and 0\n",
    "        self.thetas_layer2[0] = self.thetas_layer2[0] - self.l_rate * self.dE_dbias2\n",
    "        self.thetas_layer1[0] = self.thetas_layer1[0] - self.l_rate * self.dE_dbias1\n",
    "        self.thetas_layer0[0] = self.thetas_layer0[0] - self.l_rate * self.dE_dbias0\n",
    "        return self\n",
    "    def fit(self,epochs,Backpropagate):\n",
    "        losses=[]\n",
    "        for i in range(epochs):\n",
    "            self.feedforward()\n",
    "            error=self.cost_func()\n",
    "            losses.append(error)\n",
    "            if Backpropagate==True:\n",
    "                self.backprop()\n",
    "            else:\n",
    "                self.Rbackprop()\n",
    "            print(\"iteration #\",i+1)\n",
    "            print('accuracy: ',self.calculate_accuracy())\n",
    "            print(\"Cost: \\n\",error,\"\\n\")\n",
    "    def evaluate(self, x,y):\n",
    "        inputs_layer0 = self.embedding(x)\n",
    "        Z1 = self.thetas_layer0[0] + np.dot(inputs_layer0, self.thetas_layer0[1:])\n",
    "        layer1 = sigmoid(Z1)\n",
    "\n",
    "        Z2 = self.thetas_layer1[0] + np.dot(layer1, self.thetas_layer1[1:])\n",
    "        layer2 = sigmoid(Z2)\n",
    "\n",
    "        Z3 = self.thetas_layer2[0] + np.dot(layer2, self.thetas_layer2[1:])\n",
    "        layer3 = sigmoid(Z3)\n",
    "        loss= (1/inputs_layer0.shape[0]) * np.sum(-y * np.log(layer3) - (1 - y) * np.log(1 - layer3)) #cross entropy\n",
    "        actual_output=y\n",
    "        predicted_output=layer3\n",
    "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
    "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
    "\n",
    "        # Compare predicted classes with actual classes\n",
    "        correct_predictions = (predicted_classes == actual_output).sum()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = correct_predictions / len(actual_output)\n",
    "        return loss,accuracy,layer3          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 1\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 7.219087715414043 \n",
      "\n",
      "iteration # 2\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 6.302535098676623 \n",
      "\n",
      "iteration # 3\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 5.386030158871479 \n",
      "\n",
      "iteration # 4\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 4.469774396744802 \n",
      "\n",
      "iteration # 5\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 3.554853540012603 \n",
      "\n",
      "iteration # 6\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 2.64706626317619 \n",
      "\n",
      "iteration # 7\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.7760735513282853 \n",
      "\n",
      "iteration # 8\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.0649191795423756 \n",
      "\n",
      "iteration # 9\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7382407153018583 \n",
      "\n",
      "iteration # 10\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6920024432624904 \n",
      "\n",
      "iteration # 11\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6894312419396587 \n",
      "\n",
      "iteration # 12\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892984218946997 \n",
      "\n",
      "iteration # 13\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892913875175786 \n",
      "\n",
      "iteration # 14\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892910111583549 \n",
      "\n",
      "iteration # 15\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909901104864 \n",
      "\n",
      "iteration # 16\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909880620633 \n",
      "\n",
      "iteration # 17\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909870300393 \n",
      "\n",
      "iteration # 18\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909860523672 \n",
      "\n",
      "iteration # 19\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909850775735 \n",
      "\n",
      "iteration # 20\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909841029048 \n",
      "\n",
      "iteration # 21\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909831282132 \n",
      "\n",
      "iteration # 22\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909821534912 \n",
      "\n",
      "iteration # 23\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909811787381 \n",
      "\n",
      "iteration # 24\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909802039541 \n",
      "\n",
      "iteration # 25\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909792291388 \n",
      "\n",
      "iteration # 26\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909782542925 \n",
      "\n",
      "iteration # 27\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909772794152 \n",
      "\n",
      "iteration # 28\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909763045069 \n",
      "\n",
      "iteration # 29\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909753295675 \n",
      "\n",
      "iteration # 30\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689290974354597 \n",
      "\n",
      "iteration # 31\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909733795953 \n",
      "\n",
      "iteration # 32\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909724045628 \n",
      "\n",
      "iteration # 33\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909714294989 \n",
      "\n",
      "iteration # 34\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689290970454404 \n",
      "\n",
      "iteration # 35\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689290969479278 \n",
      "\n",
      "iteration # 36\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909685041207 \n",
      "\n",
      "iteration # 37\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909675289322 \n",
      "\n",
      "iteration # 38\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909665537127 \n",
      "\n",
      "iteration # 39\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689290965578462 \n",
      "\n",
      "iteration # 40\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909646031801 \n",
      "\n",
      "iteration # 41\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689290963627867 \n",
      "\n",
      "iteration # 42\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909626525227 \n",
      "\n",
      "iteration # 43\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909616771472 \n",
      "\n",
      "iteration # 44\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909607017405 \n",
      "\n",
      "iteration # 45\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909597263025 \n",
      "\n",
      "iteration # 46\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909587508333 \n",
      "\n",
      "iteration # 47\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689290957775333 \n",
      "\n",
      "iteration # 48\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909567998011 \n",
      "\n",
      "iteration # 49\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909558242382 \n",
      "\n",
      "iteration # 50\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909548486439 \n",
      "\n",
      "iteration # 51\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909538730185 \n",
      "\n",
      "iteration # 52\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909528973618 \n",
      "\n",
      "iteration # 53\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909519216737 \n",
      "\n",
      "iteration # 54\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909509459543 \n",
      "\n",
      "iteration # 55\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909499702035 \n",
      "\n",
      "iteration # 56\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909489944213 \n",
      "\n",
      "iteration # 57\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909480186078 \n",
      "\n",
      "iteration # 58\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909470427632 \n",
      "\n",
      "iteration # 59\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689290946066887 \n",
      "\n",
      "iteration # 60\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909450909795 \n",
      "\n",
      "iteration # 61\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909441150408 \n",
      "\n",
      "iteration # 62\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909431390705 \n",
      "\n",
      "iteration # 63\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909421630687 \n",
      "\n",
      "iteration # 64\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909411870357 \n",
      "\n",
      "iteration # 65\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909402109714 \n",
      "\n",
      "iteration # 66\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909392348754 \n",
      "\n",
      "iteration # 67\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909382587482 \n",
      "\n",
      "iteration # 68\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909372825893 \n",
      "\n",
      "iteration # 69\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909363063993 \n",
      "\n",
      "iteration # 70\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909353301774 \n",
      "\n",
      "iteration # 71\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909343539243 \n",
      "\n",
      "iteration # 72\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909333776397 \n",
      "\n",
      "iteration # 73\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909324013234 \n",
      "\n",
      "iteration # 74\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909314249759 \n",
      "\n",
      "iteration # 75\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909304485969 \n",
      "\n",
      "iteration # 76\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909294721863 \n",
      "\n",
      "iteration # 77\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689290928495744 \n",
      "\n",
      "iteration # 78\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909275192703 \n",
      "\n",
      "iteration # 79\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909265427651 \n",
      "\n",
      "iteration # 80\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909255662283 \n",
      "\n",
      "iteration # 81\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.68929092458966 \n",
      "\n",
      "iteration # 82\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.68929092361306 \n",
      "\n",
      "iteration # 83\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909226364285 \n",
      "\n",
      "iteration # 84\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909216597655 \n",
      "\n",
      "iteration # 85\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909206830707 \n",
      "\n",
      "iteration # 86\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909197063444 \n",
      "\n",
      "iteration # 87\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909187295864 \n",
      "\n",
      "iteration # 88\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909177527967 \n",
      "\n",
      "iteration # 89\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909167759756 \n",
      "\n",
      "iteration # 90\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909157991229 \n",
      "\n",
      "iteration # 91\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909148222384 \n",
      "\n",
      "iteration # 92\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909138453223 \n",
      "\n",
      "iteration # 93\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909128683743 \n",
      "\n",
      "iteration # 94\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909118913948 \n",
      "\n",
      "iteration # 95\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909109143837 \n",
      "\n",
      "iteration # 96\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909099373407 \n",
      "\n",
      "iteration # 97\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689290908960266 \n",
      "\n",
      "iteration # 98\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909079831595 \n",
      "\n",
      "iteration # 99\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909070060215 \n",
      "\n",
      "iteration # 100\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892909060288517 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn=NeuralNetwork(x_train,y_train)\n",
    "nn.fit(epochs=100,Backpropagate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.6905621394269598\n",
      "Acc : 0.5366470854044284\n"
     ]
    }
   ],
   "source": [
    "test=nn.evaluate(x_cv,y_cv.reshape(-1,1))\n",
    "print('Loss :',test[0])\n",
    "print('Acc :',test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.6899574235317435\n",
      "Acc : 0.540081337550836\n"
     ]
    }
   ],
   "source": [
    "Test=nn.evaluate(x_test,y_test.reshape(-1,1))\n",
    "print('Loss :',Test[0])\n",
    "print('Acc :',Test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 1\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 8.02056278901174 \n",
      "\n",
      "iteration # 2\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 7.76371028061254 \n",
      "\n",
      "iteration # 3\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 7.28279283794215 \n",
      "\n",
      "iteration # 4\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 6.378419936982094 \n",
      "\n",
      "iteration # 5\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 5.031855074525351 \n",
      "\n",
      "iteration # 6\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 3.7442144790670175 \n",
      "\n",
      "iteration # 7\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 2.6975413658964267 \n",
      "\n",
      "iteration # 8\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.533843501897184 \n",
      "\n",
      "iteration # 9\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6952624809154464 \n",
      "\n",
      "iteration # 10\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.8932826332841098 \n",
      "\n",
      "iteration # 11\n",
      "accuracy:  0.4560626600391625\n",
      "Cost: \n",
      " 0.7126756155431136 \n",
      "\n",
      "iteration # 12\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7768232774196911 \n",
      "\n",
      "iteration # 13\n",
      "accuracy:  0.548666967916855\n",
      "Cost: \n",
      " 0.6883778256845222 \n",
      "\n",
      "iteration # 14\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.89585395752362 \n",
      "\n",
      "iteration # 15\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7051233710087017 \n",
      "\n",
      "iteration # 16\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7948351732335752 \n",
      "\n",
      "iteration # 17\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6889194708556254 \n",
      "\n",
      "iteration # 18\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7704641246377262 \n",
      "\n",
      "iteration # 19\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7034727487783039 \n",
      "\n",
      "iteration # 20\n",
      "accuracy:  0.5439373399608375\n",
      "Cost: \n",
      " 0.6896211719059372 \n",
      "\n",
      "iteration # 21\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7527767896317131 \n",
      "\n",
      "iteration # 22\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7958574681865459 \n",
      "\n",
      "iteration # 23\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7815530778032869 \n",
      "\n",
      "iteration # 24\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7473927011943313 \n",
      "\n",
      "iteration # 25\n",
      "accuracy:  0.5001656876035547\n",
      "Cost: \n",
      " 0.7101683524148628 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_23140\\4003567598.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-Z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 26\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 1.2197521163421619 \n",
      "\n",
      "iteration # 27\n",
      "accuracy:  0.5427925892453683\n",
      "Cost: \n",
      " 0.6974427081861422 \n",
      "\n",
      "iteration # 28\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.8113514283535478 \n",
      "\n",
      "iteration # 29\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7699235952029498 \n",
      "\n",
      "iteration # 30\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.9640498224001851 \n",
      "\n",
      "iteration # 31\n",
      "accuracy:  0.5439674649796656\n",
      "Cost: \n",
      " 0.8402194942969355 \n",
      "\n",
      "iteration # 32\n",
      "accuracy:  0.45609278505799067\n",
      "Cost: \n",
      " 0.6976372064994109 \n",
      "\n",
      "iteration # 33\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.702001646643478 \n",
      "\n",
      "iteration # 34\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.9255961000308928 \n",
      "\n",
      "iteration # 35\n",
      "accuracy:  0.5462569664106041\n",
      "Cost: \n",
      " 0.6898086333745131 \n",
      "\n",
      "iteration # 36\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 1.5157409741250725 \n",
      "\n",
      "iteration # 37\n",
      "accuracy:  0.5439975899984938\n",
      "Cost: \n",
      " 0.9275988777002518 \n",
      "\n",
      "iteration # 38\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.8917900033872441 \n",
      "\n",
      "iteration # 39\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7870209693113523 \n",
      "\n",
      "iteration # 40\n",
      "accuracy:  0.5444494652809158\n",
      "Cost: \n",
      " 0.7972866296506552 \n",
      "\n",
      "iteration # 41\n",
      "accuracy:  0.4561530350956469\n",
      "Cost: \n",
      " 0.7147387068804563 \n",
      "\n",
      "iteration # 42\n",
      "accuracy:  0.5378219611387257\n",
      "Cost: \n",
      " 0.6901846733448692 \n",
      "\n",
      "iteration # 43\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7098680348185497 \n",
      "\n",
      "iteration # 44\n",
      "accuracy:  0.5441180900738063\n",
      "Cost: \n",
      " 0.6888038938937862 \n",
      "\n",
      "iteration # 45\n",
      "accuracy:  0.4561530350956469\n",
      "Cost: \n",
      " 0.7126850436781147 \n",
      "\n",
      "iteration # 46\n",
      "accuracy:  0.4561530350956469\n",
      "Cost: \n",
      " 0.714852402620974 \n",
      "\n",
      "iteration # 47\n",
      "accuracy:  0.4726012953758096\n",
      "Cost: \n",
      " 0.698568620862517 \n",
      "\n",
      "iteration # 48\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7625242371853418 \n",
      "\n",
      "iteration # 49\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6937144458154281 \n",
      "\n",
      "iteration # 50\n",
      "accuracy:  0.4561530350956469\n",
      "Cost: \n",
      " 0.7203858728584984 \n",
      "\n",
      "iteration # 51\n",
      "accuracy:  0.5605061003163126\n",
      "Cost: \n",
      " 0.687528292386455 \n",
      "\n",
      "iteration # 52\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7042645095327821 \n",
      "\n",
      "iteration # 53\n",
      "accuracy:  0.5440277150173218\n",
      "Cost: \n",
      " 0.6859765876340357 \n",
      "\n",
      "iteration # 54\n",
      "accuracy:  0.5322488326555204\n",
      "Cost: \n",
      " 0.6908825247211925 \n",
      "\n",
      "iteration # 55\n",
      "accuracy:  0.5562584726615454\n",
      "Cost: \n",
      " 0.6864569014397153 \n",
      "\n",
      "iteration # 56\n",
      "accuracy:  0.5596625997891249\n",
      "Cost: \n",
      " 0.6861993283027181 \n",
      "\n",
      "iteration # 57\n",
      "accuracy:  0.45934628709142944\n",
      "Cost: \n",
      " 0.716182205767487 \n",
      "\n",
      "iteration # 58\n",
      "accuracy:  0.5433950896219311\n",
      "Cost: \n",
      " 0.6884788151946882 \n",
      "\n",
      "iteration # 59\n",
      "accuracy:  0.5549329718331074\n",
      "Cost: \n",
      " 0.687129893933879 \n",
      "\n",
      "iteration # 60\n",
      "accuracy:  0.5628257267660792\n",
      "Cost: \n",
      " 0.683871918420356 \n",
      "\n",
      "iteration # 61\n",
      "accuracy:  0.5555957222473265\n",
      "Cost: \n",
      " 0.6845243959592838 \n",
      "\n",
      "iteration # 62\n",
      "accuracy:  0.5662298538936587\n",
      "Cost: \n",
      " 0.6820549524509505 \n",
      "\n",
      "iteration # 63\n",
      "accuracy:  0.562102726314204\n",
      "Cost: \n",
      " 0.6822683829328663 \n",
      "\n",
      "iteration # 64\n",
      "accuracy:  0.45880403675252296\n",
      "Cost: \n",
      " 0.7165214271195561 \n",
      "\n",
      "iteration # 65\n",
      "accuracy:  0.5553848471155295\n",
      "Cost: \n",
      " 0.6834799036538473 \n",
      "\n",
      "iteration # 66\n",
      "accuracy:  0.5689109805693628\n",
      "Cost: \n",
      " 0.6803051424289029 \n",
      "\n",
      "iteration # 67\n",
      "accuracy:  0.5696339810212382\n",
      "Cost: \n",
      " 0.6800086880647226 \n",
      "\n",
      "iteration # 68\n",
      "accuracy:  0.5708389817743637\n",
      "Cost: \n",
      " 0.6791683238905347 \n",
      "\n",
      "iteration # 69\n",
      "accuracy:  0.5746347341467088\n",
      "Cost: \n",
      " 0.6788456408135785 \n",
      "\n",
      "iteration # 70\n",
      "accuracy:  0.4570266606416629\n",
      "Cost: \n",
      " 0.7169584799609163 \n",
      "\n",
      "iteration # 71\n",
      "accuracy:  0.5593613496008435\n",
      "Cost: \n",
      " 0.6829938628939204 \n",
      "\n",
      "iteration # 72\n",
      "accuracy:  0.5760807350504594\n",
      "Cost: \n",
      " 0.6777145078468272 \n",
      "\n",
      "iteration # 73\n",
      "accuracy:  0.5772254857659286\n",
      "Cost: \n",
      " 0.677723467719605 \n",
      "\n",
      "iteration # 74\n",
      "accuracy:  0.5328513330320831\n",
      "Cost: \n",
      " 0.6920311305426524 \n",
      "\n",
      "iteration # 75\n",
      "accuracy:  0.5724054827534267\n",
      "Cost: \n",
      " 0.6779540865051117 \n",
      "\n",
      "iteration # 76\n",
      "accuracy:  0.5827684892303058\n",
      "Cost: \n",
      " 0.6786192984181819 \n",
      "\n",
      "iteration # 77\n",
      "accuracy:  0.5822563639102274\n",
      "Cost: \n",
      " 0.6758987364938672 \n",
      "\n",
      "iteration # 78\n",
      "accuracy:  0.5853592408495255\n",
      "Cost: \n",
      " 0.6751214716082201 \n",
      "\n",
      "iteration # 79\n",
      "accuracy:  0.5864136165085103\n",
      "Cost: \n",
      " 0.6746764258413831 \n",
      "\n",
      "iteration # 80\n",
      "accuracy:  0.5877692423557764\n",
      "Cost: \n",
      " 0.6738998126340995 \n",
      "\n",
      "iteration # 81\n",
      "accuracy:  0.5899382437114024\n",
      "Cost: \n",
      " 0.6732085906685452 \n",
      "\n",
      "iteration # 82\n",
      "accuracy:  0.5901792438620274\n",
      "Cost: \n",
      " 0.6728382208076038 \n",
      "\n",
      "iteration # 83\n",
      "accuracy:  0.5922578701611688\n",
      "Cost: \n",
      " 0.6723633865285887 \n",
      "\n",
      "iteration # 84\n",
      "accuracy:  0.5586082241301401\n",
      "Cost: \n",
      " 0.6832697561355208 \n",
      "\n",
      "iteration # 85\n",
      "accuracy:  0.5899382437114024\n",
      "Cost: \n",
      " 0.6725637090773349 \n",
      "\n",
      "iteration # 86\n",
      "accuracy:  0.5949992468745293\n",
      "Cost: \n",
      " 0.6715576463826667 \n",
      "\n",
      "iteration # 87\n",
      "accuracy:  0.5959632474770297\n",
      "Cost: \n",
      " 0.6704916067060108 \n",
      "\n",
      "iteration # 88\n",
      "accuracy:  0.5899683687302305\n",
      "Cost: \n",
      " 0.6732320654131565 \n",
      "\n",
      "iteration # 89\n",
      "accuracy:  0.5883416177135111\n",
      "Cost: \n",
      " 0.6720404542116901 \n",
      "\n",
      "iteration # 90\n",
      "accuracy:  0.5946979966862479\n",
      "Cost: \n",
      " 0.6699195182988594 \n",
      "\n",
      "iteration # 91\n",
      "accuracy:  0.5956318722699202\n",
      "Cost: \n",
      " 0.827203105338527 \n",
      "\n",
      "iteration # 92\n",
      "accuracy:  0.5982828739267962\n",
      "Cost: \n",
      " 0.67718994937906 \n",
      "\n",
      "iteration # 93\n",
      "accuracy:  0.5945172465732791\n",
      "Cost: \n",
      " 0.6766663791965297 \n",
      "\n",
      "iteration # 94\n",
      "accuracy:  0.5813526133453834\n",
      "Cost: \n",
      " 0.6799284738806189 \n",
      "\n",
      "iteration # 95\n",
      "accuracy:  0.5947582467239042\n",
      "Cost: \n",
      " 0.671916055088703 \n",
      "\n",
      "iteration # 96\n",
      "accuracy:  0.5983431239644524\n",
      "Cost: \n",
      " 0.668593475136505 \n",
      "\n",
      "iteration # 97\n",
      "accuracy:  0.5980719987949993\n",
      "Cost: \n",
      " 0.6679475504620191 \n",
      "\n",
      "iteration # 98\n",
      "accuracy:  0.5965054978159361\n",
      "Cost: \n",
      " 0.6676285819397132 \n",
      "\n",
      "iteration # 99\n",
      "accuracy:  0.5987046241903902\n",
      "Cost: \n",
      " 0.6673733036328147 \n",
      "\n",
      "iteration # 100\n",
      "accuracy:  0.5983129989456243\n",
      "Cost: \n",
      " 0.6668531658388807 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn1=NeuralNetwork(x_train,y_train)\n",
    "nn1.fit(epochs=100,Backpropagate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.6715898270471344\n",
      "Acc : 0.5906009941256213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_23140\\4003567598.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-Z))\n"
     ]
    }
   ],
   "source": [
    "Test_1=nn1.evaluate(x_cv,y_cv.reshape(-1,1))\n",
    "print('Loss :',Test_1[0])\n",
    "print('Acc :',Test_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.6712509884497972\n",
      "Acc : 0.5923181201988251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_23140\\4003567598.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-Z))\n"
     ]
    }
   ],
   "source": [
    "Test_2=nn1.evaluate(x_test,y_test.reshape(-1,1))\n",
    "print('Loss :',Test_2[0])\n",
    "print('Acc :',Test_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
