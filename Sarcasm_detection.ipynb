{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MPad2BhaZdQ"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WBjD9s-bbJBB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ls2eRLAaZdY"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GPJkqtNmaZdY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((55325, 33), (55325,))"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = np.load('clean_data/features.npy')\n",
        "labels = np.load('clean_data/labels.npy')\n",
        "features.shape,labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCcQlfFIQ8hv"
      },
      "source": [
        "## Spilt Train, Cross Validation and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEuJwRv5Q9ao",
        "outputId": "15ca5805-e6db-49e9-b47a-c2e593e2d747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(33195, 33)\n",
            "(11065, 33)\n",
            "(11065, 33)\n",
            "(33195,)\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test_full, y_train, y_test_full = train_test_split(features, labels, train_size=0.6, random_state=1)\n",
        "x_test, x_cv, y_test, y_cv = train_test_split(x_test_full, y_test_full, train_size=0.5, random_state=1)\n",
        "print(x_train.shape)\n",
        "print(x_cv.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmWITO56aZdb"
      },
      "source": [
        "# Back Propogation, R  Back Propogation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz-t_T19aZdb"
      },
      "source": [
        "# Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "QkHnoDOPaZdb"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He1EtT6IaZdc"
      },
      "source": [
        "# Function to initialize weights as Gaussian distributions with specific mu and sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "9UaVnEnIaZdc"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(nodes_in, nodes_out, mu=0, sigma=0.1):\n",
        "    return np.random.normal(mu, sigma, size=(nodes_in, nodes_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkuPO21eaZdd"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "OxcIxptnaZdd"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, x, y,vocabulary_size=21361, nodes_in_layer1=32, nodes_in_layer2=16, nodes_in_layer3=1, l_rate=0.001):\n",
        "        embedding_dim = 50\n",
        "        self.x = x\n",
        "        vocab_size = vocabulary_size + 1  # Add 1 for the out-of-vocabulary token\n",
        "        self.embedding_weights = np.random.randn(vocab_size, embedding_dim)\n",
        "        self.inputs_in_layer0 = self.embedding(self.x)\n",
        "        self.y = y.reshape(-1, 1)  # reshape y to be a column vector\n",
        "\n",
        "        self.l_rate = l_rate  # learning rate\n",
        "\n",
        "        self.nodes_in_layer1 = nodes_in_layer1\n",
        "        self.nodes_in_layer2 = nodes_in_layer2\n",
        "        self.nodes_in_layer3 = nodes_in_layer3\n",
        "\n",
        "        self.thetas_layer0 = np.random.randn(self.inputs_in_layer0.shape[1] + 1, self.nodes_in_layer1) * np.sqrt(2 / (self.inputs_in_layer0.shape[1] + 1))\n",
        "        self.thetas_layer1 = np.random.randn(self.nodes_in_layer1 + 1, self.nodes_in_layer2) * np.sqrt(2 / (self.nodes_in_layer1 + 1))\n",
        "        self.thetas_layer2 = np.random.randn(self.nodes_in_layer2 + 1, self.nodes_in_layer3) * np.sqrt(2 / (self.nodes_in_layer2 + 1))\n",
        "\n",
        "        self.epsilon = 1e-5\n",
        "        self.momentum = 0.9\n",
        "        self.gamma1 = np.ones(nodes_in_layer1)\n",
        "        self.beta1 = np.zeros(nodes_in_layer1)\n",
        "        self.gamma2 = np.ones(nodes_in_layer2)\n",
        "        self.beta2 = np.zeros(nodes_in_layer2)\n",
        "\n",
        "\n",
        "    def batch_normalize(self, input_data, gamma, beta):\n",
        "        batch_mean = np.mean(input_data, axis=0)\n",
        "        batch_var = np.var(input_data, axis=0)\n",
        "        normalized_data = (input_data - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
        "        scaled_and_shifted_data = gamma * normalized_data + beta\n",
        "        return scaled_and_shifted_data\n",
        "\n",
        "    def feedforward(self):\n",
        "        self.n = self.inputs_in_layer0.shape[0]\n",
        "\n",
        "        self.Z1 = self.thetas_layer0[0] + np.dot(self.inputs_in_layer0, self.thetas_layer0[1:])\n",
        "        self.layer1 = relu(self.Z1)\n",
        "        self.normalized_data_layer1 = self.batch_normalize(self.layer1, self.gamma1, self.beta1)\n",
        "\n",
        "        self.Z2 = self.thetas_layer1[0] + np.dot(self.normalized_data_layer1, self.thetas_layer1[1:])\n",
        "        self.layer2 = relu(self.Z2)\n",
        "        self.normalized_data_layer2 = self.batch_normalize(self.layer2, self.gamma2, self.beta2)\n",
        "\n",
        "        self.Z3 = self.thetas_layer2[0] + np.dot(self.normalized_data_layer2, self.thetas_layer2[1:])\n",
        "        self.layer3 = sigmoid(self.Z3)\n",
        "\n",
        "        return self.layer3\n",
        "\n",
        "    def cost_func(self):\n",
        "        y_pred=self.layer3\n",
        "        y_true=self.y\n",
        "        epsilon = 1e-15  # small constant to avoid log(0)\n",
        "            \n",
        "        # Clip predicted probabilities to avoid log(0) and log(1)\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "        self.cost = - np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "            \n",
        "        return self.cost\n",
        "    def embedding(self, x):\n",
        "        self.embedded_input = self.embedding_weights[x]\n",
        "        pooled_embeddings = np.mean(self.embedded_input, axis=1)\n",
        "        return pooled_embeddings\n",
        "    def calculate_accuracy(self ):\n",
        "        actual_output=self.y\n",
        "        predicted_output=self.layer3\n",
        "\n",
        "        predicted_classes = (predicted_output >= 0.5 ).astype(int)\n",
        "\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "        return accuracy\n",
        "\n",
        "    def Rbackprop(self):\n",
        "        # Define RProp parameters\n",
        "        delta0 = self.l_rate  # Initial update value\n",
        "        delta_max = 60  # Maximum update value\n",
        "        delta_min = 1e-6  # Minimum update value\n",
        "        eta_plus = 1.5  # Increase factor\n",
        "        eta_minus = 0.5  # Decrease factor\n",
        "        epsilon = 1e-15\n",
        "\n",
        "        self.dE_dlayer3 = (1/self.n) * (self.layer3-self.y)/(self.layer3*(1-self.layer3)+epsilon)\n",
        "        self.dE_dZ3 = np.multiply(self.dE_dlayer3, (sigmoid(self.Z3)* (1-sigmoid(self.Z3))))\n",
        "        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n",
        "        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n",
        "\n",
        "        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n",
        "        self.dE_dZ2 = np.multiply(self.dE_dlayer2, relu_derivative(self.Z2))\n",
        "        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n",
        "        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n",
        "\n",
        "        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n",
        "        self.dE_dZ1 = np.multiply(self.dE_dlayer1, relu_derivative(self.Z1))\n",
        "        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n",
        "        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n",
        "\n",
        "\n",
        "        if not hasattr(self, 'prev_dE_dtheta2'):\n",
        "            self.prev_dE_dtheta2 = np.zeros_like(self.dE_dtheta2)\n",
        "            self.delta_theta2 = np.full_like(self.dE_dtheta2, delta0)\n",
        "        else:\n",
        "            self.delta_theta2 = np.where(self.dE_dtheta2 * self.prev_dE_dtheta2 > 0,\n",
        "                                        np.minimum(self.delta_theta2 * eta_plus, delta_max),\n",
        "                                        np.maximum(self.delta_theta2 * eta_minus, delta_min))\n",
        "        self.prev_dE_dtheta2 = self.dE_dtheta2\n",
        "\n",
        "        if not hasattr(self, 'prev_dE_dtheta1'):\n",
        "            self.prev_dE_dtheta1 = np.zeros_like(self.dE_dtheta1)\n",
        "            self.delta_theta1 = np.full_like(self.dE_dtheta1, delta0)\n",
        "        else:\n",
        "            self.delta_theta1 = np.where(self.dE_dtheta1 * self.prev_dE_dtheta1 > 0,\n",
        "                                        np.minimum(self.delta_theta1 * eta_plus, delta_max),\n",
        "                                        np.maximum(self.delta_theta1 * eta_minus, delta_min))\n",
        "        self.prev_dE_dtheta1 = self.dE_dtheta1\n",
        "\n",
        "        if not hasattr(self, 'prev_dE_dtheta0'):\n",
        "            self.prev_dE_dtheta0 = np.zeros_like(self.dE_dtheta0)\n",
        "            self.delta_theta0 = np.full_like(self.dE_dtheta0, delta0)\n",
        "        else:\n",
        "            self.delta_theta0 = np.where(self.dE_dtheta0 * self.prev_dE_dtheta0 > 0,\n",
        "                                        np.minimum(self.delta_theta0 * eta_plus, delta_max),\n",
        "                                        np.maximum(self.delta_theta0 * eta_minus, delta_min))\n",
        "        self.prev_dE_dtheta0 = self.dE_dtheta0\n",
        "\n",
        "        self.thetas_layer2[1:] -= np.sign(self.dE_dtheta2) * self.delta_theta2\n",
        "        self.thetas_layer1[1:] -= np.sign(self.dE_dtheta1) * self.delta_theta1\n",
        "        self.thetas_layer0[1:] -= np.sign(self.dE_dtheta0) * self.delta_theta0\n",
        "\n",
        "        self.thetas_layer2[0] -= np.sign(self.dE_dbias2) * self.delta_theta2[0]\n",
        "        self.thetas_layer1[0] -= np.sign(self.dE_dbias1) * self.delta_theta1[0]\n",
        "        self.thetas_layer0[0] -= np.sign(self.dE_dbias0) * self.delta_theta0[0]\n",
        "        return self\n",
        "\n",
        "    def backprop(self):\n",
        "        epsilon=1e-15\n",
        "\n",
        "        self.dE_dlayer3 = (1/self.n) * (self.layer3-self.y)/(self.layer3*(1-self.layer3)+epsilon)\n",
        "        self.dE_dZ3 = np.multiply(self.dE_dlayer3, (sigmoid(self.Z3)* (1-sigmoid(self.Z3))))\n",
        "        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n",
        "        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n",
        "\n",
        "        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n",
        "        self.dE_dZ2 = np.multiply(self.dE_dlayer2, relu_derivative(self.Z2))\n",
        "        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n",
        "        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n",
        "        # Gradient for batch normalization parameters in layer2\n",
        "        dL_dgamma2 = np.sum(self.dE_dZ2 * self.normalized_data_layer2, axis=0)\n",
        "        dL_dbeta2 = np.sum(self.dE_dZ2, axis=0)\n",
        "\n",
        "        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n",
        "        self.dE_dZ1 = np.multiply(self.dE_dlayer1, relu_derivative(self.Z1))\n",
        "        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n",
        "        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n",
        "        # Gradient for batch normalization parameters in layer1\n",
        "        dL_dgamma1 = np.sum(self.dE_dZ1 * self.normalized_data_layer1, axis=0)\n",
        "        dL_dbeta1 = np.sum(self.dE_dZ1, axis=0)\n",
        "\n",
        "        self.thetas_layer2[1:] = self.thetas_layer2[1:] - self.l_rate * self.dE_dtheta2\n",
        "        self.thetas_layer1[1:] = self.thetas_layer1[1:] - self.l_rate * self.dE_dtheta1\n",
        "        self.thetas_layer0[1:] = self.thetas_layer0[1:] - self.l_rate * self.dE_dtheta0\n",
        "\n",
        "        # self.de_wegihts = np.dot(self.embedded_input.T, self.dE_dZ1)\n",
        "        # self.embedding_weights = -self.embedding_weights - self.l_rate * self.de_wegihts\n",
        "        # Update batch normalization parameters using gradient descent\n",
        "        self.gamma1 -= self.l_rate * dL_dgamma1\n",
        "        self.beta1 -= self.l_rate * dL_dbeta1\n",
        "        self.gamma2 -= self.l_rate * dL_dgamma2\n",
        "        self.beta2 -= self.l_rate * dL_dbeta2\n",
        "\n",
        "        #updating bias using gradient descent in layers 2, 1, and 0\n",
        "        self.thetas_layer2[0] = self.thetas_layer2[0] - self.l_rate * self.dE_dbias2\n",
        "        self.thetas_layer1[0] = self.thetas_layer1[0] - self.l_rate * self.dE_dbias1\n",
        "        self.thetas_layer0[0] = self.thetas_layer0[0] - self.l_rate * self.dE_dbias0\n",
        "        return self\n",
        "    \n",
        "    def fit(self,epochs,Backpropagate):\n",
        "        losses=[]\n",
        "        for i in range(epochs):\n",
        "            self.feedforward()\n",
        "            error=self.cost_func()\n",
        "            losses.append(error)\n",
        "            if Backpropagate==True:\n",
        "                self.backprop()\n",
        "            else:\n",
        "                self.Rbackprop()\n",
        "            print(\"iteration #\",i+1)\n",
        "            print('accuracy: ',self.calculate_accuracy())\n",
        "            print(\"Cost: \\n\",error,\"\\n\")\n",
        "\n",
        "    def evaluate(self, x,y):\n",
        "        epsilon=1e-15\n",
        "        inputs_layer0 = self.embedding(x)\n",
        "        Z1 = self.thetas_layer0[0] + np.dot(inputs_layer0, self.thetas_layer0[1:])\n",
        "        layer1 = relu(Z1)\n",
        "        layer1 = self.batch_normalize(layer1, self.gamma1, self.beta1)\n",
        "        Z2 = self.thetas_layer1[0] + np.dot(layer1, self.thetas_layer1[1:])\n",
        "        layer2 = relu(Z2)\n",
        "        layer2=self.batch_normalize(layer2,self.gamma2,self.beta2)\n",
        "        Z3 = self.thetas_layer2[0] + np.dot(layer2, self.thetas_layer2[1:])\n",
        "        layer3 = sigmoid(Z3)\n",
        "        y_pred = np.clip(layer3, epsilon, 1 - epsilon)\n",
        "            \n",
        "        loss = - np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "        \n",
        "        actual_output=y\n",
        "        predicted_output=layer3\n",
        "\n",
        "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
        "\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "        return loss,accuracy,layer3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmIcWDFoaZdf"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jam-8qoTaZdg",
        "outputId": "b4751c2c-1124-4ae1-dad4-6650df39aa41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration # 1\n",
            "accuracy:  0.4600391625244766\n",
            "Cost: \n",
            " 0.8293405161951576 \n",
            "\n",
            "iteration # 2\n",
            "accuracy:  0.46928754330471456\n",
            "Cost: \n",
            " 0.8022599169420729 \n",
            "\n",
            "iteration # 3\n",
            "accuracy:  0.4796505497815936\n",
            "Cost: \n",
            " 0.7834214088574645 \n",
            "\n",
            "iteration # 4\n",
            "accuracy:  0.49302605814128636\n",
            "Cost: \n",
            " 0.7702405037215229 \n",
            "\n",
            "iteration # 5\n",
            "accuracy:  0.498026811266757\n",
            "Cost: \n",
            " 0.7604618190590733 \n",
            "\n",
            "iteration # 6\n",
            "accuracy:  0.5065823166139479\n",
            "Cost: \n",
            " 0.7529394247246616 \n",
            "\n",
            "iteration # 7\n",
            "accuracy:  0.5101671938544962\n",
            "Cost: \n",
            " 0.7469821105709911 \n",
            "\n",
            "iteration # 8\n",
            "accuracy:  0.5124868203042627\n",
            "Cost: \n",
            " 0.7420914601167389 \n",
            "\n",
            "iteration # 9\n",
            "accuracy:  0.5164030727519204\n",
            "Cost: \n",
            " 0.7379916567591398 \n",
            "\n",
            "iteration # 10\n",
            "accuracy:  0.5190239493899683\n",
            "Cost: \n",
            " 0.7345296550701367 \n",
            "\n",
            "iteration # 11\n",
            "accuracy:  0.5211628257267661\n",
            "Cost: \n",
            " 0.7314937861511742 \n",
            "\n",
            "iteration # 12\n",
            "accuracy:  0.5233920771200482\n",
            "Cost: \n",
            " 0.7288247778775415 \n",
            "\n",
            "iteration # 13\n",
            "accuracy:  0.5256514535321585\n",
            "Cost: \n",
            " 0.7264442113335843 \n",
            "\n",
            "iteration # 14\n",
            "accuracy:  0.5283024551890345\n",
            "Cost: \n",
            " 0.7243146951983537 \n",
            "\n",
            "iteration # 15\n",
            "accuracy:  0.5305618316011448\n",
            "Cost: \n",
            " 0.7223469362707967 \n",
            "\n",
            "iteration # 16\n",
            "accuracy:  0.5333032083145052\n",
            "Cost: \n",
            " 0.7204926675605449 \n",
            "\n",
            "iteration # 17\n",
            "accuracy:  0.5358337098960687\n",
            "Cost: \n",
            " 0.718811717520278 \n",
            "\n",
            "iteration # 18\n",
            "accuracy:  0.5375809609881006\n",
            "Cost: \n",
            " 0.717262367999577 \n",
            "\n",
            "iteration # 19\n",
            "accuracy:  0.5403223377014611\n",
            "Cost: \n",
            " 0.7158656578835932 \n",
            "\n",
            "iteration # 20\n",
            "accuracy:  0.5429130893206808\n",
            "Cost: \n",
            " 0.7145151317064953 \n",
            "\n",
            "iteration # 21\n",
            "accuracy:  0.5444193402620877\n",
            "Cost: \n",
            " 0.7132029155239322 \n",
            "\n",
            "iteration # 22\n",
            "accuracy:  0.5465582165988854\n",
            "Cost: \n",
            " 0.7119975463991979 \n",
            "\n",
            "iteration # 23\n",
            "accuracy:  0.5492995933122459\n",
            "Cost: \n",
            " 0.710868708902292 \n",
            "\n",
            "iteration # 24\n",
            "accuracy:  0.5494803434252147\n",
            "Cost: \n",
            " 0.7098085838239816 \n",
            "\n",
            "iteration # 25\n",
            "accuracy:  0.5501129688206056\n",
            "Cost: \n",
            " 0.7087783350408874 \n",
            "\n",
            "iteration # 26\n",
            "accuracy:  0.5516794697996686\n",
            "Cost: \n",
            " 0.7077475424087726 \n",
            "\n",
            "iteration # 27\n",
            "accuracy:  0.5528844705527941\n",
            "Cost: \n",
            " 0.7067605929837071 \n",
            "\n",
            "iteration # 28\n",
            "accuracy:  0.5530049706281066\n",
            "Cost: \n",
            " 0.7058338417132292 \n",
            "\n",
            "iteration # 29\n",
            "accuracy:  0.5531857207410754\n",
            "Cost: \n",
            " 0.7049322040887508 \n",
            "\n",
            "iteration # 30\n",
            "accuracy:  0.5534869709293568\n",
            "Cost: \n",
            " 0.7040572302337094 \n",
            "\n",
            "iteration # 31\n",
            "accuracy:  0.5528844705527941\n",
            "Cost: \n",
            " 0.7032058335398413 \n",
            "\n",
            "iteration # 32\n",
            "accuracy:  0.552854345533966\n",
            "Cost: \n",
            " 0.7023669985053801 \n",
            "\n",
            "iteration # 33\n",
            "accuracy:  0.5532760957975599\n",
            "Cost: \n",
            " 0.7015559586413099 \n",
            "\n",
            "iteration # 34\n",
            "accuracy:  0.5530953456845911\n",
            "Cost: \n",
            " 0.7007930113664868 \n",
            "\n",
            "iteration # 35\n",
            "accuracy:  0.5529748456092785\n",
            "Cost: \n",
            " 0.7000510493177111 \n",
            "\n",
            "iteration # 36\n",
            "accuracy:  0.554390721494201\n",
            "Cost: \n",
            " 0.6993348188548438 \n",
            "\n",
            "iteration # 37\n",
            "accuracy:  0.5547823467389668\n",
            "Cost: \n",
            " 0.6986203332565472 \n",
            "\n",
            "iteration # 38\n",
            "accuracy:  0.5556559722849826\n",
            "Cost: \n",
            " 0.6979184309920946 \n",
            "\n",
            "iteration # 39\n",
            "accuracy:  0.5545714716071698\n",
            "Cost: \n",
            " 0.697232093187044 \n",
            "\n",
            "iteration # 40\n",
            "accuracy:  0.5553848471155295\n",
            "Cost: \n",
            " 0.6965714492060788 \n",
            "\n",
            "iteration # 41\n",
            "accuracy:  0.5563488477180298\n",
            "Cost: \n",
            " 0.6959406415620087 \n",
            "\n",
            "iteration # 42\n",
            "accuracy:  0.5568007230004519\n",
            "Cost: \n",
            " 0.6953293338858533 \n",
            "\n",
            "iteration # 43\n",
            "accuracy:  0.5567404729627956\n",
            "Cost: \n",
            " 0.6947459161351139 \n",
            "\n",
            "iteration # 44\n",
            "accuracy:  0.5573429733393583\n",
            "Cost: \n",
            " 0.694183235101926 \n",
            "\n",
            "iteration # 45\n",
            "accuracy:  0.5575237234523271\n",
            "Cost: \n",
            " 0.6936231505140925 \n",
            "\n",
            "iteration # 46\n",
            "accuracy:  0.5577647236029523\n",
            "Cost: \n",
            " 0.6930749823767757 \n",
            "\n",
            "iteration # 47\n",
            "accuracy:  0.558156348847718\n",
            "Cost: \n",
            " 0.692540329988245 \n",
            "\n",
            "iteration # 48\n",
            "accuracy:  0.5589395993372496\n",
            "Cost: \n",
            " 0.6920156486127472 \n",
            "\n",
            "iteration # 49\n",
            "accuracy:  0.559481849676156\n",
            "Cost: \n",
            " 0.691506030640701 \n",
            "\n",
            "iteration # 50\n",
            "accuracy:  0.5598433499020937\n",
            "Cost: \n",
            " 0.6909967716077723 \n",
            "\n",
            "iteration # 51\n",
            "accuracy:  0.5609278505799066\n",
            "Cost: \n",
            " 0.6905055881390235 \n",
            "\n",
            "iteration # 52\n",
            "accuracy:  0.5615604759752975\n",
            "Cost: \n",
            " 0.6900342209100967 \n",
            "\n",
            "iteration # 53\n",
            "accuracy:  0.5623738514836572\n",
            "Cost: \n",
            " 0.6895789483540488 \n",
            "\n",
            "iteration # 54\n",
            "accuracy:  0.5630366018978762\n",
            "Cost: \n",
            " 0.6891457627159313 \n",
            "\n",
            "iteration # 55\n",
            "accuracy:  0.5633981021238138\n",
            "Cost: \n",
            " 0.6887170464242484 \n",
            "\n",
            "iteration # 56\n",
            "accuracy:  0.5640608525380328\n",
            "Cost: \n",
            " 0.6882945211922028 \n",
            "\n",
            "iteration # 57\n",
            "accuracy:  0.5642416026510017\n",
            "Cost: \n",
            " 0.6878824494949702 \n",
            "\n",
            "iteration # 58\n",
            "accuracy:  0.5643621027263142\n",
            "Cost: \n",
            " 0.6874807203665813 \n",
            "\n",
            "iteration # 59\n",
            "accuracy:  0.563880102425064\n",
            "Cost: \n",
            " 0.6870927230296752 \n",
            "\n",
            "iteration # 60\n",
            "accuracy:  0.5646031028769393\n",
            "Cost: \n",
            " 0.6867137641311735 \n",
            "\n",
            "iteration # 61\n",
            "accuracy:  0.5656574785359241\n",
            "Cost: \n",
            " 0.6863461279215217 \n",
            "\n",
            "iteration # 62\n",
            "accuracy:  0.5665913541195964\n",
            "Cost: \n",
            " 0.6859886288543312 \n",
            "\n",
            "iteration # 63\n",
            "accuracy:  0.5667721042325652\n",
            "Cost: \n",
            " 0.6856473682353302 \n",
            "\n",
            "iteration # 64\n",
            "accuracy:  0.5674951046844404\n",
            "Cost: \n",
            " 0.6853200556322716 \n",
            "\n",
            "iteration # 65\n",
            "accuracy:  0.5672239795149872\n",
            "Cost: \n",
            " 0.6849999710938305 \n",
            "\n",
            "iteration # 66\n",
            "accuracy:  0.5680373550233468\n",
            "Cost: \n",
            " 0.6846864422201853 \n",
            "\n",
            "iteration # 67\n",
            "accuracy:  0.5679469799668625\n",
            "Cost: \n",
            " 0.6843878892592613 \n",
            "\n",
            "iteration # 68\n",
            "accuracy:  0.5684892303057689\n",
            "Cost: \n",
            " 0.6841032886354711 \n",
            "\n",
            "iteration # 69\n",
            "accuracy:  0.5690314806446755\n",
            "Cost: \n",
            " 0.683825155462184 \n",
            "\n",
            "iteration # 70\n",
            "accuracy:  0.5688507305317065\n",
            "Cost: \n",
            " 0.683555518017344 \n",
            "\n",
            "iteration # 71\n",
            "accuracy:  0.5696339810212382\n",
            "Cost: \n",
            " 0.6832911895357271 \n",
            "\n",
            "iteration # 72\n",
            "accuracy:  0.5701762313601446\n",
            "Cost: \n",
            " 0.6830404795893167 \n",
            "\n",
            "iteration # 73\n",
            "accuracy:  0.5711703569814731\n",
            "Cost: \n",
            " 0.6827987096092076 \n",
            "\n",
            "iteration # 74\n",
            "accuracy:  0.5711703569814731\n",
            "Cost: \n",
            " 0.6825616582952777 \n",
            "\n",
            "iteration # 75\n",
            "accuracy:  0.571140231962645\n",
            "Cost: \n",
            " 0.6823260461625487 \n",
            "\n",
            "iteration # 76\n",
            "accuracy:  0.5715017321885826\n",
            "Cost: \n",
            " 0.6820975660486801 \n",
            "\n",
            "iteration # 77\n",
            "accuracy:  0.5725862328663955\n",
            "Cost: \n",
            " 0.6818793113944297 \n",
            "\n",
            "iteration # 78\n",
            "accuracy:  0.5730381081488176\n",
            "Cost: \n",
            " 0.6816690291720185 \n",
            "\n",
            "iteration # 79\n",
            "accuracy:  0.573128483205302\n",
            "Cost: \n",
            " 0.6814614262513841 \n",
            "\n",
            "iteration # 80\n",
            "accuracy:  0.572917608073505\n",
            "Cost: \n",
            " 0.6812595241273126 \n",
            "\n",
            "iteration # 81\n",
            "accuracy:  0.5736707335442085\n",
            "Cost: \n",
            " 0.681065120126716 \n",
            "\n",
            "iteration # 82\n",
            "accuracy:  0.5741527338454586\n",
            "Cost: \n",
            " 0.6808713017134133 \n",
            "\n",
            "iteration # 83\n",
            "accuracy:  0.5747251092031933\n",
            "Cost: \n",
            " 0.6806853798809664 \n",
            "\n",
            "iteration # 84\n",
            "accuracy:  0.5748154842596777\n",
            "Cost: \n",
            " 0.6805026010490124 \n",
            "\n",
            "iteration # 85\n",
            "accuracy:  0.575538484711553\n",
            "Cost: \n",
            " 0.6803270155682938 \n",
            "\n",
            "iteration # 86\n",
            "accuracy:  0.5756288597680373\n",
            "Cost: \n",
            " 0.6801566902284574 \n",
            "\n",
            "iteration # 87\n",
            "accuracy:  0.5757493598433499\n",
            "Cost: \n",
            " 0.6799911567061496 \n",
            "\n",
            "iteration # 88\n",
            "accuracy:  0.5760506100316313\n",
            "Cost: \n",
            " 0.6798287899016314 \n",
            "\n",
            "iteration # 89\n",
            "accuracy:  0.5761711101069438\n",
            "Cost: \n",
            " 0.6796713173203132 \n",
            "\n",
            "iteration # 90\n",
            "accuracy:  0.5768037355023347\n",
            "Cost: \n",
            " 0.679513419631973 \n",
            "\n",
            "iteration # 91\n",
            "accuracy:  0.5777978611236632\n",
            "Cost: \n",
            " 0.6793566114338573 \n",
            "\n",
            "iteration # 92\n",
            "accuracy:  0.5782497364060852\n",
            "Cost: \n",
            " 0.6792021212545738 \n",
            "\n",
            "iteration # 93\n",
            "accuracy:  0.5788221117638198\n",
            "Cost: \n",
            " 0.6790540702473885 \n",
            "\n",
            "iteration # 94\n",
            "accuracy:  0.5791836119897575\n",
            "Cost: \n",
            " 0.6789107633625783 \n",
            "\n",
            "iteration # 95\n",
            "accuracy:  0.5795451122156952\n",
            "Cost: \n",
            " 0.678766366530656 \n",
            "\n",
            "iteration # 96\n",
            "accuracy:  0.5798463624039765\n",
            "Cost: \n",
            " 0.6786246997524642 \n",
            "\n",
            "iteration # 97\n",
            "accuracy:  0.5800572375357734\n",
            "Cost: \n",
            " 0.6784866531516827 \n",
            "\n",
            "iteration # 98\n",
            "accuracy:  0.5802379876487422\n",
            "Cost: \n",
            " 0.6783495699765872 \n",
            "\n",
            "iteration # 99\n",
            "accuracy:  0.5802681126675704\n",
            "Cost: \n",
            " 0.6782177272919868 \n",
            "\n",
            "iteration # 100\n",
            "accuracy:  0.5805091128181955\n",
            "Cost: \n",
            " 0.678090641394379 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "nn=NeuralNetwork(x_train,y_train,l_rate=0.1)\n",
        "nn.fit(epochs=100,Backpropagate=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ojw8bg1aZdg",
        "outputId": "de7fbef2-8dff-42c3-9ba8-d5a83f4e091e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.6819967033549771\n",
            "Acc : 0.5689109805693628\n"
          ]
        }
      ],
      "source": [
        "test=nn.evaluate(x_cv,y_cv.reshape(-1,1))\n",
        "print('Loss :',test[0])\n",
        "print('Acc :',test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C5S5ElwaZdh",
        "outputId": "896f9eb4-0c96-43f8-9853-6917ed580de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.6772721694977464\n",
            "Acc : 0.5759602349751468\n"
          ]
        }
      ],
      "source": [
        "Test=nn.evaluate(x_test,y_test.reshape(-1,1))\n",
        "print('Loss :',Test[0])\n",
        "print('Acc :',Test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKHWV5RNaZdh",
        "outputId": "529d6112-52d6-43ce-e809-50781e7818f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration # 1\n",
            "accuracy:  0.5158005723753577\n",
            "Cost: \n",
            " 0.8606728146887423 \n",
            "\n",
            "iteration # 2\n",
            "accuracy:  0.5189034493146558\n",
            "Cost: \n",
            " 0.8586469465039336 \n",
            "\n",
            "iteration # 3\n",
            "accuracy:  0.5228498267811418\n",
            "Cost: \n",
            " 0.8577665845609518 \n",
            "\n",
            "iteration # 4\n",
            "accuracy:  0.5263443289652056\n",
            "Cost: \n",
            " 0.858618010110387 \n",
            "\n",
            "iteration # 5\n",
            "accuracy:  0.530170206356379\n",
            "Cost: \n",
            " 0.8628512366956532 \n",
            "\n",
            "iteration # 6\n",
            "accuracy:  0.5372797107998193\n",
            "Cost: \n",
            " 0.8756422940305512 \n",
            "\n",
            "iteration # 7\n",
            "accuracy:  0.5466184666365417\n",
            "Cost: \n",
            " 0.8902087601556512 \n",
            "\n",
            "iteration # 8\n",
            "accuracy:  0.549118843199277\n",
            "Cost: \n",
            " 0.9201626330751682 \n",
            "\n",
            "iteration # 9\n",
            "accuracy:  0.5534267208917005\n",
            "Cost: \n",
            " 0.9166920352201093 \n",
            "\n",
            "iteration # 10\n",
            "accuracy:  0.5586684741677963\n",
            "Cost: \n",
            " 0.9323729936389107 \n",
            "\n",
            "iteration # 11\n",
            "accuracy:  0.5608676005422504\n",
            "Cost: \n",
            " 0.8967934819400316 \n",
            "\n",
            "iteration # 12\n",
            "accuracy:  0.5659888537430335\n",
            "Cost: \n",
            " 0.8291606547111694 \n",
            "\n",
            "iteration # 13\n",
            "accuracy:  0.562343726464829\n",
            "Cost: \n",
            " 0.7488526406317421 \n",
            "\n",
            "iteration # 14\n",
            "accuracy:  0.5329115830697394\n",
            "Cost: \n",
            " 0.7087456558906102 \n",
            "\n",
            "iteration # 15\n",
            "accuracy:  0.4561831601144751\n",
            "Cost: \n",
            " 0.838473632630493 \n",
            "\n",
            "iteration # 16\n",
            "accuracy:  0.515198071998795\n",
            "Cost: \n",
            " 0.7196307504769008 \n",
            "\n",
            "iteration # 17\n",
            "accuracy:  0.5688206055128785\n",
            "Cost: \n",
            " 0.7370048557231487 \n",
            "\n",
            "iteration # 18\n",
            "accuracy:  0.5709896068685043\n",
            "Cost: \n",
            " 0.6806253412117824 \n",
            "\n",
            "iteration # 19\n",
            "accuracy:  0.5620424762765477\n",
            "Cost: \n",
            " 0.6833023913910324 \n",
            "\n",
            "iteration # 20\n",
            "accuracy:  0.5811116131947582\n",
            "Cost: \n",
            " 0.6838324096390748 \n",
            "\n",
            "iteration # 21\n",
            "accuracy:  0.5826178641361651\n",
            "Cost: \n",
            " 0.6738036116202357 \n",
            "\n",
            "iteration # 22\n",
            "accuracy:  0.5731586082241301\n",
            "Cost: \n",
            " 0.6786995669286456 \n",
            "\n",
            "iteration # 23\n",
            "accuracy:  0.5426419641512276\n",
            "Cost: \n",
            " 0.694204717057927 \n",
            "\n",
            "iteration # 24\n",
            "accuracy:  0.5674649796656123\n",
            "Cost: \n",
            " 0.6773396455374977 \n",
            "\n",
            "iteration # 25\n",
            "accuracy:  0.5743937339960837\n",
            "Cost: \n",
            " 0.6773925733819911 \n",
            "\n",
            "iteration # 26\n",
            "accuracy:  0.5830697394185871\n",
            "Cost: \n",
            " 0.6722664458325152 \n",
            "\n",
            "iteration # 27\n",
            "accuracy:  0.582587739117337\n",
            "Cost: \n",
            " 0.671352727059152 \n",
            "\n",
            "iteration # 28\n",
            "accuracy:  0.5792739870462419\n",
            "Cost: \n",
            " 0.6800759470432949 \n",
            "\n",
            "iteration # 29\n",
            "accuracy:  0.5831902394938997\n",
            "Cost: \n",
            " 0.6724728241012322 \n",
            "\n",
            "iteration # 30\n",
            "accuracy:  0.5851182406989004\n",
            "Cost: \n",
            " 0.6720172165726069 \n",
            "\n",
            "iteration # 31\n",
            "accuracy:  0.5838529899081187\n",
            "Cost: \n",
            " 0.669899628822786 \n",
            "\n",
            "iteration # 32\n",
            "accuracy:  0.5826178641361651\n",
            "Cost: \n",
            " 0.6734910759322078 \n",
            "\n",
            "iteration # 33\n",
            "accuracy:  0.5836421147763218\n",
            "Cost: \n",
            " 0.6735293943863018 \n",
            "\n",
            "iteration # 34\n",
            "accuracy:  0.5858111161319476\n",
            "Cost: \n",
            " 0.6705761129172727 \n",
            "\n",
            "iteration # 35\n",
            "accuracy:  0.5858111161319476\n",
            "Cost: \n",
            " 0.6703246233371362 \n",
            "\n",
            "iteration # 36\n",
            "accuracy:  0.5879499924687452\n",
            "Cost: \n",
            " 0.671084217348438 \n",
            "\n",
            "iteration # 37\n",
            "accuracy:  0.5877391173369483\n",
            "Cost: \n",
            " 0.6705789617321989 \n",
            "\n",
            "iteration # 38\n",
            "accuracy:  0.5856604910378069\n",
            "Cost: \n",
            " 0.6715676078495669 \n",
            "\n",
            "iteration # 39\n",
            "accuracy:  0.5891248682030427\n",
            "Cost: \n",
            " 0.6702368113253937 \n",
            "\n",
            "iteration # 40\n",
            "accuracy:  0.5910227443892152\n",
            "Cost: \n",
            " 0.6702736240669257 \n",
            "\n",
            "iteration # 41\n",
            "accuracy:  0.5923181201988251\n",
            "Cost: \n",
            " 0.6674954556049045 \n",
            "\n",
            "iteration # 42\n",
            "accuracy:  0.5899683687302305\n",
            "Cost: \n",
            " 0.6673428021587785 \n",
            "\n",
            "iteration # 43\n",
            "accuracy:  0.5917457448410905\n",
            "Cost: \n",
            " 0.6666620361971383 \n",
            "\n",
            "iteration # 44\n",
            "accuracy:  0.5924084952553096\n",
            "Cost: \n",
            " 0.6661180152325844 \n",
            "\n",
            "iteration # 45\n",
            "accuracy:  0.5925591203494502\n",
            "Cost: \n",
            " 0.6654480486129927 \n",
            "\n",
            "iteration # 46\n",
            "accuracy:  0.5925892453682784\n",
            "Cost: \n",
            " 0.6661766299520695 \n",
            "\n",
            "iteration # 47\n",
            "accuracy:  0.5920469950293719\n",
            "Cost: \n",
            " 0.665162490351872 \n",
            "\n",
            "iteration # 48\n",
            "accuracy:  0.5928302455189034\n",
            "Cost: \n",
            " 0.6650768024124738 \n",
            "\n",
            "iteration # 49\n",
            "accuracy:  0.592950745594216\n",
            "Cost: \n",
            " 0.6649212592416546 \n",
            "\n",
            "iteration # 50\n",
            "accuracy:  0.5933724958578099\n",
            "Cost: \n",
            " 0.6646317956660173 \n",
            "\n",
            "iteration # 51\n",
            "accuracy:  0.5928302455189034\n",
            "Cost: \n",
            " 0.6644120193485649 \n",
            "\n",
            "iteration # 52\n",
            "accuracy:  0.5928904955565597\n",
            "Cost: \n",
            " 0.6641724583996974 \n",
            "\n",
            "iteration # 53\n",
            "accuracy:  0.5905708691067932\n",
            "Cost: \n",
            " 0.6635726608730179 \n",
            "\n",
            "iteration # 54\n",
            "accuracy:  0.5946979966862479\n",
            "Cost: \n",
            " 0.664447503249947 \n",
            "\n",
            "iteration # 55\n",
            "accuracy:  0.5926193703871064\n",
            "Cost: \n",
            " 0.6634257977937594 \n",
            "\n",
            "iteration # 56\n",
            "accuracy:  0.5899382437114024\n",
            "Cost: \n",
            " 0.6642403681703856 \n",
            "\n",
            "iteration # 57\n",
            "accuracy:  0.5903298689561681\n",
            "Cost: \n",
            " 0.663006976529931 \n",
            "\n",
            "iteration # 58\n",
            "accuracy:  0.5957523723452327\n",
            "Cost: \n",
            " 0.6637924104328031 \n",
            "\n",
            "iteration # 59\n",
            "accuracy:  0.5901491188431993\n",
            "Cost: \n",
            " 0.6635734824216571 \n",
            "\n",
            "iteration # 60\n",
            "accuracy:  0.5931013706883567\n",
            "Cost: \n",
            " 0.6634895425075051 \n",
            "\n",
            "iteration # 61\n",
            "accuracy:  0.5924687452929658\n",
            "Cost: \n",
            " 0.663131094049817 \n",
            "\n",
            "iteration # 62\n",
            "accuracy:  0.5935532459707787\n",
            "Cost: \n",
            " 0.6627126725516782 \n",
            "\n",
            "iteration # 63\n",
            "accuracy:  0.5943063714414821\n",
            "Cost: \n",
            " 0.6624081696619005 \n",
            "\n",
            "iteration # 64\n",
            "accuracy:  0.5933423708389818\n",
            "Cost: \n",
            " 0.6616943777177418 \n",
            "\n",
            "iteration # 65\n",
            "accuracy:  0.5949088718180449\n",
            "Cost: \n",
            " 0.6608386394257385 \n",
            "\n",
            "iteration # 66\n",
            "accuracy:  0.5974694984184366\n",
            "Cost: \n",
            " 0.6611039704848556 \n",
            "\n",
            "iteration # 67\n",
            "accuracy:  0.5965958728724206\n",
            "Cost: \n",
            " 0.6603795511346814 \n",
            "\n",
            "iteration # 68\n",
            "accuracy:  0.5948184967615605\n",
            "Cost: \n",
            " 0.6604252413508999 \n",
            "\n",
            "iteration # 69\n",
            "accuracy:  0.596716372947733\n",
            "Cost: \n",
            " 0.6600598603395786 \n",
            "\n",
            "iteration # 70\n",
            "accuracy:  0.5970477481548426\n",
            "Cost: \n",
            " 0.66010429314048 \n",
            "\n",
            "iteration # 71\n",
            "accuracy:  0.5970477481548426\n",
            "Cost: \n",
            " 0.6599278616768613 \n",
            "\n",
            "iteration # 72\n",
            "accuracy:  0.5965054978159361\n",
            "Cost: \n",
            " 0.6597976462964845 \n",
            "\n",
            "iteration # 73\n",
            "accuracy:  0.597800873625546\n",
            "Cost: \n",
            " 0.6598526428296848 \n",
            "\n",
            "iteration # 74\n",
            "accuracy:  0.5986443741527339\n",
            "Cost: \n",
            " 0.6597769374218838 \n",
            "\n",
            "iteration # 75\n",
            "accuracy:  0.599126374453984\n",
            "Cost: \n",
            " 0.6599169628083073 \n",
            "\n",
            "iteration # 76\n",
            "accuracy:  0.5950594969121855\n",
            "Cost: \n",
            " 0.6612105166065002 \n",
            "\n",
            "iteration # 77\n",
            "accuracy:  0.5974393733996084\n",
            "Cost: \n",
            " 0.6595955724376013 \n",
            "\n",
            "iteration # 78\n",
            "accuracy:  0.6002410001506251\n",
            "Cost: \n",
            " 0.6604276573771006 \n",
            "\n",
            "iteration # 79\n",
            "accuracy:  0.5984937490585932\n",
            "Cost: \n",
            " 0.6609680591834162 \n",
            "\n",
            "iteration # 80\n",
            "accuracy:  0.5982527489079681\n",
            "Cost: \n",
            " 0.6609895486811719 \n",
            "\n",
            "iteration # 81\n",
            "accuracy:  0.5095044434402771\n",
            "Cost: \n",
            " 0.7051817361049869 \n",
            "\n",
            "iteration # 82\n",
            "accuracy:  0.5703268564542853\n",
            "Cost: \n",
            " 0.6723901982935138 \n",
            "\n",
            "iteration # 83\n",
            "accuracy:  0.5350504594065372\n",
            "Cost: \n",
            " 0.6933061412555779 \n",
            "\n",
            "iteration # 84\n",
            "accuracy:  0.519385449615906\n",
            "Cost: \n",
            " 0.7105920578958879 \n",
            "\n",
            "iteration # 85\n",
            "accuracy:  0.5150775719234825\n",
            "Cost: \n",
            " 0.7299931374577724 \n",
            "\n",
            "iteration # 86\n",
            "accuracy:  0.5152884470552794\n",
            "Cost: \n",
            " 0.7411728254222828 \n",
            "\n",
            "iteration # 87\n",
            "accuracy:  0.5368880855550535\n",
            "Cost: \n",
            " 0.7053937959447505 \n",
            "\n",
            "iteration # 88\n",
            "accuracy:  0.5096550685344179\n",
            "Cost: \n",
            " 0.7076126598405068 \n",
            "\n",
            "iteration # 89\n",
            "accuracy:  0.5357734598584124\n",
            "Cost: \n",
            " 0.6859570151487016 \n",
            "\n",
            "iteration # 90\n",
            "accuracy:  0.5443892152432596\n",
            "Cost: \n",
            " 0.6854200010218677 \n",
            "\n",
            "iteration # 91\n",
            "accuracy:  0.561229100768188\n",
            "Cost: \n",
            " 0.678769549296066 \n",
            "\n",
            "iteration # 92\n",
            "accuracy:  0.5599337249585781\n",
            "Cost: \n",
            " 0.6794086217063131 \n",
            "\n",
            "iteration # 93\n",
            "accuracy:  0.5821961138725712\n",
            "Cost: \n",
            " 0.6727127226584964 \n",
            "\n",
            "iteration # 94\n",
            "accuracy:  0.5896369935231209\n",
            "Cost: \n",
            " 0.6705418168727063 \n",
            "\n",
            "iteration # 95\n",
            "accuracy:  0.5875282422051514\n",
            "Cost: \n",
            " 0.670278633775751 \n",
            "\n",
            "iteration # 96\n",
            "accuracy:  0.5976803735502335\n",
            "Cost: \n",
            " 0.6659421968904922 \n",
            "\n",
            "iteration # 97\n",
            "accuracy:  0.5971682482301551\n",
            "Cost: \n",
            " 0.6652873053459221 \n",
            "\n",
            "iteration # 98\n",
            "accuracy:  0.5993673746046091\n",
            "Cost: \n",
            " 0.6653727351914437 \n",
            "\n",
            "iteration # 99\n",
            "accuracy:  0.6014158758849224\n",
            "Cost: \n",
            " 0.6647657091912605 \n",
            "\n",
            "iteration # 100\n",
            "accuracy:  0.5990359993974996\n",
            "Cost: \n",
            " 0.667642626978864 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "nn1=NeuralNetwork(x_train,y_train)\n",
        "nn1.fit(epochs=100,Backpropagate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3qLqPFQaZdi",
        "outputId": "c0b7ef01-5519-4a44-e146-bf0b301ed0db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.6787074805728139\n",
            "Acc : 0.596023497514686\n"
          ]
        }
      ],
      "source": [
        "Test_1=nn1.evaluate(x_cv,y_cv.reshape(-1,1))\n",
        "print('Loss :',Test_1[0])\n",
        "print('Acc :',Test_1[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK7a-qwJaZdi",
        "outputId": "637e120a-0f79-40cb-baf8-b0fd94dd948d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.6758938659124083\n",
            "Acc : 0.5882512426570267\n"
          ]
        }
      ],
      "source": [
        "Test_2=nn1.evaluate(x_test,y_test.reshape(-1,1))\n",
        "print('Loss :',Test_2[0])\n",
        "print('Acc :',Test_2[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-5QoYz8aZdj"
      },
      "source": [
        "# Bayiesn Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "nI6RJCHRaZdj"
      },
      "outputs": [],
      "source": [
        "class BayesianNeuralNetwork:\n",
        "    def __init__(self, x, y,vocabulary_size=21361,nodes_in_layer1=32, nodes_in_layer2=16, nodes_in_layer3=1, l_rate=0.1):\n",
        "        embedding_dim= 50\n",
        "        vocab_size = vocabulary_size +1 # Add 1 for the out-of-vocabulary token\n",
        "        self.embedding_weights = np.random.randn(vocab_size, embedding_dim)\n",
        "        # Define x, y\n",
        "        self.inputs_in_layer0 = self.embedding(x) # Layer 0\n",
        "        self.y = y.reshape(-1,1)\n",
        "\n",
        "        self.l_rate = l_rate  # Learning rate\n",
        "\n",
        "        # Define and set the number of neurons in each layer\n",
        "        self.nodes_in_layer1 = nodes_in_layer1\n",
        "        self.nodes_in_layer2 = nodes_in_layer2\n",
        "        self.nodes_in_layer3 = nodes_in_layer3\n",
        "\n",
        "        # Initialize weights and biases with smaller values using Gaussian distributions\n",
        "        self.thetas_layer0 = initialize_weights(self.inputs_in_layer0.shape[1] + 1, self.nodes_in_layer1, mu=0.001, sigma=0.01)\n",
        "        self.thetas_layer1 = initialize_weights(self.nodes_in_layer1 + 1, self.nodes_in_layer2, mu=0.001, sigma=0.01)\n",
        "        self.thetas_layer2 = initialize_weights(self.nodes_in_layer2 + 1, self.nodes_in_layer3, mu=0.001, sigma=0.01)\n",
        "\n",
        "        # Initialize prior distributions for weights\n",
        "        self.prior_mean_theta0 = np.zeros_like(self.thetas_layer0)\n",
        "        self.prior_mean_theta1 = np.zeros_like(self.thetas_layer1)\n",
        "        self.prior_mean_theta2 = np.zeros_like(self.thetas_layer2)\n",
        "\n",
        "        self.prior_variance_theta0 = np.ones_like(self.thetas_layer0)\n",
        "        self.prior_variance_theta1 = np.ones_like(self.thetas_layer1)\n",
        "        self.prior_variance_theta2 = np.ones_like(self.thetas_layer2)\n",
        "    def embedding(self, x):\n",
        "        self.embedded_input = self.embedding_weights[x]\n",
        "        pooled_embeddings = np.mean(self.embedded_input, axis=1)\n",
        "        return pooled_embeddings\n",
        "\n",
        "    def feedforward(self):\n",
        "        # Sample weights from their respective Gaussian distributions for each forward pass\n",
        "        self.Z1 = self.thetas_layer0[0] + np.dot(self.inputs_in_layer0, self.thetas_layer0[1:])\n",
        "        self.layer1 = relu(self.Z1)\n",
        "\n",
        "        self.Z2 = self.thetas_layer1[0] + np.dot(self.layer1, self.thetas_layer1[1:])\n",
        "        self.layer2 = relu(self.Z2)\n",
        "\n",
        "        self.Z3 = self.thetas_layer2[0] + np.dot(self.layer2, self.thetas_layer2[1:])\n",
        "        self.layer3 = sigmoid(self.Z3)\n",
        "\n",
        "        return self.layer3\n",
        "\n",
        "    def calculate_accuracy(self):\n",
        "        actual_output = self.y\n",
        "        predicted_output = self.layer3\n",
        "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
        "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
        "\n",
        "        # Compare predicted classes with actual classes\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def log_prior(self):\n",
        "        # Calculate log priors for weights using Gaussian distributions\n",
        "        log_prior_theta0 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta0) +\n",
        "                                   ((self.thetas_layer0 - self.prior_mean_theta0) ** 2) /\n",
        "                                   self.prior_variance_theta0).sum()\n",
        "\n",
        "        log_prior_theta1 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta1) +\n",
        "                                   ((self.thetas_layer1 - self.prior_mean_theta1) ** 2) /\n",
        "                                   self.prior_variance_theta1).sum()\n",
        "\n",
        "        log_prior_theta2 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta2) +\n",
        "                                   ((self.thetas_layer2 - self.prior_mean_theta2) ** 2) /\n",
        "                                   self.prior_variance_theta2).sum()\n",
        "\n",
        "        return log_prior_theta0 + log_prior_theta1 + log_prior_theta2\n",
        "\n",
        "    def log_likelihood(self):\n",
        "        # Compute log likelihood for Bernoulli distribution\n",
        "        self.n = self.inputs_in_layer0.shape[0]  # Number of training examples\n",
        "\n",
        "        # Calculate log-likelihood for Bernoulli likelihood\n",
        "        epsilon = 1e-10  # Small value to prevent log(0)\n",
        "        log_likelihood = np.sum(self.y * np.log(self.layer3 + epsilon) + (1 - self.y) * np.log(1 - self.layer3 + epsilon))\n",
        "\n",
        "        # Normalize log-likelihood by the number of training examples\n",
        "        log_likelihood /= -self.n\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def log_posterior(self):\n",
        "        # Compute log posterior using log prior and log likelihood\n",
        "        log_prior = self.log_prior()\n",
        "        log_likelihood = self.log_likelihood()\n",
        "        log_posterior = log_prior + log_likelihood\n",
        "        return log_posterior\n",
        "    def perform_MCMC(self,proposal_variance=0.01):\n",
        "         # Make a copy of the current weights for proposal\n",
        "        proposed_thetas_layer0 = np.copy(self.thetas_layer0)\n",
        "        proposed_thetas_layer1 = np.copy(self.thetas_layer1)\n",
        "        proposed_thetas_layer2 = np.copy(self.thetas_layer2)\n",
        "\n",
        "            # Perturb the weights for proposal (using a Gaussian random walk as an example)\n",
        "        proposed_thetas_layer0 += np.random.normal(0, proposal_variance, size=self.thetas_layer0.shape)\n",
        "        proposed_thetas_layer1 += np.random.normal(0, proposal_variance, size=self.thetas_layer1.shape)\n",
        "        proposed_thetas_layer2 += np.random.normal(0, proposal_variance, size=self.thetas_layer2.shape)\n",
        "\n",
        "            # Compute log-likelihoods for current and proposed weights\n",
        "        current_log_likelihood = self.log_likelihood()\n",
        "\n",
        "            # Compute log-posterior for the proposed weights\n",
        "        self.thetas_layer0 = proposed_thetas_layer0\n",
        "        self.thetas_layer1 = proposed_thetas_layer1\n",
        "        self.thetas_layer2 = proposed_thetas_layer2\n",
        "\n",
        "        proposed_log_likelihood = self.log_likelihood()\n",
        "        proposed_log_posterior = self.log_prior() + proposed_log_likelihood\n",
        "\n",
        "            # Accept or reject the proposal based on Metropolis-Hastings acceptance criterion\n",
        "        acceptance_ratio = np.exp(proposed_log_posterior - current_log_likelihood)\n",
        "        if np.random.uniform(0, 1) < acceptance_ratio:\n",
        "            # Accept the proposal\n",
        "            pass\n",
        "        else:\n",
        "            # Reject the proposal, revert weights to the previous state\n",
        "            self.thetas_layer0 = np.copy(proposed_thetas_layer0)\n",
        "            self.thetas_layer1 = np.copy(proposed_thetas_layer1)\n",
        "            self.thetas_layer2 = np.copy(proposed_thetas_layer2)\n",
        "    def fit(self,epochs):\n",
        "        losses=[]\n",
        "        for i in range(epochs):\n",
        "            self.feedforward()\n",
        "            error = self.log_likelihood()  # Compute log-likelihood as the error\n",
        "            losses.append(error)\n",
        "            self.perform_MCMC()\n",
        "            print(\"iteration #\",i+1)\n",
        "            print('accuracy: ',self.calculate_accuracy())\n",
        "            print(\"Cost: \\n\",error,\"\\n\")\n",
        "    def evaluate(self, x,y):\n",
        "        inputs_layer0 = self.embedding(x)\n",
        "        Z1 = self.thetas_layer0[0] + np.dot(inputs_layer0, self.thetas_layer0[1:])\n",
        "        layer1 = sigmoid(Z1)\n",
        "\n",
        "        Z2 = self.thetas_layer1[0] + np.dot(layer1, self.thetas_layer1[1:])\n",
        "        layer2 = sigmoid(Z2)\n",
        "\n",
        "        Z3 = self.thetas_layer2[0] + np.dot(layer2, self.thetas_layer2[1:])\n",
        "        layer3 = sigmoid(Z3)\n",
        "        loss= (1/inputs_layer0.shape[0]) * np.sum(-y * np.log(layer3) - (1 - y) * np.log(1 - layer3)) #cross entropy\n",
        "        actual_output=y\n",
        "        predicted_output=layer3\n",
        "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
        "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
        "\n",
        "        # Compare predicted classes with actual classes\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "        return loss,accuracy,layer3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L67X9undaZdk"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWxLtzJPaZdl",
        "outputId": "8228703c-2272-4cd2-8009-8a4eef9168ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration # 1\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6936701581322194 \n",
            "\n",
            "iteration # 2\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6932422332089776 \n",
            "\n",
            "iteration # 3\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.693958695939878 \n",
            "\n",
            "iteration # 4\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6939266782263037 \n",
            "\n",
            "iteration # 5\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.694155387010561 \n",
            "\n",
            "iteration # 6\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.694354794728195 \n",
            "\n",
            "iteration # 7\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6940013530969628 \n",
            "\n",
            "iteration # 8\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6935357444131872 \n",
            "\n",
            "iteration # 9\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6935101342038187 \n",
            "\n",
            "iteration # 10\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6929599867576629 \n",
            "\n",
            "iteration # 11\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6938929779126752 \n",
            "\n",
            "iteration # 12\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6938669465206667 \n",
            "\n",
            "iteration # 13\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.693979144738269 \n",
            "\n",
            "iteration # 14\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6929555348459462 \n",
            "\n",
            "iteration # 15\n",
            "accuracy:  0.45725204637591343\n",
            "Cost: \n",
            " 0.693237345136246 \n",
            "\n",
            "iteration # 16\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6937108890579979 \n",
            "\n",
            "iteration # 17\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6936387157353692 \n",
            "\n",
            "iteration # 18\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6932548408798401 \n",
            "\n",
            "iteration # 19\n",
            "accuracy:  0.4574069770444393\n",
            "Cost: \n",
            " 0.6932353064528386 \n",
            "\n",
            "iteration # 20\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6924119302109668 \n",
            "\n",
            "iteration # 21\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6924323986236568 \n",
            "\n",
            "iteration # 22\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6917226401129669 \n",
            "\n",
            "iteration # 23\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6912241526082431 \n",
            "\n",
            "iteration # 24\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6913844447038131 \n",
            "\n",
            "iteration # 25\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6919405591004473 \n",
            "\n",
            "iteration # 26\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6915197602066738 \n",
            "\n",
            "iteration # 27\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6917012503671612 \n",
            "\n",
            "iteration # 28\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6918549164202163 \n",
            "\n",
            "iteration # 29\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6909971502027654 \n",
            "\n",
            "iteration # 30\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6908374364014355 \n",
            "\n",
            "iteration # 31\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906356030803947 \n",
            "\n",
            "iteration # 32\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6911362507373654 \n",
            "\n",
            "iteration # 33\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6913902511789329 \n",
            "\n",
            "iteration # 34\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6913358068196488 \n",
            "\n",
            "iteration # 35\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.691313148821514 \n",
            "\n",
            "iteration # 36\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6910055357207967 \n",
            "\n",
            "iteration # 37\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6908359238481684 \n",
            "\n",
            "iteration # 38\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6908538986299847 \n",
            "\n",
            "iteration # 39\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906808378520692 \n",
            "\n",
            "iteration # 40\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.690533049771734 \n",
            "\n",
            "iteration # 41\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6907138135376358 \n",
            "\n",
            "iteration # 42\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906693232884198 \n",
            "\n",
            "iteration # 43\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6902044148624854 \n",
            "\n",
            "iteration # 44\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6902881541597723 \n",
            "\n",
            "iteration # 45\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901781252076632 \n",
            "\n",
            "iteration # 46\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901607271103042 \n",
            "\n",
            "iteration # 47\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6900880648634572 \n",
            "\n",
            "iteration # 48\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6898472309238267 \n",
            "\n",
            "iteration # 49\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689931130571207 \n",
            "\n",
            "iteration # 50\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689834945597026 \n",
            "\n",
            "iteration # 51\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901951488904187 \n",
            "\n",
            "iteration # 52\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6900325332310774 \n",
            "\n",
            "iteration # 53\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901518113149943 \n",
            "\n",
            "iteration # 54\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901181383271827 \n",
            "\n",
            "iteration # 55\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6899580826211303 \n",
            "\n",
            "iteration # 56\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901519574560979 \n",
            "\n",
            "iteration # 57\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897804109558686 \n",
            "\n",
            "iteration # 58\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897970895180776 \n",
            "\n",
            "iteration # 59\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896621337935782 \n",
            "\n",
            "iteration # 60\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896653787501182 \n",
            "\n",
            "iteration # 61\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896117298032708 \n",
            "\n",
            "iteration # 62\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6894737671127876 \n",
            "\n",
            "iteration # 63\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896689720204863 \n",
            "\n",
            "iteration # 64\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689695825016472 \n",
            "\n",
            "iteration # 65\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6895320128287796 \n",
            "\n",
            "iteration # 66\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689469395150524 \n",
            "\n",
            "iteration # 67\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6894549755264727 \n",
            "\n",
            "iteration # 68\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896557894384081 \n",
            "\n",
            "iteration # 69\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6895875328330674 \n",
            "\n",
            "iteration # 70\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6895610876538959 \n",
            "\n",
            "iteration # 71\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689518958891645 \n",
            "\n",
            "iteration # 72\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6895327007731389 \n",
            "\n",
            "iteration # 73\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6895835159641694 \n",
            "\n",
            "iteration # 74\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896270750820738 \n",
            "\n",
            "iteration # 75\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896315909267416 \n",
            "\n",
            "iteration # 76\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896667952697959 \n",
            "\n",
            "iteration # 77\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897875941562999 \n",
            "\n",
            "iteration # 78\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897081947187047 \n",
            "\n",
            "iteration # 79\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896950701814082 \n",
            "\n",
            "iteration # 80\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896575159270382 \n",
            "\n",
            "iteration # 81\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896290848073526 \n",
            "\n",
            "iteration # 82\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897097088817751 \n",
            "\n",
            "iteration # 83\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897094199392069 \n",
            "\n",
            "iteration # 84\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896295206157619 \n",
            "\n",
            "iteration # 85\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689638997878565 \n",
            "\n",
            "iteration # 86\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896069656143134 \n",
            "\n",
            "iteration # 87\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896434920142019 \n",
            "\n",
            "iteration # 88\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897175368190697 \n",
            "\n",
            "iteration # 89\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897320579845277 \n",
            "\n",
            "iteration # 90\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6900550671344766 \n",
            "\n",
            "iteration # 91\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6899853767337114 \n",
            "\n",
            "iteration # 92\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6900418001521034 \n",
            "\n",
            "iteration # 93\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.690181499955381 \n",
            "\n",
            "iteration # 94\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6904303714747771 \n",
            "\n",
            "iteration # 95\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906358784396264 \n",
            "\n",
            "iteration # 96\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906641867951505 \n",
            "\n",
            "iteration # 97\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906130075067984 \n",
            "\n",
            "iteration # 98\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6909671989486099 \n",
            "\n",
            "iteration # 99\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906328396102474 \n",
            "\n",
            "iteration # 100\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6902535518357265 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "bnn=BayesianNeuralNetwork(x_train,y_train)\n",
        "bnn.fit(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVb8GrNyaZdl",
        "outputId": "d8fddc6b-24a9-475e-89f8-1322f180b612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.6905902299133001\n",
            "Acc : 0.5375346427280395\n"
          ]
        }
      ],
      "source": [
        "bnn_test=bnn.evaluate(x_cv,y_cv.reshape(-1,1))\n",
        "print('Loss :',bnn_test[0])\n",
        "print('Acc :',bnn_test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ7gd1IXaZdm",
        "outputId": "3b03a91c-ada2-4530-b4a2-36ae324749ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.689897172719227\n",
            "Acc : 0.5410290396433305\n"
          ]
        }
      ],
      "source": [
        "bnn_test_1=bnn.evaluate(x_test,y_test.reshape(-1,1))\n",
        "print('Loss :',bnn_test_1[0])\n",
        "print('Acc :',bnn_test_1[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU6OgmythfRy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
