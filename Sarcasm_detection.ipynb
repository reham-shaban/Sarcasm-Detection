{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MPad2BhaZdQ"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "WBjD9s-bbJBB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ls2eRLAaZdY"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "GPJkqtNmaZdY"
      },
      "outputs": [],
      "source": [
        "features = np.load('clean_data/features.npy')\n",
        "labels = np.load('clean_data/labels.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCcQlfFIQ8hv"
      },
      "source": [
        "## Spilt Train, Cross Validation and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "tEuJwRv5Q9ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ca5805-e6db-49e9-b47a-c2e593e2d747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(38727, 33)\n",
            "(8299, 33)\n",
            "(8299, 33)\n",
            "(38727,)\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test_full, y_train, y_test_full = train_test_split(features, labels, train_size=0.7, random_state=1)\n",
        "x_test, x_cv, y_test, y_cv = train_test_split(x_test_full, y_test_full, train_size=0.5, random_state=1)\n",
        "print(x_train.shape)\n",
        "print(x_cv.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmWITO56aZdb"
      },
      "source": [
        "# Back Propogation, R  Back Propogation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz-t_T19aZdb"
      },
      "source": [
        "# Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "QkHnoDOPaZdb"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He1EtT6IaZdc"
      },
      "source": [
        "# Function to initialize weights as Gaussian distributions with specific mu and sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "9UaVnEnIaZdc"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(nodes_in, nodes_out, mu=0, sigma=0.1):\n",
        "    return np.random.normal(mu, sigma, size=(nodes_in, nodes_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkuPO21eaZdd"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "OxcIxptnaZdd"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, x, y,vocabulary_size=21361, nodes_in_layer1=32, nodes_in_layer2=16, nodes_in_layer3=1, l_rate=0.001):\n",
        "        embedding_dim =50\n",
        "        self.x=x\n",
        "        vocab_size = vocabulary_size +1 # Add 1 for the out-of-vocabulary token\n",
        "        self.embedding_weights = np.random.randn(vocab_size, embedding_dim)\n",
        "        # define x, y\n",
        "        self.inputs_in_layer0 =self.embedding(self.x)\n",
        "        self.y = y.reshape(-1, 1)  # reshape y to be a column vector\n",
        "\n",
        "        self.l_rate = l_rate  # learning rate\n",
        "\n",
        "        # define and set the number of neurons in each layer\n",
        "        self.nodes_in_layer1 = nodes_in_layer1\n",
        "        self.nodes_in_layer2 = nodes_in_layer2\n",
        "        self.nodes_in_layer3 = nodes_in_layer3\n",
        "\n",
        "        # initialize the weights (theta) matrices\n",
        "        # self.thetas_layer0 = np.random.rand(self.inputs_in_layer0.shape[1] + 1, self.nodes_in_layer1)\n",
        "        # self.thetas_layer1 = np.random.rand(self.nodes_in_layer1 + 1, self.nodes_in_layer2)\n",
        "        # self.thetas_layer2 = np.random.rand(self.nodes_in_layer2 + 1, self.nodes_in_layer3)\n",
        "        self.thetas_layer0 = np.random.randn(self.inputs_in_layer0.shape[1] + 1, self.nodes_in_layer1) * np.sqrt(2 / (self.inputs_in_layer0.shape[1] + 1))\n",
        "        self.thetas_layer1 = np.random.randn(self.nodes_in_layer1 + 1, self.nodes_in_layer2) * np.sqrt(2 / (self.nodes_in_layer1 + 1))\n",
        "        self.thetas_layer2 = np.random.randn(self.nodes_in_layer2 + 1, self.nodes_in_layer3) * np.sqrt(2 / (self.nodes_in_layer2 + 1))\n",
        "        self.epsilon = 1e-5\n",
        "        self.momentum = 0.9\n",
        "        self.gamma1 = np.ones(nodes_in_layer1)\n",
        "        self.beta1 = np.zeros(nodes_in_layer1)\n",
        "        self.gamma2 = np.ones(nodes_in_layer2)\n",
        "        self.beta2 = np.zeros(nodes_in_layer2)\n",
        "    def batch_normalize(self, input_data, gamma, beta):\n",
        "            # Calculate mean and variance\n",
        "            batch_mean = np.mean(input_data, axis=0)\n",
        "            batch_var = np.var(input_data, axis=0)\n",
        "\n",
        "            # Normalize\n",
        "            normalized_data = (input_data - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
        "\n",
        "            # Scale and shift\n",
        "            scaled_and_shifted_data = gamma * normalized_data + beta\n",
        "\n",
        "            return scaled_and_shifted_data\n",
        "\n",
        "    def feedforward(self):\n",
        "        #compute all the nodes (a1, a2, a3, a4) in layer1\n",
        "        n = self.inputs_in_layer0.shape[0]\n",
        "\n",
        "        self.Z1 = self.thetas_layer0[0] + np.dot(self.inputs_in_layer0, self.thetas_layer0[1:])\n",
        "        self.layer1 = relu(self.Z1)  #values of a1, a2, a3, a4 in layer 1\n",
        "        self.normalized_data_layer1 = self.batch_normalize(self.layer1, self.gamma1, self.beta1)\n",
        "        #compute all the nodes (a1, a2, a3) in layer2\n",
        "        self.Z2 = self.thetas_layer1[0] + np.dot(self.normalized_data_layer1, self.thetas_layer1[1:])\n",
        "        self.layer2 = relu(self.Z2)  #values of a1, a2, a3 in layer 2\n",
        "        self.normalized_data_layer2 = self.batch_normalize(self.layer2, self.gamma2, self.beta2)\n",
        "        #compute all the nodes (a1) in layer3\n",
        "        self.Z3 = self.thetas_layer2[0] + np.dot(self.normalized_data_layer2, self.thetas_layer2[1:])\n",
        "        self.layer3 = sigmoid(self.Z3)  #output layer\n",
        "\n",
        "        return self.layer3\n",
        "\n",
        "    def cost_func(self):\n",
        "        epsilon = 1e-15  # Small constant to avoid log(0)\n",
        "        self.n = self.inputs_in_layer0.shape[0]  # number of training examples\n",
        "        self.cost = (1/self.n) * np.sum(-self.y * np.log(self.layer3 + epsilon) - (1 - self.y) * np.log(1 - self.layer3 + epsilon))\n",
        "        return self.cost\n",
        "    def embedding(self, x):\n",
        "            self.embedded_input = self.embedding_weights[x]\n",
        "            pooled_embeddings = np.mean(self.embedded_input, axis=1)\n",
        "            return pooled_embeddings\n",
        "    def calculate_accuracy(self ):\n",
        "        actual_output=self.y\n",
        "        predicted_output=self.layer3\n",
        "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
        "        predicted_classes = (predicted_output >= 0.5 ).astype(int)\n",
        "\n",
        "        # Compare predicted classes with actual classes\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def Rbackprop(self):\n",
        "        # Define RProp parameters\n",
        "        delta0 = self.l_rate  # Initial update value\n",
        "        delta_max = 60  # Maximum update value\n",
        "        delta_min = 1e-6  # Minimum update value\n",
        "        eta_plus = 1.5  # Increase factor\n",
        "        eta_minus = 0.5  # Decrease factor\n",
        "        epsilon = 1e-15\n",
        "         #dervative of E with respect to theta and bias in layer2\n",
        "        self.dE_dlayer3 = (1/self.n) * (self.layer3-self.y)/(self.layer3*(1-self.layer3)+epsilon)\n",
        "        self.dE_dZ3 = np.multiply(self.dE_dlayer3, (sigmoid(self.Z3)* (1-sigmoid(self.Z3))))\n",
        "        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n",
        "        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n",
        "\n",
        "        #dervative of E with respect to theta and bias in layer1\n",
        "        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n",
        "        self.dE_dZ2 = np.multiply(self.dE_dlayer2, relu_derivative(self.Z2))\n",
        "        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n",
        "        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n",
        "\n",
        "\n",
        "        #dervative of E with respect to theta and bias in layer0\n",
        "        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n",
        "        self.dE_dZ1 = np.multiply(self.dE_dlayer1, relu_derivative(self.Z1))\n",
        "        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n",
        "        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n",
        "\n",
        "        # Initialize RProp update values\n",
        "        if not hasattr(self, 'prev_dE_dtheta2'):\n",
        "            self.prev_dE_dtheta2 = np.zeros_like(self.dE_dtheta2)\n",
        "            self.delta_theta2 = np.full_like(self.dE_dtheta2, delta0)\n",
        "        else:\n",
        "            self.delta_theta2 = np.where(self.dE_dtheta2 * self.prev_dE_dtheta2 > 0,\n",
        "                                        np.minimum(self.delta_theta2 * eta_plus, delta_max),\n",
        "                                        np.maximum(self.delta_theta2 * eta_minus, delta_min))\n",
        "        self.prev_dE_dtheta2 = self.dE_dtheta2\n",
        "\n",
        "        if not hasattr(self, 'prev_dE_dtheta1'):\n",
        "            self.prev_dE_dtheta1 = np.zeros_like(self.dE_dtheta1)\n",
        "            self.delta_theta1 = np.full_like(self.dE_dtheta1, delta0)\n",
        "        else:\n",
        "            self.delta_theta1 = np.where(self.dE_dtheta1 * self.prev_dE_dtheta1 > 0,\n",
        "                                        np.minimum(self.delta_theta1 * eta_plus, delta_max),\n",
        "                                        np.maximum(self.delta_theta1 * eta_minus, delta_min))\n",
        "        self.prev_dE_dtheta1 = self.dE_dtheta1\n",
        "\n",
        "        if not hasattr(self, 'prev_dE_dtheta0'):\n",
        "            self.prev_dE_dtheta0 = np.zeros_like(self.dE_dtheta0)\n",
        "            self.delta_theta0 = np.full_like(self.dE_dtheta0, delta0)\n",
        "        else:\n",
        "            self.delta_theta0 = np.where(self.dE_dtheta0 * self.prev_dE_dtheta0 > 0,\n",
        "                                        np.minimum(self.delta_theta0 * eta_plus, delta_max),\n",
        "                                        np.maximum(self.delta_theta0 * eta_minus, delta_min))\n",
        "        self.prev_dE_dtheta0 = self.dE_dtheta0\n",
        "\n",
        "        # Updating theta using RProp in layers 2, 1, and 0\n",
        "        self.thetas_layer2[1:] -= np.sign(self.dE_dtheta2) * self.delta_theta2\n",
        "        self.thetas_layer1[1:] -= np.sign(self.dE_dtheta1) * self.delta_theta1\n",
        "        self.thetas_layer0[1:] -= np.sign(self.dE_dtheta0) * self.delta_theta0\n",
        "\n",
        "        # Updating bias using RProp in layers 2, 1, and 0\n",
        "        self.thetas_layer2[0] -= np.sign(self.dE_dbias2) * self.delta_theta2[0]\n",
        "        self.thetas_layer1[0] -= np.sign(self.dE_dbias1) * self.delta_theta1[0]\n",
        "        self.thetas_layer0[0] -= np.sign(self.dE_dbias0) * self.delta_theta0[0]\n",
        "        return self\n",
        "\n",
        "    def backprop(self):\n",
        "        epsilon=1e-15\n",
        "        #dervative of E with respect to theta and bias in layer2\n",
        "        self.dE_dlayer3 = (1/self.n) * (self.layer3-self.y)/(self.layer3*(1-self.layer3)+epsilon)\n",
        "        self.dE_dZ3 = np.multiply(self.dE_dlayer3, (sigmoid(self.Z3)* (1-sigmoid(self.Z3))))\n",
        "        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n",
        "        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n",
        "\n",
        "        #dervative of E with respect to theta and bias in layer1\n",
        "        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n",
        "        self.dE_dZ2 = np.multiply(self.dE_dlayer2, relu_derivative(self.Z2))\n",
        "        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n",
        "        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n",
        "        # Gradient for batch normalization parameters in layer2\n",
        "        dL_dgamma2 = np.sum(self.dE_dZ2 * self.normalized_data_layer2, axis=0)\n",
        "        dL_dbeta2 = np.sum(self.dE_dZ2, axis=0)\n",
        "\n",
        "        #dervative of E with respect to theta and bias in layer0\n",
        "        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n",
        "        self.dE_dZ1 = np.multiply(self.dE_dlayer1, relu_derivative(self.Z1))\n",
        "        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n",
        "        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n",
        "        # Gradient for batch normalization parameters in layer1\n",
        "        dL_dgamma1 = np.sum(self.dE_dZ1 * self.normalized_data_layer1, axis=0)\n",
        "        dL_dbeta1 = np.sum(self.dE_dZ1, axis=0)\n",
        "        #updating theta using gradient descent in layers 2, 1, and 0\n",
        "        self.thetas_layer2[1:] = self.thetas_layer2[1:] - self.l_rate * self.dE_dtheta2\n",
        "        self.thetas_layer1[1:] = self.thetas_layer1[1:] - self.l_rate * self.dE_dtheta1\n",
        "        self.thetas_layer0[1:] = self.thetas_layer0[1:] - self.l_rate * self.dE_dtheta0\n",
        "        # self.de_wegihts = np.dot(self.embedded_input.T, self.dE_dZ1)\n",
        "        # self.embedding_weights = -self.embedding_weights - self.l_rate * self.de_wegihts\n",
        "        # Update batch normalization parameters using gradient descent\n",
        "        self.gamma1 -= self.l_rate * dL_dgamma1\n",
        "        self.beta1 -= self.l_rate * dL_dbeta1\n",
        "        self.gamma2 -= self.l_rate * dL_dgamma2\n",
        "        self.beta2 -= self.l_rate * dL_dbeta2\n",
        "\n",
        "        #updating bias using gradient descent in layers 2, 1, and 0\n",
        "        self.thetas_layer2[0] = self.thetas_layer2[0] - self.l_rate * self.dE_dbias2\n",
        "        self.thetas_layer1[0] = self.thetas_layer1[0] - self.l_rate * self.dE_dbias1\n",
        "        self.thetas_layer0[0] = self.thetas_layer0[0] - self.l_rate * self.dE_dbias0\n",
        "        return self\n",
        "    def fit(self,epochs,Backpropagate):\n",
        "        losses=[]\n",
        "        for i in range(epochs):\n",
        "            self.feedforward()\n",
        "            error=self.cost_func()\n",
        "            losses.append(error)\n",
        "            if Backpropagate==True:\n",
        "                self.backprop()\n",
        "            else:\n",
        "                self.Rbackprop()\n",
        "            print(\"iteration #\",i+1)\n",
        "            print('accuracy: ',self.calculate_accuracy())\n",
        "            print(\"Cost: \\n\",error,\"\\n\")\n",
        "    def evaluate(self, x,y):\n",
        "        epsilon=1e-15\n",
        "        inputs_layer0 = self.embedding(x)\n",
        "        Z1 = self.thetas_layer0[0] + np.dot(inputs_layer0, self.thetas_layer0[1:])\n",
        "        layer1 = relu(Z1)\n",
        "        layer1 = self.batch_normalize(layer1, self.gamma1, self.beta1)\n",
        "        Z2 = self.thetas_layer1[0] + np.dot(layer1, self.thetas_layer1[1:])\n",
        "        layer2 = relu(Z2)\n",
        "        layer2=self.batch_normalize(layer2,self.gamma2,self.beta2)\n",
        "        Z3 = self.thetas_layer2[0] + np.dot(layer2, self.thetas_layer2[1:])\n",
        "        layer3 = sigmoid(Z3)\n",
        "        loss= (1/inputs_layer0.shape[0]) * np.sum(-y * np.log(layer3) - (1 - y) * np.log(1 - layer3+epsilon)) #cross entropy\n",
        "        actual_output=y\n",
        "        predicted_output=layer3\n",
        "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
        "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
        "\n",
        "        # Compare predicted classes with actual classes\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "        return loss,accuracy,layer3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmIcWDFoaZdf"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jam-8qoTaZdg",
        "outputId": "b4751c2c-1124-4ae1-dad4-6650df39aa41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration # 1\n",
            "accuracy:  0.4997805148862551\n",
            "Cost: \n",
            " 0.8436458509745449 \n",
            "\n",
            "iteration # 2\n",
            "accuracy:  0.5000903762233068\n",
            "Cost: \n",
            " 0.8334022452853972 \n",
            "\n",
            "iteration # 3\n",
            "accuracy:  0.5022077620264932\n",
            "Cost: \n",
            " 0.8242117977603617 \n",
            "\n",
            "iteration # 4\n",
            "accuracy:  0.5033439202623493\n",
            "Cost: \n",
            " 0.8158612440626661 \n",
            "\n",
            "iteration # 5\n",
            "accuracy:  0.5058744545149378\n",
            "Cost: \n",
            " 0.8080388539729118 \n",
            "\n",
            "iteration # 6\n",
            "accuracy:  0.5087664936607534\n",
            "Cost: \n",
            " 0.8008780613186712 \n",
            "\n",
            "iteration # 7\n",
            "accuracy:  0.509231285666331\n",
            "Cost: \n",
            " 0.7941488030486146 \n",
            "\n",
            "iteration # 8\n",
            "accuracy:  0.5090247114416299\n",
            "Cost: \n",
            " 0.787621621660901 \n",
            "\n",
            "iteration # 9\n",
            "accuracy:  0.5097735430061714\n",
            "Cost: \n",
            " 0.7810559687349881 \n",
            "\n",
            "iteration # 10\n",
            "accuracy:  0.5139308492782813\n",
            "Cost: \n",
            " 0.7750449445235388 \n",
            "\n",
            "iteration # 11\n",
            "accuracy:  0.5165646706432205\n",
            "Cost: \n",
            " 0.769209908281527 \n",
            "\n",
            "iteration # 12\n",
            "accuracy:  0.5194308880109484\n",
            "Cost: \n",
            " 0.7637631796413099 \n",
            "\n",
            "iteration # 13\n",
            "accuracy:  0.522322927156764\n",
            "Cost: \n",
            " 0.7588062078298793 \n",
            "\n",
            "iteration # 14\n",
            "accuracy:  0.5271774214372402\n",
            "Cost: \n",
            " 0.7546933422050033 \n",
            "\n",
            "iteration # 15\n",
            "accuracy:  0.5306375397009838\n",
            "Cost: \n",
            " 0.7508788803123261 \n",
            "\n",
            "iteration # 16\n",
            "accuracy:  0.5317736979368399\n",
            "Cost: \n",
            " 0.7474361453093541 \n",
            "\n",
            "iteration # 17\n",
            "accuracy:  0.5333230046220983\n",
            "Cost: \n",
            " 0.7443849182010387 \n",
            "\n",
            "iteration # 18\n",
            "accuracy:  0.5351047073101454\n",
            "Cost: \n",
            " 0.7415173415895369 \n",
            "\n",
            "iteration # 19\n",
            "accuracy:  0.5364990833268779\n",
            "Cost: \n",
            " 0.7389083128941286 \n",
            "\n",
            "iteration # 20\n",
            "accuracy:  0.5400624887029721\n",
            "Cost: \n",
            " 0.7364321154859618 \n",
            "\n",
            "iteration # 21\n",
            "accuracy:  0.5420249438376327\n",
            "Cost: \n",
            " 0.7341643908758502 \n",
            "\n",
            "iteration # 22\n",
            "accuracy:  0.5436775376352415\n",
            "Cost: \n",
            " 0.732124980469077 \n",
            "\n",
            "iteration # 23\n",
            "accuracy:  0.5438066465256798\n",
            "Cost: \n",
            " 0.730329972599979 \n",
            "\n",
            "iteration # 24\n",
            "accuracy:  0.5454592403232886\n",
            "Cost: \n",
            " 0.7285644603321122 \n",
            "\n",
            "iteration # 25\n",
            "accuracy:  0.5458982105507785\n",
            "Cost: \n",
            " 0.7268583518132176 \n",
            "\n",
            "iteration # 26\n",
            "accuracy:  0.5461822501097425\n",
            "Cost: \n",
            " 0.7252714866269263 \n",
            "\n",
            "iteration # 27\n",
            "accuracy:  0.5478090221292639\n",
            "Cost: \n",
            " 0.7237829875290143 \n",
            "\n",
            "iteration # 28\n",
            "accuracy:  0.548583675471893\n",
            "Cost: \n",
            " 0.722364389786313 \n",
            "\n",
            "iteration # 29\n",
            "accuracy:  0.5489968239212952\n",
            "Cost: \n",
            " 0.7210195520758621 \n",
            "\n",
            "iteration # 30\n",
            "accuracy:  0.5499005861543625\n",
            "Cost: \n",
            " 0.7197787849534256 \n",
            "\n",
            "iteration # 31\n",
            "accuracy:  0.5506752394969917\n",
            "Cost: \n",
            " 0.7185876868790035 \n",
            "\n",
            "iteration # 32\n",
            "accuracy:  0.5511142097244817\n",
            "Cost: \n",
            " 0.7174729581703045 \n",
            "\n",
            "iteration # 33\n",
            "accuracy:  0.5520954372918119\n",
            "Cost: \n",
            " 0.7163823159229771 \n",
            "\n",
            "iteration # 34\n",
            "accuracy:  0.5528442688563534\n",
            "Cost: \n",
            " 0.7153358328889651 \n",
            "\n",
            "iteration # 35\n",
            "accuracy:  0.5530766648591422\n",
            "Cost: \n",
            " 0.7143569593924277 \n",
            "\n",
            "iteration # 36\n",
            "accuracy:  0.5540837142045602\n",
            "Cost: \n",
            " 0.7134456426225672 \n",
            "\n",
            "iteration # 37\n",
            "accuracy:  0.5543161102073488\n",
            "Cost: \n",
            " 0.7125658017636065 \n",
            "\n",
            "iteration # 38\n",
            "accuracy:  0.5553231595527668\n",
            "Cost: \n",
            " 0.7117092646484554 \n",
            "\n",
            "iteration # 39\n",
            "accuracy:  0.5553489813308544\n",
            "Cost: \n",
            " 0.710877256067056 \n",
            "\n",
            "iteration # 40\n",
            "accuracy:  0.5564851395667105\n",
            "Cost: \n",
            " 0.7100683759748017 \n",
            "\n",
            "iteration # 41\n",
            "accuracy:  0.5564076742324476\n",
            "Cost: \n",
            " 0.709285017854954 \n",
            "\n",
            "iteration # 42\n",
            "accuracy:  0.5574405453559532\n",
            "Cost: \n",
            " 0.708524538884832 \n",
            "\n",
            "iteration # 43\n",
            "accuracy:  0.5584734164794588\n",
            "Cost: \n",
            " 0.70778415800398 \n",
            "\n",
            "iteration # 44\n",
            "accuracy:  0.5594288222687014\n",
            "Cost: \n",
            " 0.7070600702671501 \n",
            "\n",
            "iteration # 45\n",
            "accuracy:  0.5597386836057531\n",
            "Cost: \n",
            " 0.706359799903619 \n",
            "\n",
            "iteration # 46\n",
            "accuracy:  0.5599969013866295\n",
            "Cost: \n",
            " 0.7056747948201761 \n",
            "\n",
            "iteration # 47\n",
            "accuracy:  0.5600743667208924\n",
            "Cost: \n",
            " 0.70500957253643 \n",
            "\n",
            "iteration # 48\n",
            "accuracy:  0.5610555942882227\n",
            "Cost: \n",
            " 0.7043590984305118 \n",
            "\n",
            "iteration # 49\n",
            "accuracy:  0.5615720298499755\n",
            "Cost: \n",
            " 0.7037335031521744 \n",
            "\n",
            "iteration # 50\n",
            "accuracy:  0.5625274356392181\n",
            "Cost: \n",
            " 0.7031306222694937 \n",
            "\n",
            "iteration # 51\n",
            "accuracy:  0.5632246236475844\n",
            "Cost: \n",
            " 0.7025469012735627 \n",
            "\n",
            "iteration # 52\n",
            "accuracy:  0.5636377720969866\n",
            "Cost: \n",
            " 0.70198170245112 \n",
            "\n",
            "iteration # 53\n",
            "accuracy:  0.5634053760941978\n",
            "Cost: \n",
            " 0.7014272220848161 \n",
            "\n",
            "iteration # 54\n",
            "accuracy:  0.563250445425672\n",
            "Cost: \n",
            " 0.7008900341867 \n",
            "\n",
            "iteration # 55\n",
            "accuracy:  0.5635344849846361\n",
            "Cost: \n",
            " 0.7003656534140454 \n",
            "\n",
            "iteration # 56\n",
            "accuracy:  0.5642058512149146\n",
            "Cost: \n",
            " 0.6998509066242254 \n",
            "\n",
            "iteration # 57\n",
            "accuracy:  0.5640767423244765\n",
            "Cost: \n",
            " 0.6993431093972714 \n",
            "\n",
            "iteration # 58\n",
            "accuracy:  0.5641283858806517\n",
            "Cost: \n",
            " 0.698844214329258 \n",
            "\n",
            "iteration # 59\n",
            "accuracy:  0.5649546827794562\n",
            "Cost: \n",
            " 0.6983549154798474 \n",
            "\n",
            "iteration # 60\n",
            "accuracy:  0.5652903658945955\n",
            "Cost: \n",
            " 0.6978788113770638 \n",
            "\n",
            "iteration # 61\n",
            "accuracy:  0.5656002272316472\n",
            "Cost: \n",
            " 0.6974166085016886 \n",
            "\n",
            "iteration # 62\n",
            "accuracy:  0.5661166627934\n",
            "Cost: \n",
            " 0.6969643310291821 \n",
            "\n",
            "iteration # 63\n",
            "accuracy:  0.5665814547989775\n",
            "Cost: \n",
            " 0.6965202308687067 \n",
            "\n",
            "iteration # 64\n",
            "accuracy:  0.5671237121388178\n",
            "Cost: \n",
            " 0.6960881398775111 \n",
            "\n",
            "iteration # 65\n",
            "accuracy:  0.5678467219252717\n",
            "Cost: \n",
            " 0.6956678526750655 \n",
            "\n",
            "iteration # 66\n",
            "accuracy:  0.5683115139308493\n",
            "Cost: \n",
            " 0.6952566454797348 \n",
            "\n",
            "iteration # 67\n",
            "accuracy:  0.5683889792651122\n",
            "Cost: \n",
            " 0.6948467464038235 \n",
            "\n",
            "iteration # 68\n",
            "accuracy:  0.5688795930487773\n",
            "Cost: \n",
            " 0.6944480200934043 \n",
            "\n",
            "iteration # 69\n",
            "accuracy:  0.5690345237173031\n",
            "Cost: \n",
            " 0.6940588751272216 \n",
            "\n",
            "iteration # 70\n",
            "accuracy:  0.5696026028352312\n",
            "Cost: \n",
            " 0.6936751297570978 \n",
            "\n",
            "iteration # 71\n",
            "accuracy:  0.5705838304025616\n",
            "Cost: \n",
            " 0.693298643102252 \n",
            "\n",
            "iteration # 72\n",
            "accuracy:  0.5705838304025616\n",
            "Cost: \n",
            " 0.6929261902779548 \n",
            "\n",
            "iteration # 73\n",
            "accuracy:  0.5708936917396132\n",
            "Cost: \n",
            " 0.6925574078549279 \n",
            "\n",
            "iteration # 74\n",
            "accuracy:  0.5709969788519638\n",
            "Cost: \n",
            " 0.6922047040728565 \n",
            "\n",
            "iteration # 75\n",
            "accuracy:  0.571048622408139\n",
            "Cost: \n",
            " 0.69186228697794 \n",
            "\n",
            "iteration # 76\n",
            "accuracy:  0.5711260877424019\n",
            "Cost: \n",
            " 0.6915246259580868 \n",
            "\n",
            "iteration # 77\n",
            "accuracy:  0.5713584837451907\n",
            "Cost: \n",
            " 0.691193234922993 \n",
            "\n",
            "iteration # 78\n",
            "accuracy:  0.5714359490794536\n",
            "Cost: \n",
            " 0.6908690501266281 \n",
            "\n",
            "iteration # 79\n",
            "accuracy:  0.5716683450822424\n",
            "Cost: \n",
            " 0.6905585306171983 \n",
            "\n",
            "iteration # 80\n",
            "accuracy:  0.5714875926356289\n",
            "Cost: \n",
            " 0.69026020648974 \n",
            "\n",
            "iteration # 81\n",
            "accuracy:  0.5718232757507682\n",
            "Cost: \n",
            " 0.6899717598208044 \n",
            "\n",
            "iteration # 82\n",
            "accuracy:  0.5716167015260671\n",
            "Cost: \n",
            " 0.6896827287888992 \n",
            "\n",
            "iteration # 83\n",
            "accuracy:  0.5717974539726806\n",
            "Cost: \n",
            " 0.6893941118723599 \n",
            "\n",
            "iteration # 84\n",
            "accuracy:  0.571978206419294\n",
            "Cost: \n",
            " 0.6891113022616471 \n",
            "\n",
            "iteration # 85\n",
            "accuracy:  0.5719007410850311\n",
            "Cost: \n",
            " 0.6888295235279732 \n",
            "\n",
            "iteration # 86\n",
            "accuracy:  0.5720040281973817\n",
            "Cost: \n",
            " 0.6885544897643837 \n",
            "\n",
            "iteration # 87\n",
            "accuracy:  0.5722880677563457\n",
            "Cost: \n",
            " 0.6882961935571948 \n",
            "\n",
            "iteration # 88\n",
            "accuracy:  0.5722106024220828\n",
            "Cost: \n",
            " 0.6880421389276948 \n",
            "\n",
            "iteration # 89\n",
            "accuracy:  0.5718232757507682\n",
            "Cost: \n",
            " 0.6877933588616539 \n",
            "\n",
            "iteration # 90\n",
            "accuracy:  0.5720814935316446\n",
            "Cost: \n",
            " 0.6875478110526997 \n",
            "\n",
            "iteration # 91\n",
            "accuracy:  0.5721073153097322\n",
            "Cost: \n",
            " 0.6873081870903992 \n",
            "\n",
            "iteration # 92\n",
            "accuracy:  0.5721847806439951\n",
            "Cost: \n",
            " 0.6870713523928066 \n",
            "\n",
            "iteration # 93\n",
            "accuracy:  0.5721073153097322\n",
            "Cost: \n",
            " 0.6868386548200839 \n",
            "\n",
            "iteration # 94\n",
            "accuracy:  0.5724688202029592\n",
            "Cost: \n",
            " 0.6866123091106775 \n",
            "\n",
            "iteration # 95\n",
            "accuracy:  0.5724688202029592\n",
            "Cost: \n",
            " 0.6863953567703893 \n",
            "\n",
            "iteration # 96\n",
            "accuracy:  0.5722106024220828\n",
            "Cost: \n",
            " 0.6861919773484642 \n",
            "\n",
            "iteration # 97\n",
            "accuracy:  0.5720814935316446\n",
            "Cost: \n",
            " 0.6860000611891595 \n",
            "\n",
            "iteration # 98\n",
            "accuracy:  0.5724688202029592\n",
            "Cost: \n",
            " 0.6858145680514216 \n",
            "\n",
            "iteration # 99\n",
            "accuracy:  0.5725721073153097\n",
            "Cost: \n",
            " 0.685637552840233 \n",
            "\n",
            "iteration # 100\n",
            "accuracy:  0.5723138895344333\n",
            "Cost: \n",
            " 0.6854602827382298 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "nn=NeuralNetwork(x_train,y_train,l_rate=0.1)\n",
        "nn.fit(epochs=100,Backpropagate=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ojw8bg1aZdg",
        "outputId": "de7fbef2-8dff-42c3-9ba8-d5a83f4e091e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss : 0.6896738007455112\n",
            "Acc : 0.5709121580913363\n"
          ]
        }
      ],
      "source": [
        "test=nn.evaluate(x_cv,y_cv.reshape(-1,1))\n",
        "print('Loss :',test[0])\n",
        "print('Acc :',test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C5S5ElwaZdh",
        "outputId": "896f9eb4-0c96-43f8-9853-6917ed580de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss : 0.687756925960197\n",
            "Acc : 0.5709121580913363\n"
          ]
        }
      ],
      "source": [
        "Test=nn.evaluate(x_test,y_test.reshape(-1,1))\n",
        "print('Loss :',Test[0])\n",
        "print('Acc :',Test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKHWV5RNaZdh",
        "outputId": "529d6112-52d6-43ce-e809-50781e7818f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration # 1\n",
            "accuracy:  0.5525085857412141\n",
            "Cost: \n",
            " 0.8247799918214779 \n",
            "\n",
            "iteration # 2\n",
            "accuracy:  0.5519663284013737\n",
            "Cost: \n",
            " 0.8200794113007376 \n",
            "\n",
            "iteration # 3\n",
            "accuracy:  0.5534381697523691\n",
            "Cost: \n",
            " 0.8114387664558357 \n",
            "\n",
            "iteration # 4\n",
            "accuracy:  0.557388901799778\n",
            "Cost: \n",
            " 0.7996853443386978 \n",
            "\n",
            "iteration # 5\n",
            "accuracy:  0.5579311591396183\n",
            "Cost: \n",
            " 0.7867099948875134 \n",
            "\n",
            "iteration # 6\n",
            "accuracy:  0.5597903271619283\n",
            "Cost: \n",
            " 0.7742313561118422 \n",
            "\n",
            "iteration # 7\n",
            "accuracy:  0.5596870400495778\n",
            "Cost: \n",
            " 0.7586914321114848 \n",
            "\n",
            "iteration # 8\n",
            "accuracy:  0.5697833552818448\n",
            "Cost: \n",
            " 0.737325378857916 \n",
            "\n",
            "iteration # 9\n",
            "accuracy:  0.5688795930487773\n",
            "Cost: \n",
            " 0.7149082550210897 \n",
            "\n",
            "iteration # 10\n",
            "accuracy:  0.549151754589821\n",
            "Cost: \n",
            " 0.7063938003082624 \n",
            "\n",
            "iteration # 11\n",
            "accuracy:  0.5682340485965863\n",
            "Cost: \n",
            " 0.6858900696375574 \n",
            "\n",
            "iteration # 12\n",
            "accuracy:  0.5905698866423942\n",
            "Cost: \n",
            " 0.6698825328988443 \n",
            "\n",
            "iteration # 13\n",
            "accuracy:  0.5956051333694838\n",
            "Cost: \n",
            " 0.6676947806909164 \n",
            "\n",
            "iteration # 14\n",
            "accuracy:  0.599452578304542\n",
            "Cost: \n",
            " 0.6624480757236924 \n",
            "\n",
            "iteration # 15\n",
            "accuracy:  0.59860045962765\n",
            "Cost: \n",
            " 0.6656634448516773 \n",
            "\n",
            "iteration # 16\n",
            "accuracy:  0.5977225191726703\n",
            "Cost: \n",
            " 0.6620426505096854 \n",
            "\n",
            "iteration # 17\n",
            "accuracy:  0.5940041831280501\n",
            "Cost: \n",
            " 0.665312988985639 \n",
            "\n",
            "iteration # 18\n",
            "accuracy:  0.5981873111782477\n",
            "Cost: \n",
            " 0.66277032304419 \n",
            "\n",
            "iteration # 19\n",
            "accuracy:  0.5999948356443825\n",
            "Cost: \n",
            " 0.6626290862435882 \n",
            "\n",
            "iteration # 20\n",
            "accuracy:  0.6003305187595218\n",
            "Cost: \n",
            " 0.6609645139881329 \n",
            "\n",
            "iteration # 21\n",
            "accuracy:  0.5987812120742634\n",
            "Cost: \n",
            " 0.6620535884659444 \n",
            "\n",
            "iteration # 22\n",
            "accuracy:  0.59674129160534\n",
            "Cost: \n",
            " 0.6639006124431448 \n",
            "\n",
            "iteration # 23\n",
            "accuracy:  0.5955018462571333\n",
            "Cost: \n",
            " 0.6620852327294683 \n",
            "\n",
            "iteration # 24\n",
            "accuracy:  0.5917318666563379\n",
            "Cost: \n",
            " 0.6678384676915529 \n",
            "\n",
            "iteration # 25\n",
            "accuracy:  0.599814083197769\n",
            "Cost: \n",
            " 0.6647999008348839 \n",
            "\n",
            "iteration # 26\n",
            "accuracy:  0.5976192320603196\n",
            "Cost: \n",
            " 0.6669899279617776 \n",
            "\n",
            "iteration # 27\n",
            "accuracy:  0.5938234306814367\n",
            "Cost: \n",
            " 0.6720421376720583 \n",
            "\n",
            "iteration # 28\n",
            "accuracy:  0.5926872724455806\n",
            "Cost: \n",
            " 0.6695809103510767 \n",
            "\n",
            "iteration # 29\n",
            "accuracy:  0.5931262426730705\n",
            "Cost: \n",
            " 0.6686227018675449 \n",
            "\n",
            "iteration # 30\n",
            "accuracy:  0.5904924213081313\n",
            "Cost: \n",
            " 0.6704396034172261 \n",
            "\n",
            "iteration # 31\n",
            "accuracy:  0.5951919849200816\n",
            "Cost: \n",
            " 0.6655503153062496 \n",
            "\n",
            "iteration # 32\n",
            "accuracy:  0.5987553902961757\n",
            "Cost: \n",
            " 0.6673833347273779 \n",
            "\n",
            "iteration # 33\n",
            "accuracy:  0.5989877862989645\n",
            "Cost: \n",
            " 0.6660084353305644 \n",
            "\n",
            "iteration # 34\n",
            "accuracy:  0.5998399049758566\n",
            "Cost: \n",
            " 0.6643657135606452 \n",
            "\n",
            "iteration # 35\n",
            "accuracy:  0.6002788752033466\n",
            "Cost: \n",
            " 0.6633895883442787 \n",
            "\n",
            "iteration # 36\n",
            "accuracy:  0.6031967361272498\n",
            "Cost: \n",
            " 0.6618557190117486 \n",
            "\n",
            "iteration # 37\n",
            "accuracy:  0.60417796369458\n",
            "Cost: \n",
            " 0.6613877080661108 \n",
            "\n",
            "iteration # 38\n",
            "accuracy:  0.6025253698969711\n",
            "Cost: \n",
            " 0.6608217705544753 \n",
            "\n",
            "iteration # 39\n",
            "accuracy:  0.6032225579053374\n",
            "Cost: \n",
            " 0.6604982577139217 \n",
            "\n",
            "iteration # 40\n",
            "accuracy:  0.6050817259276474\n",
            "Cost: \n",
            " 0.6603828427029689 \n",
            "\n",
            "iteration # 41\n",
            "accuracy:  0.6060887752730653\n",
            "Cost: \n",
            " 0.6593744934570935 \n",
            "\n",
            "iteration # 42\n",
            "accuracy:  0.6058822010483642\n",
            "Cost: \n",
            " 0.6587913421166423 \n",
            "\n",
            "iteration # 43\n",
            "accuracy:  0.6062437059415912\n",
            "Cost: \n",
            " 0.6578151625560503 \n",
            "\n",
            "iteration # 44\n",
            "accuracy:  0.6078188344049371\n",
            "Cost: \n",
            " 0.6571983584668403 \n",
            "\n",
            "iteration # 45\n",
            "accuracy:  0.6072507552870091\n",
            "Cost: \n",
            " 0.6565192320087563 \n",
            "\n",
            "iteration # 46\n",
            "accuracy:  0.6078446561830247\n",
            "Cost: \n",
            " 0.656895538294662 \n",
            "\n",
            "iteration # 47\n",
            "accuracy:  0.6097812895395976\n",
            "Cost: \n",
            " 0.6558227959647903 \n",
            "\n",
            "iteration # 48\n",
            "accuracy:  0.6115629922276448\n",
            "Cost: \n",
            " 0.6559732293598177 \n",
            "\n",
            "iteration # 49\n",
            "accuracy:  0.6086709530818292\n",
            "Cost: \n",
            " 0.6557503777227276 \n",
            "\n",
            "iteration # 50\n",
            "accuracy:  0.6097812895395976\n",
            "Cost: \n",
            " 0.655773067977477 \n",
            "\n",
            "iteration # 51\n",
            "accuracy:  0.6081286957419888\n",
            "Cost: \n",
            " 0.6558112286717663 \n",
            "\n",
            "iteration # 52\n",
            "accuracy:  0.6068892503937822\n",
            "Cost: \n",
            " 0.6562816740321996 \n",
            "\n",
            "iteration # 53\n",
            "accuracy:  0.6057789139360137\n",
            "Cost: \n",
            " 0.6571956108074632 \n",
            "\n",
            "iteration # 54\n",
            "accuracy:  0.6035840627985644\n",
            "Cost: \n",
            " 0.6573425674536078 \n",
            "\n",
            "iteration # 55\n",
            "accuracy:  0.6042296072507553\n",
            "Cost: \n",
            " 0.6577539819226649 \n",
            "\n",
            "iteration # 56\n",
            "accuracy:  0.6077413690706742\n",
            "Cost: \n",
            " 0.6565671013352904 \n",
            "\n",
            "iteration # 57\n",
            "accuracy:  0.6066568543909934\n",
            "Cost: \n",
            " 0.6564478321123077 \n",
            "\n",
            "iteration # 58\n",
            "accuracy:  0.6080512304077259\n",
            "Cost: \n",
            " 0.6560160873996554 \n",
            "\n",
            "iteration # 59\n",
            "accuracy:  0.6064244583882046\n",
            "Cost: \n",
            " 0.6564228393198546 \n",
            "\n",
            "iteration # 60\n",
            "accuracy:  0.6068634286156944\n",
            "Cost: \n",
            " 0.6575752165391323 \n",
            "\n",
            "iteration # 61\n",
            "accuracy:  0.6055723397113125\n",
            "Cost: \n",
            " 0.6581884766770724 \n",
            "\n",
            "iteration # 62\n",
            "accuracy:  0.6066052108348181\n",
            "Cost: \n",
            " 0.65716927833955 \n",
            "\n",
            "iteration # 63\n",
            "accuracy:  0.6042812508069305\n",
            "Cost: \n",
            " 0.6578785672708788 \n",
            "\n",
            "iteration # 64\n",
            "accuracy:  0.6039197459137036\n",
            "Cost: \n",
            " 0.6580310924987727 \n",
            "\n",
            "iteration # 65\n",
            "accuracy:  0.6044878250316317\n",
            "Cost: \n",
            " 0.6579041642513527 \n",
            "\n",
            "iteration # 66\n",
            "accuracy:  0.6047460428125081\n",
            "Cost: \n",
            " 0.6579174107633432 \n",
            "\n",
            "iteration # 67\n",
            "accuracy:  0.6066052108348181\n",
            "Cost: \n",
            " 0.6572331082496469 \n",
            "\n",
            "iteration # 68\n",
            "accuracy:  0.6059596663826271\n",
            "Cost: \n",
            " 0.656526618132908 \n",
            "\n",
            "iteration # 69\n",
            "accuracy:  0.6067343197252563\n",
            "Cost: \n",
            " 0.6565862521417287 \n",
            "\n",
            "iteration # 70\n",
            "accuracy:  0.607044181062308\n",
            "Cost: \n",
            " 0.6562414276077444 \n",
            "\n",
            "iteration # 71\n",
            "accuracy:  0.6063728148320293\n",
            "Cost: \n",
            " 0.6567545269401613 \n",
            "\n",
            "iteration # 72\n",
            "accuracy:  0.6064244583882046\n",
            "Cost: \n",
            " 0.6565817316674335 \n",
            "\n",
            "iteration # 73\n",
            "accuracy:  0.6035582410204766\n",
            "Cost: \n",
            " 0.6604017908093309 \n",
            "\n",
            "iteration # 74\n",
            "accuracy:  0.6078962997392\n",
            "Cost: \n",
            " 0.6574156792760146 \n",
            "\n",
            "iteration # 75\n",
            "accuracy:  0.6089291708627056\n",
            "Cost: \n",
            " 0.655005081297353 \n",
            "\n",
            "iteration # 76\n",
            "accuracy:  0.6068117850595192\n",
            "Cost: \n",
            " 0.6554651852560485 \n",
            "\n",
            "iteration # 77\n",
            "accuracy:  0.6081028739639012\n",
            "Cost: \n",
            " 0.6558830885807577 \n",
            "\n",
            "iteration # 78\n",
            "accuracy:  0.5995816871949803\n",
            "Cost: \n",
            " 0.6607608908937459 \n",
            "\n",
            "iteration # 79\n",
            "accuracy:  0.605469052598962\n",
            "Cost: \n",
            " 0.6564403989649009 \n",
            "\n",
            "iteration # 80\n",
            "accuracy:  0.6027319441216722\n",
            "Cost: \n",
            " 0.6578269520209226 \n",
            "\n",
            "iteration # 81\n",
            "accuracy:  0.6071474681746585\n",
            "Cost: \n",
            " 0.6566208618908853 \n",
            "\n",
            "iteration # 82\n",
            "accuracy:  0.605107547705735\n",
            "Cost: \n",
            " 0.6574124085409995 \n",
            "\n",
            "iteration # 83\n",
            "accuracy:  0.6057014486017507\n",
            "Cost: \n",
            " 0.6567052141754256 \n",
            "\n",
            "iteration # 84\n",
            "accuracy:  0.6089291708627056\n",
            "Cost: \n",
            " 0.6566827353705589 \n",
            "\n",
            "iteration # 85\n",
            "accuracy:  0.6006920236527488\n",
            "Cost: \n",
            " 0.6652349499553468 \n",
            "\n",
            "iteration # 86\n",
            "accuracy:  0.6059338446045395\n",
            "Cost: \n",
            " 0.6561332952735551 \n",
            "\n",
            "iteration # 87\n",
            "accuracy:  0.6046169339220698\n",
            "Cost: \n",
            " 0.6568074913159572 \n",
            "\n",
            "iteration # 88\n",
            "accuracy:  0.5995042218607173\n",
            "Cost: \n",
            " 0.6649598220530771 \n",
            "\n",
            "iteration # 89\n",
            "accuracy:  0.6002014098690835\n",
            "Cost: \n",
            " 0.6632482935815761 \n",
            "\n",
            "iteration # 90\n",
            "accuracy:  0.6052883001523485\n",
            "Cost: \n",
            " 0.6566031221217588 \n",
            "\n",
            "iteration # 91\n",
            "accuracy:  0.6027835876778475\n",
            "Cost: \n",
            " 0.658101793494938 \n",
            "\n",
            "iteration # 92\n",
            "accuracy:  0.6010793503240633\n",
            "Cost: \n",
            " 0.6618099303111823 \n",
            "\n",
            "iteration # 93\n",
            "accuracy:  0.5985746378495623\n",
            "Cost: \n",
            " 0.6608019472678874 \n",
            "\n",
            "iteration # 94\n",
            "accuracy:  0.6033258450176879\n",
            "Cost: \n",
            " 0.6569100068070636 \n",
            "\n",
            "iteration # 95\n",
            "accuracy:  0.6028868747901981\n",
            "Cost: \n",
            " 0.6576673966290694 \n",
            "\n",
            "iteration # 96\n",
            "accuracy:  0.6028610530121105\n",
            "Cost: \n",
            " 0.6568444212994242 \n",
            "\n",
            "iteration # 97\n",
            "accuracy:  0.6032225579053374\n",
            "Cost: \n",
            " 0.6568570534887488 \n",
            "\n",
            "iteration # 98\n",
            "accuracy:  0.6022155085599195\n",
            "Cost: \n",
            " 0.657133452749978 \n",
            "\n",
            "iteration # 99\n",
            "accuracy:  0.6028094094559351\n",
            "Cost: \n",
            " 0.657211491512532 \n",
            "\n",
            "iteration # 100\n",
            "accuracy:  0.6057789139360137\n",
            "Cost: \n",
            " 0.6576913576521963 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "nn1=NeuralNetwork(x_train,y_train)\n",
        "nn1.fit(epochs=100,Backpropagate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3qLqPFQaZdi",
        "outputId": "c0b7ef01-5519-4a44-e146-bf0b301ed0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss : 0.6711490782867995\n",
            "Acc : 0.6020002409928907\n"
          ]
        }
      ],
      "source": [
        "Test_1=nn1.evaluate(x_cv,y_cv.reshape(-1,1))\n",
        "print('Loss :',Test_1[0])\n",
        "print('Acc :',Test_1[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK7a-qwJaZdi",
        "outputId": "637e120a-0f79-40cb-baf8-b0fd94dd948d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss : 0.6719181589588227\n",
            "Acc : 0.6041691770092782\n"
          ]
        }
      ],
      "source": [
        "Test_2=nn1.evaluate(x_test,y_test.reshape(-1,1))\n",
        "print('Loss :',Test_2[0])\n",
        "print('Acc :',Test_2[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-5QoYz8aZdj"
      },
      "source": [
        "# Bayiesn Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "nI6RJCHRaZdj"
      },
      "outputs": [],
      "source": [
        "class BayesianNeuralNetwork:\n",
        "    def __init__(self, x, y,vocabulary_size=21361,nodes_in_layer1=32, nodes_in_layer2=16, nodes_in_layer3=1, l_rate=0.1):\n",
        "        embedding_dim= 50\n",
        "        vocab_size = vocabulary_size +1 # Add 1 for the out-of-vocabulary token\n",
        "        self.embedding_weights = np.random.randn(vocab_size, embedding_dim)\n",
        "        # Define x, y\n",
        "        self.inputs_in_layer0 = self.embedding(x) # Layer 0\n",
        "        self.y = y.reshape(-1,1)\n",
        "\n",
        "        self.l_rate = l_rate  # Learning rate\n",
        "\n",
        "        # Define and set the number of neurons in each layer\n",
        "        self.nodes_in_layer1 = nodes_in_layer1\n",
        "        self.nodes_in_layer2 = nodes_in_layer2\n",
        "        self.nodes_in_layer3 = nodes_in_layer3\n",
        "\n",
        "        # Initialize weights and biases with smaller values using Gaussian distributions\n",
        "        self.thetas_layer0 = initialize_weights(self.inputs_in_layer0.shape[1] + 1, self.nodes_in_layer1, mu=0.001, sigma=0.01)\n",
        "        self.thetas_layer1 = initialize_weights(self.nodes_in_layer1 + 1, self.nodes_in_layer2, mu=0.001, sigma=0.01)\n",
        "        self.thetas_layer2 = initialize_weights(self.nodes_in_layer2 + 1, self.nodes_in_layer3, mu=0.001, sigma=0.01)\n",
        "\n",
        "        # Initialize prior distributions for weights\n",
        "        self.prior_mean_theta0 = np.zeros_like(self.thetas_layer0)\n",
        "        self.prior_mean_theta1 = np.zeros_like(self.thetas_layer1)\n",
        "        self.prior_mean_theta2 = np.zeros_like(self.thetas_layer2)\n",
        "\n",
        "        self.prior_variance_theta0 = np.ones_like(self.thetas_layer0)\n",
        "        self.prior_variance_theta1 = np.ones_like(self.thetas_layer1)\n",
        "        self.prior_variance_theta2 = np.ones_like(self.thetas_layer2)\n",
        "    def embedding(self, x):\n",
        "        self.embedded_input = self.embedding_weights[x]\n",
        "        pooled_embeddings = np.mean(self.embedded_input, axis=1)\n",
        "        return pooled_embeddings\n",
        "\n",
        "    def feedforward(self):\n",
        "        # Sample weights from their respective Gaussian distributions for each forward pass\n",
        "        self.Z1 = self.thetas_layer0[0] + np.dot(self.inputs_in_layer0, self.thetas_layer0[1:])\n",
        "        self.layer1 = relu(self.Z1)\n",
        "\n",
        "        self.Z2 = self.thetas_layer1[0] + np.dot(self.layer1, self.thetas_layer1[1:])\n",
        "        self.layer2 = relu(self.Z2)\n",
        "\n",
        "        self.Z3 = self.thetas_layer2[0] + np.dot(self.layer2, self.thetas_layer2[1:])\n",
        "        self.layer3 = sigmoid(self.Z3)\n",
        "\n",
        "        return self.layer3\n",
        "\n",
        "    def calculate_accuracy(self):\n",
        "        actual_output = self.y\n",
        "        predicted_output = self.layer3\n",
        "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
        "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
        "\n",
        "        # Compare predicted classes with actual classes\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def log_prior(self):\n",
        "        # Calculate log priors for weights using Gaussian distributions\n",
        "        log_prior_theta0 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta0) +\n",
        "                                   ((self.thetas_layer0 - self.prior_mean_theta0) ** 2) /\n",
        "                                   self.prior_variance_theta0).sum()\n",
        "\n",
        "        log_prior_theta1 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta1) +\n",
        "                                   ((self.thetas_layer1 - self.prior_mean_theta1) ** 2) /\n",
        "                                   self.prior_variance_theta1).sum()\n",
        "\n",
        "        log_prior_theta2 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta2) +\n",
        "                                   ((self.thetas_layer2 - self.prior_mean_theta2) ** 2) /\n",
        "                                   self.prior_variance_theta2).sum()\n",
        "\n",
        "        return log_prior_theta0 + log_prior_theta1 + log_prior_theta2\n",
        "\n",
        "    def log_likelihood(self):\n",
        "        # Compute log likelihood for Bernoulli distribution\n",
        "        self.n = self.inputs_in_layer0.shape[0]  # Number of training examples\n",
        "\n",
        "        # Calculate log-likelihood for Bernoulli likelihood\n",
        "        epsilon = 1e-10  # Small value to prevent log(0)\n",
        "        log_likelihood = np.sum(self.y * np.log(self.layer3 + epsilon) + (1 - self.y) * np.log(1 - self.layer3 + epsilon))\n",
        "\n",
        "        # Normalize log-likelihood by the number of training examples\n",
        "        log_likelihood /= -self.n\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def log_posterior(self):\n",
        "        # Compute log posterior using log prior and log likelihood\n",
        "        log_prior = self.log_prior()\n",
        "        log_likelihood = self.log_likelihood()\n",
        "        log_posterior = log_prior + log_likelihood\n",
        "        return log_posterior\n",
        "    def perform_MCMC(self,proposal_variance=0.01):\n",
        "         # Make a copy of the current weights for proposal\n",
        "        proposed_thetas_layer0 = np.copy(self.thetas_layer0)\n",
        "        proposed_thetas_layer1 = np.copy(self.thetas_layer1)\n",
        "        proposed_thetas_layer2 = np.copy(self.thetas_layer2)\n",
        "\n",
        "            # Perturb the weights for proposal (using a Gaussian random walk as an example)\n",
        "        proposed_thetas_layer0 += np.random.normal(0, proposal_variance, size=self.thetas_layer0.shape)\n",
        "        proposed_thetas_layer1 += np.random.normal(0, proposal_variance, size=self.thetas_layer1.shape)\n",
        "        proposed_thetas_layer2 += np.random.normal(0, proposal_variance, size=self.thetas_layer2.shape)\n",
        "\n",
        "            # Compute log-likelihoods for current and proposed weights\n",
        "        current_log_likelihood = self.log_likelihood()\n",
        "\n",
        "            # Compute log-posterior for the proposed weights\n",
        "        self.thetas_layer0 = proposed_thetas_layer0\n",
        "        self.thetas_layer1 = proposed_thetas_layer1\n",
        "        self.thetas_layer2 = proposed_thetas_layer2\n",
        "\n",
        "        proposed_log_likelihood = self.log_likelihood()\n",
        "        proposed_log_posterior = self.log_prior() + proposed_log_likelihood\n",
        "\n",
        "            # Accept or reject the proposal based on Metropolis-Hastings acceptance criterion\n",
        "        acceptance_ratio = np.exp(proposed_log_posterior - current_log_likelihood)\n",
        "        if np.random.uniform(0, 1) < acceptance_ratio:\n",
        "            # Accept the proposal\n",
        "            pass\n",
        "        else:\n",
        "            # Reject the proposal, revert weights to the previous state\n",
        "            self.thetas_layer0 = np.copy(proposed_thetas_layer0)\n",
        "            self.thetas_layer1 = np.copy(proposed_thetas_layer1)\n",
        "            self.thetas_layer2 = np.copy(proposed_thetas_layer2)\n",
        "    def fit(self,epochs):\n",
        "        losses=[]\n",
        "        for i in range(epochs):\n",
        "            self.feedforward()\n",
        "            error = self.log_likelihood()  # Compute log-likelihood as the error\n",
        "            losses.append(error)\n",
        "            self.perform_MCMC()\n",
        "            print(\"iteration #\",i+1)\n",
        "            print('accuracy: ',self.calculate_accuracy())\n",
        "            print(\"Cost: \\n\",error,\"\\n\")\n",
        "    def evaluate(self, x,y):\n",
        "        inputs_layer0 = self.embedding(x)\n",
        "        Z1 = self.thetas_layer0[0] + np.dot(inputs_layer0, self.thetas_layer0[1:])\n",
        "        layer1 = sigmoid(Z1)\n",
        "\n",
        "        Z2 = self.thetas_layer1[0] + np.dot(layer1, self.thetas_layer1[1:])\n",
        "        layer2 = sigmoid(Z2)\n",
        "\n",
        "        Z3 = self.thetas_layer2[0] + np.dot(layer2, self.thetas_layer2[1:])\n",
        "        layer3 = sigmoid(Z3)\n",
        "        loss= (1/inputs_layer0.shape[0]) * np.sum(-y * np.log(layer3) - (1 - y) * np.log(1 - layer3)) #cross entropy\n",
        "        actual_output=y\n",
        "        predicted_output=layer3\n",
        "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
        "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
        "\n",
        "        # Compare predicted classes with actual classes\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "        return loss,accuracy,layer3\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L67X9undaZdk"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWxLtzJPaZdl",
        "outputId": "8228703c-2272-4cd2-8009-8a4eef9168ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration # 1\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6926860173836177 \n",
            "\n",
            "iteration # 2\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.693164582387466 \n",
            "\n",
            "iteration # 3\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.693239430976369 \n",
            "\n",
            "iteration # 4\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6936303307315869 \n",
            "\n",
            "iteration # 5\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6932849795577652 \n",
            "\n",
            "iteration # 6\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6936284416142072 \n",
            "\n",
            "iteration # 7\n",
            "accuracy:  0.4571487592635629\n",
            "Cost: \n",
            " 0.6931842445264546 \n",
            "\n",
            "iteration # 8\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6930512167431293 \n",
            "\n",
            "iteration # 9\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6931141593851355 \n",
            "\n",
            "iteration # 10\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6937797322526289 \n",
            "\n",
            "iteration # 11\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6937116949814723 \n",
            "\n",
            "iteration # 12\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6936405964429638 \n",
            "\n",
            "iteration # 13\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6938338939504345 \n",
            "\n",
            "iteration # 14\n",
            "accuracy:  0.45541870013169106\n",
            "Cost: \n",
            " 0.6932523655435266 \n",
            "\n",
            "iteration # 15\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6936009464383649 \n",
            "\n",
            "iteration # 16\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6935244968491802 \n",
            "\n",
            "iteration # 17\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6939892748619617 \n",
            "\n",
            "iteration # 18\n",
            "accuracy:  0.4571487592635629\n",
            "Cost: \n",
            " 0.6933402980502518 \n",
            "\n",
            "iteration # 19\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6935274256035792 \n",
            "\n",
            "iteration # 20\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6929588787319695 \n",
            "\n",
            "iteration # 21\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6938185832794529 \n",
            "\n",
            "iteration # 22\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6940435250036489 \n",
            "\n",
            "iteration # 23\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6945168348800085 \n",
            "\n",
            "iteration # 24\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6939882782446823 \n",
            "\n",
            "iteration # 25\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6935172463769642 \n",
            "\n",
            "iteration # 26\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6944578510918072 \n",
            "\n",
            "iteration # 27\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.695754625005575 \n",
            "\n",
            "iteration # 28\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6956026106112191 \n",
            "\n",
            "iteration # 29\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6960688516957627 \n",
            "\n",
            "iteration # 30\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.696015594451262 \n",
            "\n",
            "iteration # 31\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6962721670051804 \n",
            "\n",
            "iteration # 32\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6975262108263185 \n",
            "\n",
            "iteration # 33\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6975205063992925 \n",
            "\n",
            "iteration # 34\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6982265539019475 \n",
            "\n",
            "iteration # 35\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6982255191529053 \n",
            "\n",
            "iteration # 36\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6983905508542956 \n",
            "\n",
            "iteration # 37\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6975163944722192 \n",
            "\n",
            "iteration # 38\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6975240024094442 \n",
            "\n",
            "iteration # 39\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6971613834632114 \n",
            "\n",
            "iteration # 40\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6967380437193874 \n",
            "\n",
            "iteration # 41\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.697072582391036 \n",
            "\n",
            "iteration # 42\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.69612176799444 \n",
            "\n",
            "iteration # 43\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.697030961380318 \n",
            "\n",
            "iteration # 44\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6976223766972063 \n",
            "\n",
            "iteration # 45\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6981101913480928 \n",
            "\n",
            "iteration # 46\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6982851892739557 \n",
            "\n",
            "iteration # 47\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6988732962955061 \n",
            "\n",
            "iteration # 48\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6987971069009284 \n",
            "\n",
            "iteration # 49\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6990345166173314 \n",
            "\n",
            "iteration # 50\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6995325870067923 \n",
            "\n",
            "iteration # 51\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.699030168625508 \n",
            "\n",
            "iteration # 52\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.699341707787715 \n",
            "\n",
            "iteration # 53\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6993147197841056 \n",
            "\n",
            "iteration # 54\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6998154182475009 \n",
            "\n",
            "iteration # 55\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6978247224825374 \n",
            "\n",
            "iteration # 56\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6973419879738998 \n",
            "\n",
            "iteration # 57\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.696508420738778 \n",
            "\n",
            "iteration # 58\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6965122536275077 \n",
            "\n",
            "iteration # 59\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6949232382673631 \n",
            "\n",
            "iteration # 60\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6952420092122461 \n",
            "\n",
            "iteration # 61\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.695769299387673 \n",
            "\n",
            "iteration # 62\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6953191713778066 \n",
            "\n",
            "iteration # 63\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6955295848956764 \n",
            "\n",
            "iteration # 64\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6970899920543219 \n",
            "\n",
            "iteration # 65\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6978304467296993 \n",
            "\n",
            "iteration # 66\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6967969270036206 \n",
            "\n",
            "iteration # 67\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.697193967354205 \n",
            "\n",
            "iteration # 68\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6972039239733817 \n",
            "\n",
            "iteration # 69\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6970642770028607 \n",
            "\n",
            "iteration # 70\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6968569710009581 \n",
            "\n",
            "iteration # 71\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.697296853935865 \n",
            "\n",
            "iteration # 72\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6972382003710201 \n",
            "\n",
            "iteration # 73\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6973795123455738 \n",
            "\n",
            "iteration # 74\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6976380749555473 \n",
            "\n",
            "iteration # 75\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6965572064838765 \n",
            "\n",
            "iteration # 76\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6960439743933932 \n",
            "\n",
            "iteration # 77\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6960703114207666 \n",
            "\n",
            "iteration # 78\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.695981881610138 \n",
            "\n",
            "iteration # 79\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6960597197619983 \n",
            "\n",
            "iteration # 80\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6962557167684458 \n",
            "\n",
            "iteration # 81\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6945091997742541 \n",
            "\n",
            "iteration # 82\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6943057322579859 \n",
            "\n",
            "iteration # 83\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6944625149929783 \n",
            "\n",
            "iteration # 84\n",
            "accuracy:  0.4573295117101764\n",
            "Cost: \n",
            " 0.6937068049355642 \n",
            "\n",
            "iteration # 85\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6938284186527109 \n",
            "\n",
            "iteration # 86\n",
            "accuracy:  0.5452268443205\n",
            "Cost: \n",
            " 0.6926083169335915 \n",
            "\n",
            "iteration # 87\n",
            "accuracy:  0.5068815038603558\n",
            "Cost: \n",
            " 0.6931076929510939 \n",
            "\n",
            "iteration # 88\n",
            "accuracy:  0.45738115526635165\n",
            "Cost: \n",
            " 0.6937117027287555 \n",
            "\n",
            "iteration # 89\n",
            "accuracy:  0.500787564231673\n",
            "Cost: \n",
            " 0.6931483569680699 \n",
            "\n",
            "iteration # 90\n",
            "accuracy:  0.47829679551733933\n",
            "Cost: \n",
            " 0.6932634291922927 \n",
            "\n",
            "iteration # 91\n",
            "accuracy:  0.5428770625145247\n",
            "Cost: \n",
            " 0.6918953394304574 \n",
            "\n",
            "iteration # 92\n",
            "accuracy:  0.5458465669946032\n",
            "Cost: \n",
            " 0.692641966554481 \n",
            "\n",
            "iteration # 93\n",
            "accuracy:  0.5438324683037674\n",
            "Cost: \n",
            " 0.6926051301806404 \n",
            "\n",
            "iteration # 94\n",
            "accuracy:  0.5486353190280683\n",
            "Cost: \n",
            " 0.6924993034701395 \n",
            "\n",
            "iteration # 95\n",
            "accuracy:  0.4595243628476257\n",
            "Cost: \n",
            " 0.6935423191426945 \n",
            "\n",
            "iteration # 96\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6940599558669388 \n",
            "\n",
            "iteration # 97\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6946903249304203 \n",
            "\n",
            "iteration # 98\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6964557646073766 \n",
            "\n",
            "iteration # 99\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6964903059028614 \n",
            "\n",
            "iteration # 100\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6960791165635278 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "bnn=BayesianNeuralNetwork(x_train,y_train)\n",
        "bnn.fit(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVb8GrNyaZdl",
        "outputId": "d8fddc6b-24a9-475e-89f8-1322f180b612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss : 0.7002329939377921\n",
            "Acc : 0.4624653572719605\n"
          ]
        }
      ],
      "source": [
        "bnn_test=bnn.evaluate(x_cv,y_cv.reshape(-1,1))\n",
        "print('Loss :',bnn_test[0])\n",
        "print('Acc :',bnn_test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ7gd1IXaZdm",
        "outputId": "3b03a91c-ada2-4530-b4a2-36ae324749ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss : 0.7006930228990389\n",
            "Acc : 0.45897096035666946\n"
          ]
        }
      ],
      "source": [
        "bnn_test_1=bnn.evaluate(x_test,y_test.reshape(-1,1))\n",
        "print('Loss :',bnn_test_1[0])\n",
        "print('Acc :',bnn_test_1[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cU6OgmythfRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}