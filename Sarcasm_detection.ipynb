{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MPad2BhaZdQ"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WBjD9s-bbJBB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ls2eRLAaZdY"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GPJkqtNmaZdY"
      },
      "outputs": [],
      "source": [
        "features = np.load('clean_data/features.npy')\n",
        "labels = np.load('clean_data/labels.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCcQlfFIQ8hv"
      },
      "source": [
        "## Spilt Train, Cross Validation and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEuJwRv5Q9ao",
        "outputId": "15ca5805-e6db-49e9-b47a-c2e593e2d747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(33195, 33)\n",
            "(11065, 33)\n",
            "(11065, 33)\n",
            "(33195,)\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test_full, y_train, y_test_full = train_test_split(features, labels, train_size=0.6, random_state=1)\n",
        "x_test, x_cv, y_test, y_cv = train_test_split(x_test_full, y_test_full, train_size=0.5, random_state=1)\n",
        "print(x_train.shape)\n",
        "print(x_cv.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmWITO56aZdb"
      },
      "source": [
        "# Back Propogation, R  Back Propogation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz-t_T19aZdb"
      },
      "source": [
        "# Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "QkHnoDOPaZdb"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He1EtT6IaZdc"
      },
      "source": [
        "# Function to initialize weights as Gaussian distributions with specific mu and sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "9UaVnEnIaZdc"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(nodes_in, nodes_out, mu=0, sigma=0.1):\n",
        "    return np.random.normal(mu, sigma, size=(nodes_in, nodes_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkuPO21eaZdd"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "OxcIxptnaZdd"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, x, y,vocabulary_size=21361, nodes_in_layer1=32, nodes_in_layer2=16, nodes_in_layer3=1, l_rate=0.001):\n",
        "        embedding_dim =50\n",
        "        self.x=x\n",
        "        vocab_size = vocabulary_size +1 # Add 1 for the out-of-vocabulary token\n",
        "        self.embedding_weights = np.random.randn(vocab_size, embedding_dim)\n",
        "        # define x, y\n",
        "        self.inputs_in_layer0 =self.embedding(self.x)\n",
        "        self.y = y.reshape(-1, 1)  # reshape y to be a column vector\n",
        "\n",
        "        self.l_rate = l_rate  # learning rate\n",
        "\n",
        "        # define and set the number of neurons in each layer\n",
        "        self.nodes_in_layer1 = nodes_in_layer1\n",
        "        self.nodes_in_layer2 = nodes_in_layer2\n",
        "        self.nodes_in_layer3 = nodes_in_layer3\n",
        "\n",
        "        # initialize the weights (theta) matrices\n",
        "        # self.thetas_layer0 = np.random.rand(self.inputs_in_layer0.shape[1] + 1, self.nodes_in_layer1)\n",
        "        # self.thetas_layer1 = np.random.rand(self.nodes_in_layer1 + 1, self.nodes_in_layer2)\n",
        "        # self.thetas_layer2 = np.random.rand(self.nodes_in_layer2 + 1, self.nodes_in_layer3)\n",
        "        self.thetas_layer0 = np.random.randn(self.inputs_in_layer0.shape[1] + 1, self.nodes_in_layer1) * np.sqrt(2 / (self.inputs_in_layer0.shape[1] + 1))\n",
        "        self.thetas_layer1 = np.random.randn(self.nodes_in_layer1 + 1, self.nodes_in_layer2) * np.sqrt(2 / (self.nodes_in_layer1 + 1))\n",
        "        self.thetas_layer2 = np.random.randn(self.nodes_in_layer2 + 1, self.nodes_in_layer3) * np.sqrt(2 / (self.nodes_in_layer2 + 1))\n",
        "        self.epsilon = 1e-5\n",
        "        self.momentum = 0.9\n",
        "        self.gamma1 = np.ones(nodes_in_layer1)\n",
        "        self.beta1 = np.zeros(nodes_in_layer1)\n",
        "        self.gamma2 = np.ones(nodes_in_layer2)\n",
        "        self.beta2 = np.zeros(nodes_in_layer2)\n",
        "    def batch_normalize(self, input_data, gamma, beta):\n",
        "            # Calculate mean and variance\n",
        "            batch_mean = np.mean(input_data, axis=0)\n",
        "            batch_var = np.var(input_data, axis=0)\n",
        "\n",
        "            # Normalize\n",
        "            normalized_data = (input_data - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
        "\n",
        "            # Scale and shift\n",
        "            scaled_and_shifted_data = gamma * normalized_data + beta\n",
        "\n",
        "            return scaled_and_shifted_data\n",
        "\n",
        "    def feedforward(self):\n",
        "        #compute all the nodes (a1, a2, a3, a4) in layer1\n",
        "        n = self.inputs_in_layer0.shape[0]\n",
        "\n",
        "        self.Z1 = self.thetas_layer0[0] + np.dot(self.inputs_in_layer0, self.thetas_layer0[1:])\n",
        "        self.layer1 = relu(self.Z1)  #values of a1, a2, a3, a4 in layer 1\n",
        "        self.normalized_data_layer1 = self.batch_normalize(self.layer1, self.gamma1, self.beta1)\n",
        "        #compute all the nodes (a1, a2, a3) in layer2\n",
        "        self.Z2 = self.thetas_layer1[0] + np.dot(self.normalized_data_layer1, self.thetas_layer1[1:])\n",
        "        self.layer2 = relu(self.Z2)  #values of a1, a2, a3 in layer 2\n",
        "        self.normalized_data_layer2 = self.batch_normalize(self.layer2, self.gamma2, self.beta2)\n",
        "        #compute all the nodes (a1) in layer3\n",
        "        self.Z3 = self.thetas_layer2[0] + np.dot(self.normalized_data_layer2, self.thetas_layer2[1:])\n",
        "        self.layer3 = sigmoid(self.Z3)  #output layer\n",
        "\n",
        "        return self.layer3\n",
        "\n",
        "    def cost_func(self):\n",
        "        epsilon = 1e-15  # Small constant to avoid log(0)\n",
        "        self.n = self.inputs_in_layer0.shape[0]  # number of training examples\n",
        "        self.cost = (1/self.n) * np.sum(-self.y * np.log(self.layer3 + epsilon) - (1 - self.y) * np.log(1 - self.layer3 + epsilon))\n",
        "        return self.cost\n",
        "    def embedding(self, x):\n",
        "            self.embedded_input = self.embedding_weights[x]\n",
        "            pooled_embeddings = np.mean(self.embedded_input, axis=1)\n",
        "            return pooled_embeddings\n",
        "    def calculate_accuracy(self ):\n",
        "        actual_output=self.y\n",
        "        predicted_output=self.layer3\n",
        "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
        "        predicted_classes = (predicted_output >= 0.5 ).astype(int)\n",
        "\n",
        "        # Compare predicted classes with actual classes\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def Rbackprop(self):\n",
        "        # Define RProp parameters\n",
        "        delta0 = self.l_rate  # Initial update value\n",
        "        delta_max = 60  # Maximum update value\n",
        "        delta_min = 1e-6  # Minimum update value\n",
        "        eta_plus = 1.5  # Increase factor\n",
        "        eta_minus = 0.5  # Decrease factor\n",
        "        epsilon = 1e-15\n",
        "         #dervative of E with respect to theta and bias in layer2\n",
        "        self.dE_dlayer3 = (1/self.n) * (self.layer3-self.y)/(self.layer3*(1-self.layer3)+epsilon)\n",
        "        self.dE_dZ3 = np.multiply(self.dE_dlayer3, (sigmoid(self.Z3)* (1-sigmoid(self.Z3))))\n",
        "        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n",
        "        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n",
        "\n",
        "        #dervative of E with respect to theta and bias in layer1\n",
        "        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n",
        "        self.dE_dZ2 = np.multiply(self.dE_dlayer2, relu_derivative(self.Z2))\n",
        "        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n",
        "        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n",
        "\n",
        "\n",
        "        #dervative of E with respect to theta and bias in layer0\n",
        "        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n",
        "        self.dE_dZ1 = np.multiply(self.dE_dlayer1, relu_derivative(self.Z1))\n",
        "        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n",
        "        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n",
        "\n",
        "        # Initialize RProp update values\n",
        "        if not hasattr(self, 'prev_dE_dtheta2'):\n",
        "            self.prev_dE_dtheta2 = np.zeros_like(self.dE_dtheta2)\n",
        "            self.delta_theta2 = np.full_like(self.dE_dtheta2, delta0)\n",
        "        else:\n",
        "            self.delta_theta2 = np.where(self.dE_dtheta2 * self.prev_dE_dtheta2 > 0,\n",
        "                                        np.minimum(self.delta_theta2 * eta_plus, delta_max),\n",
        "                                        np.maximum(self.delta_theta2 * eta_minus, delta_min))\n",
        "        self.prev_dE_dtheta2 = self.dE_dtheta2\n",
        "\n",
        "        if not hasattr(self, 'prev_dE_dtheta1'):\n",
        "            self.prev_dE_dtheta1 = np.zeros_like(self.dE_dtheta1)\n",
        "            self.delta_theta1 = np.full_like(self.dE_dtheta1, delta0)\n",
        "        else:\n",
        "            self.delta_theta1 = np.where(self.dE_dtheta1 * self.prev_dE_dtheta1 > 0,\n",
        "                                        np.minimum(self.delta_theta1 * eta_plus, delta_max),\n",
        "                                        np.maximum(self.delta_theta1 * eta_minus, delta_min))\n",
        "        self.prev_dE_dtheta1 = self.dE_dtheta1\n",
        "\n",
        "        if not hasattr(self, 'prev_dE_dtheta0'):\n",
        "            self.prev_dE_dtheta0 = np.zeros_like(self.dE_dtheta0)\n",
        "            self.delta_theta0 = np.full_like(self.dE_dtheta0, delta0)\n",
        "        else:\n",
        "            self.delta_theta0 = np.where(self.dE_dtheta0 * self.prev_dE_dtheta0 > 0,\n",
        "                                        np.minimum(self.delta_theta0 * eta_plus, delta_max),\n",
        "                                        np.maximum(self.delta_theta0 * eta_minus, delta_min))\n",
        "        self.prev_dE_dtheta0 = self.dE_dtheta0\n",
        "\n",
        "        # Updating theta using RProp in layers 2, 1, and 0\n",
        "        self.thetas_layer2[1:] -= np.sign(self.dE_dtheta2) * self.delta_theta2\n",
        "        self.thetas_layer1[1:] -= np.sign(self.dE_dtheta1) * self.delta_theta1\n",
        "        self.thetas_layer0[1:] -= np.sign(self.dE_dtheta0) * self.delta_theta0\n",
        "\n",
        "        # Updating bias using RProp in layers 2, 1, and 0\n",
        "        self.thetas_layer2[0] -= np.sign(self.dE_dbias2) * self.delta_theta2[0]\n",
        "        self.thetas_layer1[0] -= np.sign(self.dE_dbias1) * self.delta_theta1[0]\n",
        "        self.thetas_layer0[0] -= np.sign(self.dE_dbias0) * self.delta_theta0[0]\n",
        "        return self\n",
        "\n",
        "    def backprop(self):\n",
        "        epsilon=1e-15\n",
        "        #dervative of E with respect to theta and bias in layer2\n",
        "        self.dE_dlayer3 = (1/self.n) * (self.layer3-self.y)/(self.layer3*(1-self.layer3)+epsilon)\n",
        "        self.dE_dZ3 = np.multiply(self.dE_dlayer3, (sigmoid(self.Z3)* (1-sigmoid(self.Z3))))\n",
        "        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n",
        "        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n",
        "\n",
        "        #dervative of E with respect to theta and bias in layer1\n",
        "        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n",
        "        self.dE_dZ2 = np.multiply(self.dE_dlayer2, relu_derivative(self.Z2))\n",
        "        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n",
        "        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n",
        "        # Gradient for batch normalization parameters in layer2\n",
        "        dL_dgamma2 = np.sum(self.dE_dZ2 * self.normalized_data_layer2, axis=0)\n",
        "        dL_dbeta2 = np.sum(self.dE_dZ2, axis=0)\n",
        "\n",
        "        #dervative of E with respect to theta and bias in layer0\n",
        "        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n",
        "        self.dE_dZ1 = np.multiply(self.dE_dlayer1, relu_derivative(self.Z1))\n",
        "        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n",
        "        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n",
        "        # Gradient for batch normalization parameters in layer1\n",
        "        dL_dgamma1 = np.sum(self.dE_dZ1 * self.normalized_data_layer1, axis=0)\n",
        "        dL_dbeta1 = np.sum(self.dE_dZ1, axis=0)\n",
        "        #updating theta using gradient descent in layers 2, 1, and 0\n",
        "        self.thetas_layer2[1:] = self.thetas_layer2[1:] - self.l_rate * self.dE_dtheta2\n",
        "        self.thetas_layer1[1:] = self.thetas_layer1[1:] - self.l_rate * self.dE_dtheta1\n",
        "        self.thetas_layer0[1:] = self.thetas_layer0[1:] - self.l_rate * self.dE_dtheta0\n",
        "        # self.de_wegihts = np.dot(self.embedded_input.T, self.dE_dZ1)\n",
        "        # self.embedding_weights = -self.embedding_weights - self.l_rate * self.de_wegihts\n",
        "        # Update batch normalization parameters using gradient descent\n",
        "        self.gamma1 -= self.l_rate * dL_dgamma1\n",
        "        self.beta1 -= self.l_rate * dL_dbeta1\n",
        "        self.gamma2 -= self.l_rate * dL_dgamma2\n",
        "        self.beta2 -= self.l_rate * dL_dbeta2\n",
        "\n",
        "        #updating bias using gradient descent in layers 2, 1, and 0\n",
        "        self.thetas_layer2[0] = self.thetas_layer2[0] - self.l_rate * self.dE_dbias2\n",
        "        self.thetas_layer1[0] = self.thetas_layer1[0] - self.l_rate * self.dE_dbias1\n",
        "        self.thetas_layer0[0] = self.thetas_layer0[0] - self.l_rate * self.dE_dbias0\n",
        "        return self\n",
        "    def fit(self,epochs,Backpropagate):\n",
        "        losses=[]\n",
        "        for i in range(epochs):\n",
        "            self.feedforward()\n",
        "            error=self.cost_func()\n",
        "            losses.append(error)\n",
        "            if Backpropagate==True:\n",
        "                self.backprop()\n",
        "            else:\n",
        "                self.Rbackprop()\n",
        "            print(\"iteration #\",i+1)\n",
        "            print('accuracy: ',self.calculate_accuracy())\n",
        "            print(\"Cost: \\n\",error,\"\\n\")\n",
        "    def evaluate(self, x,y):\n",
        "        epsilon=1e-15\n",
        "        inputs_layer0 = self.embedding(x)\n",
        "        Z1 = self.thetas_layer0[0] + np.dot(inputs_layer0, self.thetas_layer0[1:])\n",
        "        layer1 = relu(Z1)\n",
        "        layer1 = self.batch_normalize(layer1, self.gamma1, self.beta1)\n",
        "        Z2 = self.thetas_layer1[0] + np.dot(layer1, self.thetas_layer1[1:])\n",
        "        layer2 = relu(Z2)\n",
        "        layer2=self.batch_normalize(layer2,self.gamma2,self.beta2)\n",
        "        Z3 = self.thetas_layer2[0] + np.dot(layer2, self.thetas_layer2[1:])\n",
        "        layer3 = sigmoid(Z3)\n",
        "        loss= (1/inputs_layer0.shape[0]) * np.sum(-y * np.log(layer3) - (1 - y) * np.log(1 - layer3+epsilon)) #cross entropy\n",
        "        actual_output=y\n",
        "        predicted_output=layer3\n",
        "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
        "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
        "\n",
        "        # Compare predicted classes with actual classes\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "        return loss,accuracy,layer3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmIcWDFoaZdf"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jam-8qoTaZdg",
        "outputId": "b4751c2c-1124-4ae1-dad4-6650df39aa41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration # 1\n",
            "accuracy:  0.5037505648441031\n",
            "Cost: \n",
            " 0.8301520162663414 \n",
            "\n",
            "iteration # 2\n",
            "accuracy:  0.5129386955866847\n",
            "Cost: \n",
            " 0.8236464463401286 \n",
            "\n",
            "iteration # 3\n",
            "accuracy:  0.5161319475824673\n",
            "Cost: \n",
            " 0.8184885113757765 \n",
            "\n",
            "iteration # 4\n",
            "accuracy:  0.5187528242205152\n",
            "Cost: \n",
            " 0.813663892884398 \n",
            "\n",
            "iteration # 5\n",
            "accuracy:  0.5190239493899683\n",
            "Cost: \n",
            " 0.808784739939057 \n",
            "\n",
            "iteration # 6\n",
            "accuracy:  0.5187829492393433\n",
            "Cost: \n",
            " 0.8038793323320171 \n",
            "\n",
            "iteration # 7\n",
            "accuracy:  0.5219762012351258\n",
            "Cost: \n",
            " 0.7989493481866173 \n",
            "\n",
            "iteration # 8\n",
            "accuracy:  0.5216147010091882\n",
            "Cost: \n",
            " 0.7940160456158043 \n",
            "\n",
            "iteration # 9\n",
            "accuracy:  0.5226088266305167\n",
            "Cost: \n",
            " 0.7891774005926893 \n",
            "\n",
            "iteration # 10\n",
            "accuracy:  0.5238138273836421\n",
            "Cost: \n",
            " 0.7844445764374331 \n",
            "\n",
            "iteration # 11\n",
            "accuracy:  0.5247778279861425\n",
            "Cost: \n",
            " 0.779915819753001 \n",
            "\n",
            "iteration # 12\n",
            "accuracy:  0.5262539539087212\n",
            "Cost: \n",
            " 0.7757149815532908 \n",
            "\n",
            "iteration # 13\n",
            "accuracy:  0.5264648290405182\n",
            "Cost: \n",
            " 0.7717162335483864 \n",
            "\n",
            "iteration # 14\n",
            "accuracy:  0.5274288296430185\n",
            "Cost: \n",
            " 0.7679755653557987 \n",
            "\n",
            "iteration # 15\n",
            "accuracy:  0.5280313300195812\n",
            "Cost: \n",
            " 0.7644939754446186 \n",
            "\n",
            "iteration # 16\n",
            "accuracy:  0.5280614550384094\n",
            "Cost: \n",
            " 0.7612428317392129 \n",
            "\n",
            "iteration # 17\n",
            "accuracy:  0.5285735803584877\n",
            "Cost: \n",
            " 0.7582259830213582 \n",
            "\n",
            "iteration # 18\n",
            "accuracy:  0.5296279560174725\n",
            "Cost: \n",
            " 0.7554497144499538 \n",
            "\n",
            "iteration # 19\n",
            "accuracy:  0.5285434553396596\n",
            "Cost: \n",
            " 0.7529159661015365 \n",
            "\n",
            "iteration # 20\n",
            "accuracy:  0.5286639554149721\n",
            "Cost: \n",
            " 0.7505600373643122 \n",
            "\n",
            "iteration # 21\n",
            "accuracy:  0.5300195812622382\n",
            "Cost: \n",
            " 0.748344488327126 \n",
            "\n",
            "iteration # 22\n",
            "accuracy:  0.5300195812622382\n",
            "Cost: \n",
            " 0.7462791220601067 \n",
            "\n",
            "iteration # 23\n",
            "accuracy:  0.5303208314505197\n",
            "Cost: \n",
            " 0.7443194309702872 \n",
            "\n",
            "iteration # 24\n",
            "accuracy:  0.530170206356379\n",
            "Cost: \n",
            " 0.742479430440425 \n",
            "\n",
            "iteration # 25\n",
            "accuracy:  0.5312547070341919\n",
            "Cost: \n",
            " 0.7407395723488189 \n",
            "\n",
            "iteration # 26\n",
            "accuracy:  0.5318270823919264\n",
            "Cost: \n",
            " 0.7390709326471329 \n",
            "\n",
            "iteration # 27\n",
            "accuracy:  0.5320680825425516\n",
            "Cost: \n",
            " 0.7375082336037437 \n",
            "\n",
            "iteration # 28\n",
            "accuracy:  0.5313149570718482\n",
            "Cost: \n",
            " 0.7360586109376748 \n",
            "\n",
            "iteration # 29\n",
            "accuracy:  0.5324295827684893\n",
            "Cost: \n",
            " 0.7346666788082004 \n",
            "\n",
            "iteration # 30\n",
            "accuracy:  0.5329718331073957\n",
            "Cost: \n",
            " 0.7333608576701016 \n",
            "\n",
            "iteration # 31\n",
            "accuracy:  0.5331525832203645\n",
            "Cost: \n",
            " 0.7321014508245304 \n",
            "\n",
            "iteration # 32\n",
            "accuracy:  0.5334237083898178\n",
            "Cost: \n",
            " 0.7309015544174757 \n",
            "\n",
            "iteration # 33\n",
            "accuracy:  0.5329115830697394\n",
            "Cost: \n",
            " 0.7297704917026866 \n",
            "\n",
            "iteration # 34\n",
            "accuracy:  0.533242958276849\n",
            "Cost: \n",
            " 0.7286648593892668 \n",
            "\n",
            "iteration # 35\n",
            "accuracy:  0.5337550835969272\n",
            "Cost: \n",
            " 0.7276064973678666 \n",
            "\n",
            "iteration # 36\n",
            "accuracy:  0.534146708841693\n",
            "Cost: \n",
            " 0.7265829648747084 \n",
            "\n",
            "iteration # 37\n",
            "accuracy:  0.5347190841994276\n",
            "Cost: \n",
            " 0.7255972955108987 \n",
            "\n",
            "iteration # 38\n",
            "accuracy:  0.5356529597830999\n",
            "Cost: \n",
            " 0.7246552706695815 \n",
            "\n",
            "iteration # 39\n",
            "accuracy:  0.5362855851784908\n",
            "Cost: \n",
            " 0.7237707503529305 \n",
            "\n",
            "iteration # 40\n",
            "accuracy:  0.5363759602349751\n",
            "Cost: \n",
            " 0.722904473743279 \n",
            "\n",
            "iteration # 41\n",
            "accuracy:  0.5359542099713812\n",
            "Cost: \n",
            " 0.7220303515604718 \n",
            "\n",
            "iteration # 42\n",
            "accuracy:  0.5377315860822413\n",
            "Cost: \n",
            " 0.721149967549496 \n",
            "\n",
            "iteration # 43\n",
            "accuracy:  0.5391173369483356\n",
            "Cost: \n",
            " 0.7202989892462637 \n",
            "\n",
            "iteration # 44\n",
            "accuracy:  0.5390269618918512\n",
            "Cost: \n",
            " 0.719493829278128 \n",
            "\n",
            "iteration # 45\n",
            "accuracy:  0.5397499623437265\n",
            "Cost: \n",
            " 0.7187130949078994 \n",
            "\n",
            "iteration # 46\n",
            "accuracy:  0.5401114625696641\n",
            "Cost: \n",
            " 0.7179462082167171 \n",
            "\n",
            "iteration # 47\n",
            "accuracy:  0.5403223377014611\n",
            "Cost: \n",
            " 0.7172179757225886 \n",
            "\n",
            "iteration # 48\n",
            "accuracy:  0.5416478385298991\n",
            "Cost: \n",
            " 0.7165033481078112 \n",
            "\n",
            "iteration # 49\n",
            "accuracy:  0.54228046392529\n",
            "Cost: \n",
            " 0.7158026051285906 \n",
            "\n",
            "iteration # 50\n",
            "accuracy:  0.5437264648290405\n",
            "Cost: \n",
            " 0.7151060743343973 \n",
            "\n",
            "iteration # 51\n",
            "accuracy:  0.5437867148666968\n",
            "Cost: \n",
            " 0.7144169103190601 \n",
            "\n",
            "iteration # 52\n",
            "accuracy:  0.5433348395842748\n",
            "Cost: \n",
            " 0.7137439919805296 \n",
            "\n",
            "iteration # 53\n",
            "accuracy:  0.5445097153185721\n",
            "Cost: \n",
            " 0.7130777495043827 \n",
            "\n",
            "iteration # 54\n",
            "accuracy:  0.543154089471306\n",
            "Cost: \n",
            " 0.7124228193563346 \n",
            "\n",
            "iteration # 55\n",
            "accuracy:  0.5435457147160717\n",
            "Cost: \n",
            " 0.7117852287321774 \n",
            "\n",
            "iteration # 56\n",
            "accuracy:  0.5435758397348999\n",
            "Cost: \n",
            " 0.7111570609118176 \n",
            "\n",
            "iteration # 57\n",
            "accuracy:  0.5445398403374002\n",
            "Cost: \n",
            " 0.710546568040114 \n",
            "\n",
            "iteration # 58\n",
            "accuracy:  0.5453230908269318\n",
            "Cost: \n",
            " 0.7099490927895792 \n",
            "\n",
            "iteration # 59\n",
            "accuracy:  0.5462569664106041\n",
            "Cost: \n",
            " 0.7093601657178035 \n",
            "\n",
            "iteration # 60\n",
            "accuracy:  0.5465582165988854\n",
            "Cost: \n",
            " 0.7087758601864945 \n",
            "\n",
            "iteration # 61\n",
            "accuracy:  0.5473414670884169\n",
            "Cost: \n",
            " 0.7082035771520327 \n",
            "\n",
            "iteration # 62\n",
            "accuracy:  0.5481849676156048\n",
            "Cost: \n",
            " 0.7076526338778366 \n",
            "\n",
            "iteration # 63\n",
            "accuracy:  0.548004217502636\n",
            "Cost: \n",
            " 0.7071087065226727 \n",
            "\n",
            "iteration # 64\n",
            "accuracy:  0.5483958427474017\n",
            "Cost: \n",
            " 0.7065665662423545 \n",
            "\n",
            "iteration # 65\n",
            "accuracy:  0.5493899683687302\n",
            "Cost: \n",
            " 0.7060331682675133 \n",
            "\n",
            "iteration # 66\n",
            "accuracy:  0.5495405934628709\n",
            "Cost: \n",
            " 0.7055076078001644 \n",
            "\n",
            "iteration # 67\n",
            "accuracy:  0.5498418436511523\n",
            "Cost: \n",
            " 0.7049973709870969 \n",
            "\n",
            "iteration # 68\n",
            "accuracy:  0.5499322187076366\n",
            "Cost: \n",
            " 0.7044939409799905 \n",
            "\n",
            "iteration # 69\n",
            "accuracy:  0.5500828438017774\n",
            "Cost: \n",
            " 0.7039969503769712 \n",
            "\n",
            "iteration # 70\n",
            "accuracy:  0.5509865943666215\n",
            "Cost: \n",
            " 0.7034992399592173 \n",
            "\n",
            "iteration # 71\n",
            "accuracy:  0.5511974694984184\n",
            "Cost: \n",
            " 0.7030111962346706 \n",
            "\n",
            "iteration # 72\n",
            "accuracy:  0.5516192197620123\n",
            "Cost: \n",
            " 0.7025356369329486 \n",
            "\n",
            "iteration # 73\n",
            "accuracy:  0.5521915951197469\n",
            "Cost: \n",
            " 0.7020657803276417 \n",
            "\n",
            "iteration # 74\n",
            "accuracy:  0.5530652206657629\n",
            "Cost: \n",
            " 0.7016026932140554 \n",
            "\n",
            "iteration # 75\n",
            "accuracy:  0.5536978460611538\n",
            "Cost: \n",
            " 0.7011485252720344 \n",
            "\n",
            "iteration # 76\n",
            "accuracy:  0.5544509715318572\n",
            "Cost: \n",
            " 0.7007034007100664 \n",
            "\n",
            "iteration # 77\n",
            "accuracy:  0.5546618466636541\n",
            "Cost: \n",
            " 0.7002668796726137 \n",
            "\n",
            "iteration # 78\n",
            "accuracy:  0.5553547220967013\n",
            "Cost: \n",
            " 0.6998350056440579 \n",
            "\n",
            "iteration # 79\n",
            "accuracy:  0.5551739719837325\n",
            "Cost: \n",
            " 0.6994056365962251 \n",
            "\n",
            "iteration # 80\n",
            "accuracy:  0.5556258472661545\n",
            "Cost: \n",
            " 0.6989804310963992 \n",
            "\n",
            "iteration # 81\n",
            "accuracy:  0.5559572224732641\n",
            "Cost: \n",
            " 0.6985612386130811 \n",
            "\n",
            "iteration # 82\n",
            "accuracy:  0.556589847868655\n",
            "Cost: \n",
            " 0.6981450434667219 \n",
            "\n",
            "iteration # 83\n",
            "accuracy:  0.556168097605061\n",
            "Cost: \n",
            " 0.6977315356779292 \n",
            "\n",
            "iteration # 84\n",
            "accuracy:  0.556378972736858\n",
            "Cost: \n",
            " 0.6973233853511491 \n",
            "\n",
            "iteration # 85\n",
            "accuracy:  0.5571019731887332\n",
            "Cost: \n",
            " 0.6969185956014632 \n",
            "\n",
            "iteration # 86\n",
            "accuracy:  0.5570718481699051\n",
            "Cost: \n",
            " 0.6965242562130668 \n",
            "\n",
            "iteration # 87\n",
            "accuracy:  0.5574634734146708\n",
            "Cost: \n",
            " 0.6961386653386922 \n",
            "\n",
            "iteration # 88\n",
            "accuracy:  0.5580358487724055\n",
            "Cost: \n",
            " 0.6957556737430921 \n",
            "\n",
            "iteration # 89\n",
            "accuracy:  0.5580659737912336\n",
            "Cost: \n",
            " 0.695374325536232 \n",
            "\n",
            "iteration # 90\n",
            "accuracy:  0.5580057237535774\n",
            "Cost: \n",
            " 0.6949966915753962 \n",
            "\n",
            "iteration # 91\n",
            "accuracy:  0.5586684741677963\n",
            "Cost: \n",
            " 0.6946245432973113 \n",
            "\n",
            "iteration # 92\n",
            "accuracy:  0.5587889742431089\n",
            "Cost: \n",
            " 0.6942610734859691 \n",
            "\n",
            "iteration # 93\n",
            "accuracy:  0.5586082241301401\n",
            "Cost: \n",
            " 0.6939028066305626 \n",
            "\n",
            "iteration # 94\n",
            "accuracy:  0.5595119746949841\n",
            "Cost: \n",
            " 0.6935467347292252 \n",
            "\n",
            "iteration # 95\n",
            "accuracy:  0.5591203494502184\n",
            "Cost: \n",
            " 0.6931916563090974 \n",
            "\n",
            "iteration # 96\n",
            "accuracy:  0.5597228498267811\n",
            "Cost: \n",
            " 0.6928364905526476 \n",
            "\n",
            "iteration # 97\n",
            "accuracy:  0.5604157252598283\n",
            "Cost: \n",
            " 0.6924827339278563 \n",
            "\n",
            "iteration # 98\n",
            "accuracy:  0.5604157252598283\n",
            "Cost: \n",
            " 0.6921292132046991 \n",
            "\n",
            "iteration # 99\n",
            "accuracy:  0.5608977255610785\n",
            "Cost: \n",
            " 0.6917749850938286 \n",
            "\n",
            "iteration # 100\n",
            "accuracy:  0.5612592257870161\n",
            "Cost: \n",
            " 0.6914219625930473 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "nn=NeuralNetwork(x_train,y_train,l_rate=0.1)\n",
        "nn.fit(epochs=100,Backpropagate=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ojw8bg1aZdg",
        "outputId": "de7fbef2-8dff-42c3-9ba8-d5a83f4e091e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.6975919037125433\n",
            "Acc : 0.559240849525531\n"
          ]
        }
      ],
      "source": [
        "test=nn.evaluate(x_cv,y_cv.reshape(-1,1))\n",
        "print('Loss :',test[0])\n",
        "print('Acc :',test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C5S5ElwaZdh",
        "outputId": "896f9eb4-0c96-43f8-9853-6917ed580de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.6876555824734552\n",
            "Acc : 0.5645729778581111\n"
          ]
        }
      ],
      "source": [
        "Test=nn.evaluate(x_test,y_test.reshape(-1,1))\n",
        "print('Loss :',Test[0])\n",
        "print('Acc :',Test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKHWV5RNaZdh",
        "outputId": "529d6112-52d6-43ce-e809-50781e7818f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration # 1\n",
            "accuracy:  0.4745292965808104\n",
            "Cost: \n",
            " 0.9350743817667224 \n",
            "\n",
            "iteration # 2\n",
            "accuracy:  0.479289049555656\n",
            "Cost: \n",
            " 0.9187708257326423 \n",
            "\n",
            "iteration # 3\n",
            "accuracy:  0.48293417683386053\n",
            "Cost: \n",
            " 0.8979709868336968 \n",
            "\n",
            "iteration # 4\n",
            "accuracy:  0.4915198071998795\n",
            "Cost: \n",
            " 0.8756976035582067 \n",
            "\n",
            "iteration # 5\n",
            "accuracy:  0.506612441632776\n",
            "Cost: \n",
            " 0.8579945029119508 \n",
            "\n",
            "iteration # 6\n",
            "accuracy:  0.5208917005573128\n",
            "Cost: \n",
            " 0.8496793426484547 \n",
            "\n",
            "iteration # 7\n",
            "accuracy:  0.5320078325048954\n",
            "Cost: \n",
            " 0.8733054688595563 \n",
            "\n",
            "iteration # 8\n",
            "accuracy:  0.5350805844253652\n",
            "Cost: \n",
            " 0.946629827459773 \n",
            "\n",
            "iteration # 9\n",
            "accuracy:  0.5340563337852087\n",
            "Cost: \n",
            " 0.9686344101220035 \n",
            "\n",
            "iteration # 10\n",
            "accuracy:  0.5402018376261485\n",
            "Cost: \n",
            " 0.9454877505500293 \n",
            "\n",
            "iteration # 11\n",
            "accuracy:  0.5405030878144299\n",
            "Cost: \n",
            " 0.9958199780816223 \n",
            "\n",
            "iteration # 12\n",
            "accuracy:  0.5446302153938847\n",
            "Cost: \n",
            " 0.8804516235051347 \n",
            "\n",
            "iteration # 13\n",
            "accuracy:  0.5448109655068535\n",
            "Cost: \n",
            " 0.8011909843464643 \n",
            "\n",
            "iteration # 14\n",
            "accuracy:  0.5172766982979364\n",
            "Cost: \n",
            " 0.7140984697089302 \n",
            "\n",
            "iteration # 15\n",
            "accuracy:  0.5488175930109956\n",
            "Cost: \n",
            " 0.7047049384234393 \n",
            "\n",
            "iteration # 16\n",
            "accuracy:  0.5420997138123211\n",
            "Cost: \n",
            " 0.7038868553864946 \n",
            "\n",
            "iteration # 17\n",
            "accuracy:  0.5492092182557614\n",
            "Cost: \n",
            " 0.7004984917861533 \n",
            "\n",
            "iteration # 18\n",
            "accuracy:  0.5589395993372496\n",
            "Cost: \n",
            " 0.7056550628906822 \n",
            "\n",
            "iteration # 19\n",
            "accuracy:  0.5613496008435005\n",
            "Cost: \n",
            " 0.6931501946861167 \n",
            "\n",
            "iteration # 20\n",
            "accuracy:  0.5643922277451423\n",
            "Cost: \n",
            " 0.6859881517995483 \n",
            "\n",
            "iteration # 21\n",
            "accuracy:  0.58237686398554\n",
            "Cost: \n",
            " 0.6793489877372155 \n",
            "\n",
            "iteration # 22\n",
            "accuracy:  0.4981171863232415\n",
            "Cost: \n",
            " 0.7312578060023234 \n",
            "\n",
            "iteration # 23\n",
            "accuracy:  0.5339659587287242\n",
            "Cost: \n",
            " 0.6935900141043837 \n",
            "\n",
            "iteration # 24\n",
            "accuracy:  0.5813827383642115\n",
            "Cost: \n",
            " 0.6786528791495021 \n",
            "\n",
            "iteration # 25\n",
            "accuracy:  0.5794547371592107\n",
            "Cost: \n",
            " 0.6763857804536978 \n",
            "\n",
            "iteration # 26\n",
            "accuracy:  0.5770146106341316\n",
            "Cost: \n",
            " 0.6741776881195889 \n",
            "\n",
            "iteration # 27\n",
            "accuracy:  0.5812019882512427\n",
            "Cost: \n",
            " 0.6717779934267338 \n",
            "\n",
            "iteration # 28\n",
            "accuracy:  0.5834312396445248\n",
            "Cost: \n",
            " 0.671366356929033 \n",
            "\n",
            "iteration # 29\n",
            "accuracy:  0.5780991113119446\n",
            "Cost: \n",
            " 0.6759549565575329 \n",
            "\n",
            "iteration # 30\n",
            "accuracy:  0.580177737611086\n",
            "Cost: \n",
            " 0.6830367750382293 \n",
            "\n",
            "iteration # 31\n",
            "accuracy:  0.5876487422804639\n",
            "Cost: \n",
            " 0.6720519478451911 \n",
            "\n",
            "iteration # 32\n",
            "accuracy:  0.5890646181653864\n",
            "Cost: \n",
            " 0.6715685908937545 \n",
            "\n",
            "iteration # 33\n",
            "accuracy:  0.5918361198975749\n",
            "Cost: \n",
            " 0.672334475817697 \n",
            "\n",
            "iteration # 34\n",
            "accuracy:  0.5930712456695285\n",
            "Cost: \n",
            " 0.6703337051830598 \n",
            "\n",
            "iteration # 35\n",
            "accuracy:  0.5951799969874981\n",
            "Cost: \n",
            " 0.6683871320151683 \n",
            "\n",
            "iteration # 36\n",
            "accuracy:  0.5946979966862479\n",
            "Cost: \n",
            " 0.6679229363862556 \n",
            "\n",
            "iteration # 37\n",
            "accuracy:  0.5897876186172616\n",
            "Cost: \n",
            " 0.6686675422277459 \n",
            "\n",
            "iteration # 38\n",
            "accuracy:  0.5853592408495255\n",
            "Cost: \n",
            " 0.6714176545558842 \n",
            "\n",
            "iteration # 39\n",
            "accuracy:  0.5867751167344479\n",
            "Cost: \n",
            " 0.6719562844817121 \n",
            "\n",
            "iteration # 40\n",
            "accuracy:  0.5817743636089773\n",
            "Cost: \n",
            " 0.6770965829279526 \n",
            "\n",
            "iteration # 41\n",
            "accuracy:  0.5838831149269468\n",
            "Cost: \n",
            " 0.6723118696199185 \n",
            "\n",
            "iteration # 42\n",
            "accuracy:  0.5848772405482754\n",
            "Cost: \n",
            " 0.673600334317727 \n",
            "\n",
            "iteration # 43\n",
            "accuracy:  0.5828588642867902\n",
            "Cost: \n",
            " 0.6792691182705473 \n",
            "\n",
            "iteration # 44\n",
            "accuracy:  0.587016116885073\n",
            "Cost: \n",
            " 0.6755773081457053 \n",
            "\n",
            "iteration # 45\n",
            "accuracy:  0.589426118391324\n",
            "Cost: \n",
            " 0.6719505823494462 \n",
            "\n",
            "iteration # 46\n",
            "accuracy:  0.5868052417532761\n",
            "Cost: \n",
            " 0.669482821889468 \n",
            "\n",
            "iteration # 47\n",
            "accuracy:  0.5877993673746046\n",
            "Cost: \n",
            " 0.6706066725965498 \n",
            "\n",
            "iteration # 48\n",
            "accuracy:  0.5897273685796054\n",
            "Cost: \n",
            " 0.6687922407065117 \n",
            "\n",
            "iteration # 49\n",
            "accuracy:  0.5838529899081187\n",
            "Cost: \n",
            " 0.6813800128517371 \n",
            "\n",
            "iteration # 50\n",
            "accuracy:  0.5877391173369483\n",
            "Cost: \n",
            " 0.6712982373711136 \n",
            "\n",
            "iteration # 51\n",
            "accuracy:  0.5925591203494502\n",
            "Cost: \n",
            " 0.6699258977026321 \n",
            "\n",
            "iteration # 52\n",
            "accuracy:  0.5929206205753879\n",
            "Cost: \n",
            " 0.6668452479416765 \n",
            "\n",
            "iteration # 53\n",
            "accuracy:  0.5899984937490586\n",
            "Cost: \n",
            " 0.6669980621487664 \n",
            "\n",
            "iteration # 54\n",
            "accuracy:  0.592287995179997\n",
            "Cost: \n",
            " 0.666415982868973 \n",
            "\n",
            "iteration # 55\n",
            "accuracy:  0.5855098659436662\n",
            "Cost: \n",
            " 0.6742082227440175 \n",
            "\n",
            "iteration # 56\n",
            "accuracy:  0.5845458653411658\n",
            "Cost: \n",
            " 0.6718576989752795 \n",
            "\n",
            "iteration # 57\n",
            "accuracy:  0.5827684892303058\n",
            "Cost: \n",
            " 0.6784730631096305 \n",
            "\n",
            "iteration # 58\n",
            "accuracy:  0.5904804940503088\n",
            "Cost: \n",
            " 0.6688010354715368 \n",
            "\n",
            "iteration # 59\n",
            "accuracy:  0.5895767434854646\n",
            "Cost: \n",
            " 0.6665926955833797 \n",
            "\n",
            "iteration # 60\n",
            "accuracy:  0.5905407440879651\n",
            "Cost: \n",
            " 0.6661996380559441 \n",
            "\n",
            "iteration # 61\n",
            "accuracy:  0.5890947431842145\n",
            "Cost: \n",
            " 0.6662797813087928 \n",
            "\n",
            "iteration # 62\n",
            "accuracy:  0.5845458653411658\n",
            "Cost: \n",
            " 0.6723499134774962 \n",
            "\n",
            "iteration # 63\n",
            "accuracy:  0.5868654917909324\n",
            "Cost: \n",
            " 0.6673748409718318 \n",
            "\n",
            "iteration # 64\n",
            "accuracy:  0.5871968669980419\n",
            "Cost: \n",
            " 0.6700748704448432 \n",
            "\n",
            "iteration # 65\n",
            "accuracy:  0.5895164934478084\n",
            "Cost: \n",
            " 0.6668009168472109 \n",
            "\n",
            "iteration # 66\n",
            "accuracy:  0.5931917457448411\n",
            "Cost: \n",
            " 0.6648410991641138 \n",
            "\n",
            "iteration # 67\n",
            "accuracy:  0.5919566199728875\n",
            "Cost: \n",
            " 0.6650524180230859 \n",
            "\n",
            "iteration # 68\n",
            "accuracy:  0.5898177436360897\n",
            "Cost: \n",
            " 0.6670464792943809 \n",
            "\n",
            "iteration # 69\n",
            "accuracy:  0.5851182406989004\n",
            "Cost: \n",
            " 0.6784640920368682 \n",
            "\n",
            "iteration # 70\n",
            "accuracy:  0.5896068685042928\n",
            "Cost: \n",
            " 0.671038071910549 \n",
            "\n",
            "iteration # 71\n",
            "accuracy:  0.592739870462419\n",
            "Cost: \n",
            " 0.6666507729596368 \n",
            "\n",
            "iteration # 72\n",
            "accuracy:  0.5888838680524175\n",
            "Cost: \n",
            " 0.66651287555999 \n",
            "\n",
            "iteration # 73\n",
            "accuracy:  0.5893959933724958\n",
            "Cost: \n",
            " 0.6662926039028179 \n",
            "\n",
            "iteration # 74\n",
            "accuracy:  0.5897574935984335\n",
            "Cost: \n",
            " 0.6666620943354155 \n",
            "\n",
            "iteration # 75\n",
            "accuracy:  0.5933724958578099\n",
            "Cost: \n",
            " 0.6669796753460846 \n",
            "\n",
            "iteration # 76\n",
            "accuracy:  0.594728121705076\n",
            "Cost: \n",
            " 0.6661045974895707 \n",
            "\n",
            "iteration # 77\n",
            "accuracy:  0.591173369483356\n",
            "Cost: \n",
            " 0.6650647387189688 \n",
            "\n",
            "iteration # 78\n",
            "accuracy:  0.5902696189185118\n",
            "Cost: \n",
            " 0.6670211086427968 \n",
            "\n",
            "iteration # 79\n",
            "accuracy:  0.5943967464979666\n",
            "Cost: \n",
            " 0.666341906045599 \n",
            "\n",
            "iteration # 80\n",
            "accuracy:  0.5935833709896069\n",
            "Cost: \n",
            " 0.666283343549062 \n",
            "\n",
            "iteration # 81\n",
            "accuracy:  0.5928001205000754\n",
            "Cost: \n",
            " 0.6662727700013401 \n",
            "\n",
            "iteration # 82\n",
            "accuracy:  0.5932218707636693\n",
            "Cost: \n",
            " 0.6651122973582428 \n",
            "\n",
            "iteration # 83\n",
            "accuracy:  0.5917156198222624\n",
            "Cost: \n",
            " 0.6639622496133398 \n",
            "\n",
            "iteration # 84\n",
            "accuracy:  0.5927699954812472\n",
            "Cost: \n",
            " 0.6638433807043876 \n",
            "\n",
            "iteration # 85\n",
            "accuracy:  0.596023497514686\n",
            "Cost: \n",
            " 0.6661420968910836 \n",
            "\n",
            "iteration # 86\n",
            "accuracy:  0.5962042476276548\n",
            "Cost: \n",
            " 0.6649305779019876 \n",
            "\n",
            "iteration # 87\n",
            "accuracy:  0.596716372947733\n",
            "Cost: \n",
            " 0.6637864607738079 \n",
            "\n",
            "iteration # 88\n",
            "accuracy:  0.5940954963096852\n",
            "Cost: \n",
            " 0.6637971815696934 \n",
            "\n",
            "iteration # 89\n",
            "accuracy:  0.5944569965356228\n",
            "Cost: \n",
            " 0.6632186302584574 \n",
            "\n",
            "iteration # 90\n",
            "accuracy:  0.5946377466485917\n",
            "Cost: \n",
            " 0.6632900380766318 \n",
            "\n",
            "iteration # 91\n",
            "accuracy:  0.5888236180147612\n",
            "Cost: \n",
            " 0.6745483901238236 \n",
            "\n",
            "iteration # 92\n",
            "accuracy:  0.5959632474770297\n",
            "Cost: \n",
            " 0.6701457986425611 \n",
            "\n",
            "iteration # 93\n",
            "accuracy:  0.5953004970628106\n",
            "Cost: \n",
            " 0.6669071915959729 \n",
            "\n",
            "iteration # 94\n",
            "accuracy:  0.5838529899081187\n",
            "Cost: \n",
            " 0.6806013375201667 \n",
            "\n",
            "iteration # 95\n",
            "accuracy:  0.5855098659436662\n",
            "Cost: \n",
            " 0.6728775207133101 \n",
            "\n",
            "iteration # 96\n",
            "accuracy:  0.5968669980418738\n",
            "Cost: \n",
            " 0.6676898856966281 \n",
            "\n",
            "iteration # 97\n",
            "accuracy:  0.5955113721946076\n",
            "Cost: \n",
            " 0.6662156617366202 \n",
            "\n",
            "iteration # 98\n",
            "accuracy:  0.5896972435607772\n",
            "Cost: \n",
            " 0.6704173117659823 \n",
            "\n",
            "iteration # 99\n",
            "accuracy:  0.5940051212532008\n",
            "Cost: \n",
            " 0.6654026721544135 \n",
            "\n",
            "iteration # 100\n",
            "accuracy:  0.595601747251092\n",
            "Cost: \n",
            " 0.6661858415005922 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "nn1=NeuralNetwork(x_train,y_train)\n",
        "nn1.fit(epochs=100,Backpropagate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3qLqPFQaZdi",
        "outputId": "c0b7ef01-5519-4a44-e146-bf0b301ed0db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.6705560818851898\n",
            "Acc : 0.5908721192950745\n"
          ]
        }
      ],
      "source": [
        "Test_1=nn1.evaluate(x_cv,y_cv.reshape(-1,1))\n",
        "print('Loss :',Test_1[0])\n",
        "print('Acc :',Test_1[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK7a-qwJaZdi",
        "outputId": "637e120a-0f79-40cb-baf8-b0fd94dd948d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.6720969592486892\n",
            "Acc : 0.5939448712155445\n"
          ]
        }
      ],
      "source": [
        "Test_2=nn1.evaluate(x_test,y_test.reshape(-1,1))\n",
        "print('Loss :',Test_2[0])\n",
        "print('Acc :',Test_2[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-5QoYz8aZdj"
      },
      "source": [
        "# Bayiesn Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nI6RJCHRaZdj"
      },
      "outputs": [],
      "source": [
        "class BayesianNeuralNetwork:\n",
        "    def __init__(self, x, y,vocabulary_size=21361,nodes_in_layer1=32, nodes_in_layer2=16, nodes_in_layer3=1, l_rate=0.1):\n",
        "        embedding_dim= 50\n",
        "        vocab_size = vocabulary_size +1 # Add 1 for the out-of-vocabulary token\n",
        "        self.embedding_weights = np.random.randn(vocab_size, embedding_dim)\n",
        "        # Define x, y\n",
        "        self.inputs_in_layer0 = self.embedding(x) # Layer 0\n",
        "        self.y = y.reshape(-1,1)\n",
        "\n",
        "        self.l_rate = l_rate  # Learning rate\n",
        "\n",
        "        # Define and set the number of neurons in each layer\n",
        "        self.nodes_in_layer1 = nodes_in_layer1\n",
        "        self.nodes_in_layer2 = nodes_in_layer2\n",
        "        self.nodes_in_layer3 = nodes_in_layer3\n",
        "\n",
        "        # Initialize weights and biases with smaller values using Gaussian distributions\n",
        "        self.thetas_layer0 = initialize_weights(self.inputs_in_layer0.shape[1] + 1, self.nodes_in_layer1, mu=0.001, sigma=0.01)\n",
        "        self.thetas_layer1 = initialize_weights(self.nodes_in_layer1 + 1, self.nodes_in_layer2, mu=0.001, sigma=0.01)\n",
        "        self.thetas_layer2 = initialize_weights(self.nodes_in_layer2 + 1, self.nodes_in_layer3, mu=0.001, sigma=0.01)\n",
        "\n",
        "        # Initialize prior distributions for weights\n",
        "        self.prior_mean_theta0 = np.zeros_like(self.thetas_layer0)\n",
        "        self.prior_mean_theta1 = np.zeros_like(self.thetas_layer1)\n",
        "        self.prior_mean_theta2 = np.zeros_like(self.thetas_layer2)\n",
        "\n",
        "        self.prior_variance_theta0 = np.ones_like(self.thetas_layer0)\n",
        "        self.prior_variance_theta1 = np.ones_like(self.thetas_layer1)\n",
        "        self.prior_variance_theta2 = np.ones_like(self.thetas_layer2)\n",
        "    def embedding(self, x):\n",
        "        self.embedded_input = self.embedding_weights[x]\n",
        "        pooled_embeddings = np.mean(self.embedded_input, axis=1)\n",
        "        return pooled_embeddings\n",
        "\n",
        "    def feedforward(self):\n",
        "        # Sample weights from their respective Gaussian distributions for each forward pass\n",
        "        self.Z1 = self.thetas_layer0[0] + np.dot(self.inputs_in_layer0, self.thetas_layer0[1:])\n",
        "        self.layer1 = relu(self.Z1)\n",
        "\n",
        "        self.Z2 = self.thetas_layer1[0] + np.dot(self.layer1, self.thetas_layer1[1:])\n",
        "        self.layer2 = relu(self.Z2)\n",
        "\n",
        "        self.Z3 = self.thetas_layer2[0] + np.dot(self.layer2, self.thetas_layer2[1:])\n",
        "        self.layer3 = sigmoid(self.Z3)\n",
        "\n",
        "        return self.layer3\n",
        "\n",
        "    def calculate_accuracy(self):\n",
        "        actual_output = self.y\n",
        "        predicted_output = self.layer3\n",
        "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
        "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
        "\n",
        "        # Compare predicted classes with actual classes\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def log_prior(self):\n",
        "        # Calculate log priors for weights using Gaussian distributions\n",
        "        log_prior_theta0 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta0) +\n",
        "                                   ((self.thetas_layer0 - self.prior_mean_theta0) ** 2) /\n",
        "                                   self.prior_variance_theta0).sum()\n",
        "\n",
        "        log_prior_theta1 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta1) +\n",
        "                                   ((self.thetas_layer1 - self.prior_mean_theta1) ** 2) /\n",
        "                                   self.prior_variance_theta1).sum()\n",
        "\n",
        "        log_prior_theta2 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta2) +\n",
        "                                   ((self.thetas_layer2 - self.prior_mean_theta2) ** 2) /\n",
        "                                   self.prior_variance_theta2).sum()\n",
        "\n",
        "        return log_prior_theta0 + log_prior_theta1 + log_prior_theta2\n",
        "\n",
        "    def log_likelihood(self):\n",
        "        # Compute log likelihood for Bernoulli distribution\n",
        "        self.n = self.inputs_in_layer0.shape[0]  # Number of training examples\n",
        "\n",
        "        # Calculate log-likelihood for Bernoulli likelihood\n",
        "        epsilon = 1e-10  # Small value to prevent log(0)\n",
        "        log_likelihood = np.sum(self.y * np.log(self.layer3 + epsilon) + (1 - self.y) * np.log(1 - self.layer3 + epsilon))\n",
        "\n",
        "        # Normalize log-likelihood by the number of training examples\n",
        "        log_likelihood /= -self.n\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def log_posterior(self):\n",
        "        # Compute log posterior using log prior and log likelihood\n",
        "        log_prior = self.log_prior()\n",
        "        log_likelihood = self.log_likelihood()\n",
        "        log_posterior = log_prior + log_likelihood\n",
        "        return log_posterior\n",
        "    def perform_MCMC(self,proposal_variance=0.01):\n",
        "         # Make a copy of the current weights for proposal\n",
        "        proposed_thetas_layer0 = np.copy(self.thetas_layer0)\n",
        "        proposed_thetas_layer1 = np.copy(self.thetas_layer1)\n",
        "        proposed_thetas_layer2 = np.copy(self.thetas_layer2)\n",
        "\n",
        "            # Perturb the weights for proposal (using a Gaussian random walk as an example)\n",
        "        proposed_thetas_layer0 += np.random.normal(0, proposal_variance, size=self.thetas_layer0.shape)\n",
        "        proposed_thetas_layer1 += np.random.normal(0, proposal_variance, size=self.thetas_layer1.shape)\n",
        "        proposed_thetas_layer2 += np.random.normal(0, proposal_variance, size=self.thetas_layer2.shape)\n",
        "\n",
        "            # Compute log-likelihoods for current and proposed weights\n",
        "        current_log_likelihood = self.log_likelihood()\n",
        "\n",
        "            # Compute log-posterior for the proposed weights\n",
        "        self.thetas_layer0 = proposed_thetas_layer0\n",
        "        self.thetas_layer1 = proposed_thetas_layer1\n",
        "        self.thetas_layer2 = proposed_thetas_layer2\n",
        "\n",
        "        proposed_log_likelihood = self.log_likelihood()\n",
        "        proposed_log_posterior = self.log_prior() + proposed_log_likelihood\n",
        "\n",
        "            # Accept or reject the proposal based on Metropolis-Hastings acceptance criterion\n",
        "        acceptance_ratio = np.exp(proposed_log_posterior - current_log_likelihood)\n",
        "        if np.random.uniform(0, 1) < acceptance_ratio:\n",
        "            # Accept the proposal\n",
        "            pass\n",
        "        else:\n",
        "            # Reject the proposal, revert weights to the previous state\n",
        "            self.thetas_layer0 = np.copy(proposed_thetas_layer0)\n",
        "            self.thetas_layer1 = np.copy(proposed_thetas_layer1)\n",
        "            self.thetas_layer2 = np.copy(proposed_thetas_layer2)\n",
        "    def fit(self,epochs):\n",
        "        losses=[]\n",
        "        for i in range(epochs):\n",
        "            self.feedforward()\n",
        "            error = self.log_likelihood()  # Compute log-likelihood as the error\n",
        "            losses.append(error)\n",
        "            self.perform_MCMC()\n",
        "            print(\"iteration #\",i+1)\n",
        "            print('accuracy: ',self.calculate_accuracy())\n",
        "            print(\"Cost: \\n\",error,\"\\n\")\n",
        "    def evaluate(self, x,y):\n",
        "        inputs_layer0 = self.embedding(x)\n",
        "        Z1 = self.thetas_layer0[0] + np.dot(inputs_layer0, self.thetas_layer0[1:])\n",
        "        layer1 = sigmoid(Z1)\n",
        "\n",
        "        Z2 = self.thetas_layer1[0] + np.dot(layer1, self.thetas_layer1[1:])\n",
        "        layer2 = sigmoid(Z2)\n",
        "\n",
        "        Z3 = self.thetas_layer2[0] + np.dot(layer2, self.thetas_layer2[1:])\n",
        "        layer3 = sigmoid(Z3)\n",
        "        loss= (1/inputs_layer0.shape[0]) * np.sum(-y * np.log(layer3) - (1 - y) * np.log(1 - layer3)) #cross entropy\n",
        "        actual_output=y\n",
        "        predicted_output=layer3\n",
        "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
        "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
        "\n",
        "        # Compare predicted classes with actual classes\n",
        "        correct_predictions = (predicted_classes == actual_output).sum()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct_predictions / len(actual_output)\n",
        "        return loss,accuracy,layer3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L67X9undaZdk"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWxLtzJPaZdl",
        "outputId": "8228703c-2272-4cd2-8009-8a4eef9168ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration # 1\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6936701581322194 \n",
            "\n",
            "iteration # 2\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6932422332089776 \n",
            "\n",
            "iteration # 3\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.693958695939878 \n",
            "\n",
            "iteration # 4\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6939266782263037 \n",
            "\n",
            "iteration # 5\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.694155387010561 \n",
            "\n",
            "iteration # 6\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.694354794728195 \n",
            "\n",
            "iteration # 7\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6940013530969628 \n",
            "\n",
            "iteration # 8\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6935357444131872 \n",
            "\n",
            "iteration # 9\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6935101342038187 \n",
            "\n",
            "iteration # 10\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6929599867576629 \n",
            "\n",
            "iteration # 11\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6938929779126752 \n",
            "\n",
            "iteration # 12\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6938669465206667 \n",
            "\n",
            "iteration # 13\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.693979144738269 \n",
            "\n",
            "iteration # 14\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6929555348459462 \n",
            "\n",
            "iteration # 15\n",
            "accuracy:  0.45725204637591343\n",
            "Cost: \n",
            " 0.693237345136246 \n",
            "\n",
            "iteration # 16\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6937108890579979 \n",
            "\n",
            "iteration # 17\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6936387157353692 \n",
            "\n",
            "iteration # 18\n",
            "accuracy:  0.4573036899320887\n",
            "Cost: \n",
            " 0.6932548408798401 \n",
            "\n",
            "iteration # 19\n",
            "accuracy:  0.4574069770444393\n",
            "Cost: \n",
            " 0.6932353064528386 \n",
            "\n",
            "iteration # 20\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6924119302109668 \n",
            "\n",
            "iteration # 21\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6924323986236568 \n",
            "\n",
            "iteration # 22\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6917226401129669 \n",
            "\n",
            "iteration # 23\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6912241526082431 \n",
            "\n",
            "iteration # 24\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6913844447038131 \n",
            "\n",
            "iteration # 25\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6919405591004473 \n",
            "\n",
            "iteration # 26\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6915197602066738 \n",
            "\n",
            "iteration # 27\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6917012503671612 \n",
            "\n",
            "iteration # 28\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6918549164202163 \n",
            "\n",
            "iteration # 29\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6909971502027654 \n",
            "\n",
            "iteration # 30\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6908374364014355 \n",
            "\n",
            "iteration # 31\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906356030803947 \n",
            "\n",
            "iteration # 32\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6911362507373654 \n",
            "\n",
            "iteration # 33\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6913902511789329 \n",
            "\n",
            "iteration # 34\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6913358068196488 \n",
            "\n",
            "iteration # 35\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.691313148821514 \n",
            "\n",
            "iteration # 36\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6910055357207967 \n",
            "\n",
            "iteration # 37\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6908359238481684 \n",
            "\n",
            "iteration # 38\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6908538986299847 \n",
            "\n",
            "iteration # 39\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906808378520692 \n",
            "\n",
            "iteration # 40\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.690533049771734 \n",
            "\n",
            "iteration # 41\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6907138135376358 \n",
            "\n",
            "iteration # 42\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906693232884198 \n",
            "\n",
            "iteration # 43\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6902044148624854 \n",
            "\n",
            "iteration # 44\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6902881541597723 \n",
            "\n",
            "iteration # 45\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901781252076632 \n",
            "\n",
            "iteration # 46\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901607271103042 \n",
            "\n",
            "iteration # 47\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6900880648634572 \n",
            "\n",
            "iteration # 48\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6898472309238267 \n",
            "\n",
            "iteration # 49\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689931130571207 \n",
            "\n",
            "iteration # 50\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689834945597026 \n",
            "\n",
            "iteration # 51\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901951488904187 \n",
            "\n",
            "iteration # 52\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6900325332310774 \n",
            "\n",
            "iteration # 53\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901518113149943 \n",
            "\n",
            "iteration # 54\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901181383271827 \n",
            "\n",
            "iteration # 55\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6899580826211303 \n",
            "\n",
            "iteration # 56\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6901519574560979 \n",
            "\n",
            "iteration # 57\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897804109558686 \n",
            "\n",
            "iteration # 58\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897970895180776 \n",
            "\n",
            "iteration # 59\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896621337935782 \n",
            "\n",
            "iteration # 60\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896653787501182 \n",
            "\n",
            "iteration # 61\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896117298032708 \n",
            "\n",
            "iteration # 62\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6894737671127876 \n",
            "\n",
            "iteration # 63\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896689720204863 \n",
            "\n",
            "iteration # 64\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689695825016472 \n",
            "\n",
            "iteration # 65\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6895320128287796 \n",
            "\n",
            "iteration # 66\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689469395150524 \n",
            "\n",
            "iteration # 67\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6894549755264727 \n",
            "\n",
            "iteration # 68\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896557894384081 \n",
            "\n",
            "iteration # 69\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6895875328330674 \n",
            "\n",
            "iteration # 70\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6895610876538959 \n",
            "\n",
            "iteration # 71\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689518958891645 \n",
            "\n",
            "iteration # 72\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6895327007731389 \n",
            "\n",
            "iteration # 73\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6895835159641694 \n",
            "\n",
            "iteration # 74\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896270750820738 \n",
            "\n",
            "iteration # 75\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896315909267416 \n",
            "\n",
            "iteration # 76\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896667952697959 \n",
            "\n",
            "iteration # 77\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897875941562999 \n",
            "\n",
            "iteration # 78\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897081947187047 \n",
            "\n",
            "iteration # 79\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896950701814082 \n",
            "\n",
            "iteration # 80\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896575159270382 \n",
            "\n",
            "iteration # 81\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896290848073526 \n",
            "\n",
            "iteration # 82\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897097088817751 \n",
            "\n",
            "iteration # 83\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897094199392069 \n",
            "\n",
            "iteration # 84\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896295206157619 \n",
            "\n",
            "iteration # 85\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.689638997878565 \n",
            "\n",
            "iteration # 86\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896069656143134 \n",
            "\n",
            "iteration # 87\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6896434920142019 \n",
            "\n",
            "iteration # 88\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897175368190697 \n",
            "\n",
            "iteration # 89\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6897320579845277 \n",
            "\n",
            "iteration # 90\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6900550671344766 \n",
            "\n",
            "iteration # 91\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6899853767337114 \n",
            "\n",
            "iteration # 92\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6900418001521034 \n",
            "\n",
            "iteration # 93\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.690181499955381 \n",
            "\n",
            "iteration # 94\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6904303714747771 \n",
            "\n",
            "iteration # 95\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906358784396264 \n",
            "\n",
            "iteration # 96\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906641867951505 \n",
            "\n",
            "iteration # 97\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906130075067984 \n",
            "\n",
            "iteration # 98\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6909671989486099 \n",
            "\n",
            "iteration # 99\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6906328396102474 \n",
            "\n",
            "iteration # 100\n",
            "accuracy:  0.5426963100679113\n",
            "Cost: \n",
            " 0.6902535518357265 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "bnn=BayesianNeuralNetwork(x_train,y_train)\n",
        "bnn.fit(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVb8GrNyaZdl",
        "outputId": "d8fddc6b-24a9-475e-89f8-1322f180b612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.6905902299133001\n",
            "Acc : 0.5375346427280395\n"
          ]
        }
      ],
      "source": [
        "bnn_test=bnn.evaluate(x_cv,y_cv.reshape(-1,1))\n",
        "print('Loss :',bnn_test[0])\n",
        "print('Acc :',bnn_test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ7gd1IXaZdm",
        "outputId": "3b03a91c-ada2-4530-b4a2-36ae324749ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss : 0.689897172719227\n",
            "Acc : 0.5410290396433305\n"
          ]
        }
      ],
      "source": [
        "bnn_test_1=bnn.evaluate(x_test,y_test.reshape(-1,1))\n",
        "print('Loss :',bnn_test_1[0])\n",
        "print('Acc :',bnn_test_1[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU6OgmythfRy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
