{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBjD9s-bbJBB",
    "outputId": "fef5b7c8-0a60-469d-8c39-8e665803332f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('clean_data/features.npy')\n",
    "labels = np.load('clean_data/labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCcQlfFIQ8hv"
   },
   "source": [
    "## Spilt Train, Cross Validation and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "tEuJwRv5Q9ao"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33195, 33)\n",
      "(11065, 33)\n",
      "(11065, 33)\n",
      "(33195,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test_full, y_train, y_test_full = train_test_split(features, labels, train_size=0.6, random_state=1)\n",
    "x_test, x_cv, y_test, y_cv = train_test_split(x_test_full, y_test_full, train_size=0.5, random_state=1)\n",
    "print(x_train.shape)\n",
    "print(x_cv.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propogation, R  Back Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z): \n",
    "    return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to initialize weights as Gaussian distributions with specific mu and sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(nodes_in, nodes_out, mu=0, sigma=0.1):\n",
    "    return np.random.normal(mu, sigma, size=(nodes_in, nodes_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y,vocabulary_size=21361, nodes_in_layer1=60, nodes_in_layer2=30, nodes_in_layer3=1, l_rate=0.1):\n",
    "        embedding_dim =60\n",
    "        self.x=x\n",
    "        vocab_size = vocabulary_size +1 # Add 1 for the out-of-vocabulary token\n",
    "        self.embedding_weights = np.random.randn(vocab_size, embedding_dim)\n",
    "        # define x, y\n",
    "        self.inputs_in_layer0 =self.embedding(self.x)\n",
    "        self.y = y.reshape(-1, 1)  # reshape y to be a column vector\n",
    "        \n",
    "        self.l_rate = l_rate  # learning rate\n",
    "        \n",
    "        # define and set the number of neurons in each layer\n",
    "        self.nodes_in_layer1 = nodes_in_layer1\n",
    "        self.nodes_in_layer2 = nodes_in_layer2\n",
    "        self.nodes_in_layer3 = nodes_in_layer3\n",
    "        \n",
    "        # initialize the weights (theta) matrices\n",
    "        self.thetas_layer0 = np.random.rand(self.inputs_in_layer0.shape[1] + 1, self.nodes_in_layer1) \n",
    "        self.thetas_layer1 = np.random.rand(self.nodes_in_layer1 + 1, self.nodes_in_layer2) \n",
    "        self.thetas_layer2 = np.random.rand(self.nodes_in_layer2 + 1, self.nodes_in_layer3)  \n",
    "    def feedforward(self):      \n",
    "        #compute all the nodes (a1, a2, a3, a4) in layer1\n",
    "        n = self.inputs_in_layer0.shape[0]\n",
    "\n",
    "        self.Z1 = self.thetas_layer0[0] + np.dot(self.inputs_in_layer0, self.thetas_layer0[1:])\n",
    "        self.layer1 = sigmoid(self.Z1)  #values of a1, a2, a3, a4 in layer 1\n",
    "        \n",
    "        #compute all the nodes (a1, a2, a3) in layer2\n",
    "        self.Z2 = self.thetas_layer1[0] + np.dot(self.layer1, self.thetas_layer1[1:])\n",
    "        self.layer2 = sigmoid(self.Z2)  #values of a1, a2, a3 in layer 2\n",
    "        \n",
    "        #compute all the nodes (a1) in layer3\n",
    "        self.Z3 = self.thetas_layer2[0] + np.dot(self.layer2, self.thetas_layer2[1:])\n",
    "        self.layer3 = sigmoid(self.Z3)  #output layer      \n",
    "        \n",
    "        return self.layer3\n",
    "    \n",
    "    def cost_func(self):\n",
    "        self.n = self.inputs_in_layer0.shape[0] #number of training examples\n",
    "        self.cost = (1/self.n) * np.sum(-self.y * np.log(self.layer3) - (1 - self.y) * np.log(1 - self.layer3)) #cross entropy\n",
    "        return self.cost \n",
    "    def embedding(self, x):\n",
    "            self.embedded_input = self.embedding_weights[x]\n",
    "            pooled_embeddings = np.mean(self.embedded_input, axis=1)\n",
    "            return pooled_embeddings\n",
    "    def calculate_accuracy(self ):\n",
    "        actual_output=self.y\n",
    "        predicted_output=self.layer3\n",
    "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
    "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
    "\n",
    "        # Compare predicted classes with actual classes\n",
    "        correct_predictions = (predicted_classes == actual_output).sum()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = correct_predictions / len(actual_output)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def Rbackprop(self):\n",
    "        # Define RProp parameters\n",
    "        delta0 = 0.01  # Initial update value\n",
    "        delta_max = 20  # Maximum update value\n",
    "        delta_min = 1e-6  # Minimum update value\n",
    "        eta_plus = 1.5  # Increase factor\n",
    "        eta_minus = 0.5  # Decrease factor\n",
    "\n",
    "        # dervative of E with respect to theta and bias in layer2\n",
    "        self.dE_dlayer3 = (1 / self.n) * (self.layer3 - self.y) / (self.layer3 * (1 - self.layer3))\n",
    "        self.dE_dZ3 = np.multiply(self.dE_dlayer3, sigmoid(self.Z3) * (1 - sigmoid(self.Z3)))\n",
    "        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n",
    "        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n",
    "\n",
    "        # dervative of E with respect to theta and bias in layer1\n",
    "        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n",
    "        self.dE_dZ2 = np.multiply(self.dE_dlayer2, sigmoid(self.Z2) * (1 - sigmoid(self.Z2)))\n",
    "        # self.dE_dZ1 = np.multiply(self.dE_dlayer2, dervative_relu(self.Z2))\n",
    "        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n",
    "        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n",
    "\n",
    "        # dervative of E with respect to theta and bias in layer0\n",
    "        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n",
    "        self.dE_dZ1 = np.multiply(self.dE_dlayer1, sigmoid(self.Z1) * (1 - sigmoid(self.Z1)))\n",
    "        # self.dE_dZ1 = np.multiply(self.dE_dlayer1, dervative_relu(self.Z1))\n",
    "        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n",
    "        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n",
    "\n",
    "        # Initialize RProp update values\n",
    "        if not hasattr(self, 'prev_dE_dtheta2'):\n",
    "            self.prev_dE_dtheta2 = np.zeros_like(self.dE_dtheta2)\n",
    "            self.delta_theta2 = np.full_like(self.dE_dtheta2, delta0)\n",
    "        else:\n",
    "            self.delta_theta2 = np.where(self.dE_dtheta2 * self.prev_dE_dtheta2 > 0,\n",
    "                                        np.minimum(self.delta_theta2 * eta_plus, delta_max),\n",
    "                                        np.maximum(self.delta_theta2 * eta_minus, delta_min))\n",
    "        self.prev_dE_dtheta2 = self.dE_dtheta2\n",
    "\n",
    "        if not hasattr(self, 'prev_dE_dtheta1'):\n",
    "            self.prev_dE_dtheta1 = np.zeros_like(self.dE_dtheta1)\n",
    "            self.delta_theta1 = np.full_like(self.dE_dtheta1, delta0)\n",
    "        else:\n",
    "            self.delta_theta1 = np.where(self.dE_dtheta1 * self.prev_dE_dtheta1 > 0,\n",
    "                                        np.minimum(self.delta_theta1 * eta_plus, delta_max),\n",
    "                                        np.maximum(self.delta_theta1 * eta_minus, delta_min))\n",
    "        self.prev_dE_dtheta1 = self.dE_dtheta1\n",
    "\n",
    "        if not hasattr(self, 'prev_dE_dtheta0'):\n",
    "            self.prev_dE_dtheta0 = np.zeros_like(self.dE_dtheta0)\n",
    "            self.delta_theta0 = np.full_like(self.dE_dtheta0, delta0)\n",
    "        else:\n",
    "            self.delta_theta0 = np.where(self.dE_dtheta0 * self.prev_dE_dtheta0 > 0,\n",
    "                                        np.minimum(self.delta_theta0 * eta_plus, delta_max),\n",
    "                                        np.maximum(self.delta_theta0 * eta_minus, delta_min))\n",
    "        self.prev_dE_dtheta0 = self.dE_dtheta0\n",
    "\n",
    "        # Updating theta using RProp in layers 2, 1, and 0\n",
    "        self.thetas_layer2[1:] -= np.sign(self.dE_dtheta2) * self.delta_theta2\n",
    "        self.thetas_layer1[1:] -= np.sign(self.dE_dtheta1) * self.delta_theta1\n",
    "        self.thetas_layer0[1:] -= np.sign(self.dE_dtheta0) * self.delta_theta0\n",
    "\n",
    "        # Updating bias using RProp in layers 2, 1, and 0\n",
    "        self.thetas_layer2[0] -= np.sign(self.dE_dbias2) * self.delta_theta2[0]\n",
    "        self.thetas_layer1[0] -= np.sign(self.dE_dbias1) * self.delta_theta1[0]\n",
    "        self.thetas_layer0[0] -= np.sign(self.dE_dbias0) * self.delta_theta0[0]\n",
    "        return self \n",
    "    \n",
    "    def backprop(self):\n",
    "        #dervative of E with respect to theta and bias in layer2\n",
    "        self.dE_dlayer3 = (1/self.n) * (self.layer3-self.y)/(self.layer3*(1-self.layer3))\n",
    "        self.dE_dZ3 = np.multiply(self.dE_dlayer3, (sigmoid(self.Z3)* (1-sigmoid(self.Z3))))\n",
    "        self.dE_dtheta2 = np.dot(self.layer2.T, self.dE_dZ3)\n",
    "        self.dE_dbias2 = np.dot(np.ones(self.n), self.dE_dZ3)\n",
    "        \n",
    "        #dervative of E with respect to theta and bias in layer1\n",
    "        self.dE_dlayer2 = np.dot(self.dE_dZ3, self.thetas_layer2[1:].T)\n",
    "        self.dE_dZ2 = np.multiply(self.dE_dlayer2, sigmoid(self.Z2)* (1-sigmoid(self.Z2)))\n",
    "        self.dE_dtheta1 = np.dot(self.layer1.T, self.dE_dZ2)\n",
    "        self.dE_dbias1 = np.dot(np.ones(self.n), self.dE_dZ2)\n",
    "        \n",
    "        #dervative of E with respect to theta and bias in layer0\n",
    "        self.dE_dlayer1 = np.dot(self.dE_dZ2, self.thetas_layer1[1:].T)\n",
    "        self.dE_dZ1 = np.multiply(self.dE_dlayer1, sigmoid(self.Z1)* (1-sigmoid(self.Z1)))\n",
    "        self.dE_dtheta0 = np.dot(self.inputs_in_layer0.T, self.dE_dZ1)\n",
    "        self.dE_dbias0 = np.dot(np.ones(self.n), self.dE_dZ1)\n",
    "        #updating theta using gradient descent in layers 2, 1, and 0\n",
    "        self.thetas_layer2[1:] = self.thetas_layer2[1:] - self.l_rate * self.dE_dtheta2\n",
    "        self.thetas_layer1[1:] = self.thetas_layer1[1:] - self.l_rate * self.dE_dtheta1\n",
    "        self.thetas_layer0[1:] = self.thetas_layer0[1:] - self.l_rate * self.dE_dtheta0\n",
    "        # self.de_wegihts = np.dot(self.embedded_input.T, self.dE_dZ1)\n",
    "        # self.embedding_weights = -self.embedding_weights - self.l_rate * self.de_wegihts\n",
    "\n",
    "        \n",
    "        #updating bias using gradient descent in layers 2, 1, and 0\n",
    "        self.thetas_layer2[0] = self.thetas_layer2[0] - self.l_rate * self.dE_dbias2\n",
    "        self.thetas_layer1[0] = self.thetas_layer1[0] - self.l_rate * self.dE_dbias1\n",
    "        self.thetas_layer0[0] = self.thetas_layer0[0] - self.l_rate * self.dE_dbias0\n",
    "        return self\n",
    "    def fit(self,epochs,Backpropagate):\n",
    "        losses=[]\n",
    "        for i in range(epochs):\n",
    "            self.feedforward()\n",
    "            error=self.cost_func()\n",
    "            losses.append(error)\n",
    "            if Backpropagate==True:\n",
    "                self.backprop()\n",
    "            else:\n",
    "                self.Rbackprop()\n",
    "            print(\"iteration #\",i+1)\n",
    "            print('accuracy: ',self.calculate_accuracy())\n",
    "            print(\"Cost: \\n\",error,\"\\n\")\n",
    "    def evaluate(self, x,y):\n",
    "        inputs_layer0 = self.embedding(x)\n",
    "        Z1 = self.thetas_layer0[0] + np.dot(inputs_layer0, self.thetas_layer0[1:])\n",
    "        layer1 = sigmoid(Z1)\n",
    "\n",
    "        Z2 = self.thetas_layer1[0] + np.dot(layer1, self.thetas_layer1[1:])\n",
    "        layer2 = sigmoid(Z2)\n",
    "\n",
    "        Z3 = self.thetas_layer2[0] + np.dot(layer2, self.thetas_layer2[1:])\n",
    "        layer3 = sigmoid(Z3)\n",
    "        loss= (1/inputs_layer0.shape[0]) * np.sum(-y * np.log(layer3) - (1 - y) * np.log(1 - layer3)) #cross entropy\n",
    "        actual_output=y\n",
    "        predicted_output=layer3\n",
    "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
    "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
    "\n",
    "        # Compare predicted classes with actual classes\n",
    "        correct_predictions = (predicted_classes == actual_output).sum()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = correct_predictions / len(actual_output)\n",
    "        return loss,accuracy,layer3          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 1\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 6.8272322301306385 \n",
      "\n",
      "iteration # 2\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 5.910436889736193 \n",
      "\n",
      "iteration # 3\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 4.99373627675229 \n",
      "\n",
      "iteration # 4\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 4.077545484909513 \n",
      "\n",
      "iteration # 5\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 3.1640960364965376 \n",
      "\n",
      "iteration # 6\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 2.265184971886224 \n",
      "\n",
      "iteration # 7\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.4380177945862087 \n",
      "\n",
      "iteration # 8\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.8706935103947374 \n",
      "\n",
      "iteration # 9\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7039672090653267 \n",
      "\n",
      "iteration # 10\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6900458301318018 \n",
      "\n",
      "iteration # 11\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6893311280685598 \n",
      "\n",
      "iteration # 12\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892937814639241 \n",
      "\n",
      "iteration # 13\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892917979360313 \n",
      "\n",
      "iteration # 14\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916921295053 \n",
      "\n",
      "iteration # 15\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916864773132 \n",
      "\n",
      "iteration # 16\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861729058 \n",
      "\n",
      "iteration # 17\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861541167 \n",
      "\n",
      "iteration # 18\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291686150583 \n",
      "\n",
      "iteration # 19\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861478642 \n",
      "\n",
      "iteration # 20\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861451889 \n",
      "\n",
      "iteration # 21\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861425157 \n",
      "\n",
      "iteration # 22\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861398429 \n",
      "\n",
      "iteration # 23\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861371701 \n",
      "\n",
      "iteration # 24\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861344971 \n",
      "\n",
      "iteration # 25\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861318243 \n",
      "\n",
      "iteration # 26\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861291516 \n",
      "\n",
      "iteration # 27\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861264788 \n",
      "\n",
      "iteration # 28\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291686123806 \n",
      "\n",
      "iteration # 29\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861211331 \n",
      "\n",
      "iteration # 30\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861184604 \n",
      "\n",
      "iteration # 31\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861157876 \n",
      "\n",
      "iteration # 32\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861131148 \n",
      "\n",
      "iteration # 33\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291686110442 \n",
      "\n",
      "iteration # 34\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861077694 \n",
      "\n",
      "iteration # 35\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861050966 \n",
      "\n",
      "iteration # 36\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916861024238 \n",
      "\n",
      "iteration # 37\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860997511 \n",
      "\n",
      "iteration # 38\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860970785 \n",
      "\n",
      "iteration # 39\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860944057 \n",
      "\n",
      "iteration # 40\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291686091733 \n",
      "\n",
      "iteration # 41\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860890602 \n",
      "\n",
      "iteration # 42\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860863876 \n",
      "\n",
      "iteration # 43\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860837149 \n",
      "\n",
      "iteration # 44\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860810423 \n",
      "\n",
      "iteration # 45\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860783695 \n",
      "\n",
      "iteration # 46\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860756969 \n",
      "\n",
      "iteration # 47\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860730243 \n",
      "\n",
      "iteration # 48\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860703517 \n",
      "\n",
      "iteration # 49\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860676791 \n",
      "\n",
      "iteration # 50\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860650065 \n",
      "\n",
      "iteration # 51\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860623338 \n",
      "\n",
      "iteration # 52\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860596612 \n",
      "\n",
      "iteration # 53\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860569888 \n",
      "\n",
      "iteration # 54\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860543162 \n",
      "\n",
      "iteration # 55\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860516435 \n",
      "\n",
      "iteration # 56\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860489711 \n",
      "\n",
      "iteration # 57\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860462984 \n",
      "\n",
      "iteration # 58\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291686043626 \n",
      "\n",
      "iteration # 59\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860409534 \n",
      "\n",
      "iteration # 60\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291686038281 \n",
      "\n",
      "iteration # 61\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860356083 \n",
      "\n",
      "iteration # 62\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860329359 \n",
      "\n",
      "iteration # 63\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860302634 \n",
      "\n",
      "iteration # 64\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860275908 \n",
      "\n",
      "iteration # 65\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860249183 \n",
      "\n",
      "iteration # 66\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291686022246 \n",
      "\n",
      "iteration # 67\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860195734 \n",
      "\n",
      "iteration # 68\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291686016901 \n",
      "\n",
      "iteration # 69\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860142286 \n",
      "\n",
      "iteration # 70\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291686011556 \n",
      "\n",
      "iteration # 71\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860088837 \n",
      "\n",
      "iteration # 72\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860062113 \n",
      "\n",
      "iteration # 73\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860035389 \n",
      "\n",
      "iteration # 74\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916860008663 \n",
      "\n",
      "iteration # 75\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291685998194 \n",
      "\n",
      "iteration # 76\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859955217 \n",
      "\n",
      "iteration # 77\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859928494 \n",
      "\n",
      "iteration # 78\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291685990177 \n",
      "\n",
      "iteration # 79\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859875046 \n",
      "\n",
      "iteration # 80\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859848323 \n",
      "\n",
      "iteration # 81\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.68929168598216 \n",
      "\n",
      "iteration # 82\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859794875 \n",
      "\n",
      "iteration # 83\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859768153 \n",
      "\n",
      "iteration # 84\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689291685974143 \n",
      "\n",
      "iteration # 85\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859714706 \n",
      "\n",
      "iteration # 86\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859687984 \n",
      "\n",
      "iteration # 87\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859661262 \n",
      "\n",
      "iteration # 88\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859634537 \n",
      "\n",
      "iteration # 89\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859607815 \n",
      "\n",
      "iteration # 90\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859581093 \n",
      "\n",
      "iteration # 91\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859554371 \n",
      "\n",
      "iteration # 92\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859527649 \n",
      "\n",
      "iteration # 93\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859500926 \n",
      "\n",
      "iteration # 94\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859474204 \n",
      "\n",
      "iteration # 95\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859447481 \n",
      "\n",
      "iteration # 96\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859420759 \n",
      "\n",
      "iteration # 97\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859394038 \n",
      "\n",
      "iteration # 98\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859367316 \n",
      "\n",
      "iteration # 99\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859340593 \n",
      "\n",
      "iteration # 100\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6892916859313873 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn=NeuralNetwork(x_train,y_train)\n",
    "nn.fit(epochs=100,Backpropagate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.690561579228496\n",
      "Acc : 0.5366470854044284\n"
     ]
    }
   ],
   "source": [
    "test=nn.evaluate(x_cv,y_cv.reshape(-1,1))\n",
    "print('Loss :',test[0])\n",
    "print('Acc :',test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.6899587685866853\n",
      "Acc : 0.540081337550836\n"
     ]
    }
   ],
   "source": [
    "Test=nn.evaluate(x_test,y_test.reshape(-1,1))\n",
    "print('Loss :',Test[0])\n",
    "print('Acc :',Test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 1\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 8.672378898961265 \n",
      "\n",
      "iteration # 2\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 8.503777014071376 \n",
      "\n",
      "iteration # 3\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 8.250873701830685 \n",
      "\n",
      "iteration # 4\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 7.871482103587883 \n",
      "\n",
      "iteration # 5\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 7.295060137484709 \n",
      "\n",
      "iteration # 6\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 5.8905378363281775 \n",
      "\n",
      "iteration # 7\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 3.2567309052839795 \n",
      "\n",
      "iteration # 8\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.8779260392374009 \n",
      "\n",
      "iteration # 9\n",
      "accuracy:  0.45600241000150626\n",
      "Cost: \n",
      " 0.7175000074122505 \n",
      "\n",
      "iteration # 10\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 1.9241632901961079 \n",
      "\n",
      "iteration # 11\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.9450100308529096 \n",
      "\n",
      "iteration # 12\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.0690898964609068 \n",
      "\n",
      "iteration # 13\n",
      "accuracy:  0.5440277150173218\n",
      "Cost: \n",
      " 0.69064283224657 \n",
      "\n",
      "iteration # 14\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.5911084588822284 \n",
      "\n",
      "iteration # 15\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.199251894820777 \n",
      "\n",
      "iteration # 16\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7229207843274722 \n",
      "\n",
      "iteration # 17\n",
      "accuracy:  0.5440277150173218\n",
      "Cost: \n",
      " 1.6292872088818313 \n",
      "\n",
      "iteration # 18\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.8162677039514166 \n",
      "\n",
      "iteration # 19\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.661513325608038 \n",
      "\n",
      "iteration # 20\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7043394353894327 \n",
      "\n",
      "iteration # 21\n",
      "accuracy:  0.5439975899984938\n",
      "Cost: \n",
      " 2.361880709145948 \n",
      "\n",
      "iteration # 22\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.3826465226711977 \n",
      "\n",
      "iteration # 23\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.9497034636247046 \n",
      "\n",
      "iteration # 24\n",
      "accuracy:  0.5440277150173218\n",
      "Cost: \n",
      " 1.6301729006485701 \n",
      "\n",
      "iteration # 25\n",
      "accuracy:  0.5527338454586534\n",
      "Cost: \n",
      " 0.7396370085444033 \n",
      "\n",
      "iteration # 26\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.465622706873584 \n",
      "\n",
      "iteration # 27\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 1.2886643279474015 \n",
      "\n",
      "iteration # 28\n",
      "accuracy:  0.5448712155445097\n",
      "Cost: \n",
      " 0.8498542187071448 \n",
      "\n",
      "iteration # 29\n",
      "accuracy:  0.45594215996384996\n",
      "Cost: \n",
      " 0.8061974466407648 \n",
      "\n",
      "iteration # 30\n",
      "accuracy:  0.5546015966259978\n",
      "Cost: \n",
      " 0.6911459478622124 \n",
      "\n",
      "iteration # 31\n",
      "accuracy:  0.5621629763518602\n",
      "Cost: \n",
      " 0.6843699970507766 \n",
      "\n",
      "iteration # 32\n",
      "accuracy:  0.4561530350956469\n",
      "Cost: \n",
      " 1.160539439497115 \n",
      "\n",
      "iteration # 33\n",
      "accuracy:  0.5434252146407591\n",
      "Cost: \n",
      " 0.6902049969141452 \n",
      "\n",
      "iteration # 34\n",
      "accuracy:  0.5559572224732641\n",
      "Cost: \n",
      " 0.6877861239290743 \n",
      "\n",
      "iteration # 35\n",
      "accuracy:  0.5505949691218557\n",
      "Cost: \n",
      " 0.7063511546790581 \n",
      "\n",
      "iteration # 36\n",
      "accuracy:  0.575327609579756\n",
      "Cost: \n",
      " 0.6827439022731812 \n",
      "\n",
      "iteration # 37\n",
      "accuracy:  0.46588341617713513\n",
      "Cost: \n",
      " 0.7652402608056039 \n",
      "\n",
      "iteration # 38\n",
      "accuracy:  0.565868353667721\n",
      "Cost: \n",
      " 0.6994629889690787 \n",
      "\n",
      "iteration # 39\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7612274832809516 \n",
      "\n",
      "iteration # 40\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.7537943831870697 \n",
      "\n",
      "iteration # 41\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.8007631622395047 \n",
      "\n",
      "iteration # 42\n",
      "accuracy:  0.4560626600391625\n",
      "Cost: \n",
      " 0.6956194253295609 \n",
      "\n",
      "iteration # 43\n",
      "accuracy:  0.5491489682181051\n",
      "Cost: \n",
      " 0.813184448944593 \n",
      "\n",
      "iteration # 44\n",
      "accuracy:  0.5537882211176381\n",
      "Cost: \n",
      " 0.7022370777335761 \n",
      "\n",
      "iteration # 45\n",
      "accuracy:  0.5188130742581714\n",
      "Cost: \n",
      " 0.6991244681236918 \n",
      "\n",
      "iteration # 46\n",
      "accuracy:  0.5696942310588944\n",
      "Cost: \n",
      " 0.6788721349996526 \n",
      "\n",
      "iteration # 47\n",
      "accuracy:  0.5749962343726465\n",
      "Cost: \n",
      " 0.6834539608271128 \n",
      "\n",
      "iteration # 48\n",
      "accuracy:  0.45871366169603855\n",
      "Cost: \n",
      " 0.7168516334233207 \n",
      "\n",
      "iteration # 49\n",
      "accuracy:  0.5762313601446001\n",
      "Cost: \n",
      " 0.6766784894922487 \n",
      "\n",
      "iteration # 50\n",
      "accuracy:  0.5786714866696792\n",
      "Cost: \n",
      " 0.6757645607809288 \n",
      "\n",
      "iteration # 51\n",
      "accuracy:  0.5800572375357734\n",
      "Cost: \n",
      " 0.6738871624647498 \n",
      "\n",
      "iteration # 52\n",
      "accuracy:  0.5808706130441331\n",
      "Cost: \n",
      " 0.6726400608455291 \n",
      "\n",
      "iteration # 53\n",
      "accuracy:  0.5817442385901491\n",
      "Cost: \n",
      " 0.6724303080861015 \n",
      "\n",
      "iteration # 54\n",
      "accuracy:  0.5749359843349902\n",
      "Cost: \n",
      " 0.6736851081113044 \n",
      "\n",
      "iteration # 55\n",
      "accuracy:  0.5861726163578852\n",
      "Cost: \n",
      " 0.6716044936549141 \n",
      "\n",
      "iteration # 56\n",
      "accuracy:  0.5851784907365567\n",
      "Cost: \n",
      " 0.6704325235028645 \n",
      "\n",
      "iteration # 57\n",
      "accuracy:  0.5868956168097605\n",
      "Cost: \n",
      " 0.6697964247518775 \n",
      "\n",
      "iteration # 58\n",
      "accuracy:  0.586564241602651\n",
      "Cost: \n",
      " 0.6700994826342892 \n",
      "\n",
      "iteration # 59\n",
      "accuracy:  0.5773761108600692\n",
      "Cost: \n",
      " 0.6727356086780434 \n",
      "\n",
      "iteration # 60\n",
      "accuracy:  0.5879801174875734\n",
      "Cost: \n",
      " 0.6684012095413355 \n",
      "\n",
      "iteration # 61\n",
      "accuracy:  0.5877692423557764\n",
      "Cost: \n",
      " 0.6696328587182241 \n",
      "\n",
      "iteration # 62\n",
      "accuracy:  0.5895466184666366\n",
      "Cost: \n",
      " 0.6681206006339167 \n",
      "\n",
      "iteration # 63\n",
      "accuracy:  0.5859918662449164\n",
      "Cost: \n",
      " 0.6689724882096951 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_23140\\4003567598.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-Z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 64\n",
      "accuracy:  0.590751619219762\n",
      "Cost: \n",
      " 0.6667485325902698 \n",
      "\n",
      "iteration # 65\n",
      "accuracy:  0.5904804940503088\n",
      "Cost: \n",
      " 0.6659219478552831 \n",
      "\n",
      "iteration # 66\n",
      "accuracy:  0.5906009941256213\n",
      "Cost: \n",
      " 0.6653281336771593 \n",
      "\n",
      "iteration # 67\n",
      "accuracy:  0.5900286187678867\n",
      "Cost: \n",
      " 0.6657653269675723 \n",
      "\n",
      "iteration # 68\n",
      "accuracy:  0.5920771200482\n",
      "Cost: \n",
      " 0.6643081107488705 \n",
      "\n",
      "iteration # 69\n",
      "accuracy:  0.5935231209519506\n",
      "Cost: \n",
      " 0.6647686406501818 \n",
      "\n",
      "iteration # 70\n",
      "accuracy:  0.5932519957824973\n",
      "Cost: \n",
      " 0.6633699240712567 \n",
      "\n",
      "iteration # 71\n",
      "accuracy:  0.5934929959331224\n",
      "Cost: \n",
      " 0.6635039258871261 \n",
      "\n",
      "iteration # 72\n",
      "accuracy:  0.5931917457448411\n",
      "Cost: \n",
      " 0.6623713270137145 \n",
      "\n",
      "iteration # 73\n",
      "accuracy:  0.5939147461967164\n",
      "Cost: \n",
      " 0.6617209463864179 \n",
      "\n",
      "iteration # 74\n",
      "accuracy:  0.594728121705076\n",
      "Cost: \n",
      " 0.6609973695752498 \n",
      "\n",
      "iteration # 75\n",
      "accuracy:  0.5962343726464829\n",
      "Cost: \n",
      " 0.6599372896535085 \n",
      "\n",
      "iteration # 76\n",
      "accuracy:  0.5999397499623437\n",
      "Cost: \n",
      " 0.659965695936053 \n",
      "\n",
      "iteration # 77\n",
      "accuracy:  0.5964151227594517\n",
      "Cost: \n",
      " 0.6599712851606053 \n",
      "\n",
      "iteration # 78\n",
      "accuracy:  0.6009640006025003\n",
      "Cost: \n",
      " 0.6587538210603376 \n",
      "\n",
      "iteration # 79\n",
      "accuracy:  0.6001506250941406\n",
      "Cost: \n",
      " 0.6581757825823304 \n",
      "\n",
      "iteration # 80\n",
      "accuracy:  0.6\n",
      "Cost: \n",
      " 0.6577086051971235 \n",
      "\n",
      "iteration # 81\n",
      "accuracy:  0.6018978761861726\n",
      "Cost: \n",
      " 0.6571634494291517 \n",
      "\n",
      "iteration # 82\n",
      "accuracy:  0.6040066275041421\n",
      "Cost: \n",
      " 0.6564045920691273 \n",
      "\n",
      "iteration # 83\n",
      "accuracy:  0.6014460009037506\n",
      "Cost: \n",
      " 0.6559770359676919 \n",
      "\n",
      "iteration # 84\n",
      "accuracy:  0.6048802530501581\n",
      "Cost: \n",
      " 0.6551813830897457 \n",
      "\n",
      "iteration # 85\n",
      "accuracy:  0.6059045036903148\n",
      "Cost: \n",
      " 0.654844108362617 \n",
      "\n",
      "iteration # 86\n",
      "accuracy:  0.6072300045187529\n",
      "Cost: \n",
      " 0.6543352019346051 \n",
      "\n",
      "iteration # 87\n",
      "accuracy:  0.6060250037656274\n",
      "Cost: \n",
      " 0.6534453415525899 \n",
      "\n",
      "iteration # 88\n",
      "accuracy:  0.6073505045940654\n",
      "Cost: \n",
      " 0.6527085695784044 \n",
      "\n",
      "iteration # 89\n",
      "accuracy:  0.6071095044434403\n",
      "Cost: \n",
      " 0.6519529258745622 \n",
      "\n",
      "iteration # 90\n",
      "accuracy:  0.6084952553095345\n",
      "Cost: \n",
      " 0.6521240674842607 \n",
      "\n",
      "iteration # 91\n",
      "accuracy:  0.6101220063262539\n",
      "Cost: \n",
      " 0.6511947706164689 \n",
      "\n",
      "iteration # 92\n",
      "accuracy:  0.6087965054978159\n",
      "Cost: \n",
      " 0.6508457778551502 \n",
      "\n",
      "iteration # 93\n",
      "accuracy:  0.6097303810814881\n",
      "Cost: \n",
      " 0.6503944919438507 \n",
      "\n",
      "iteration # 94\n",
      "accuracy:  0.6109956318722699\n",
      "Cost: \n",
      " 0.6500385747257623 \n",
      "\n",
      "iteration # 95\n",
      "accuracy:  0.6114173821358638\n",
      "Cost: \n",
      " 0.6495055884354876 \n",
      "\n",
      "iteration # 96\n",
      "accuracy:  0.6118090073806296\n",
      "Cost: \n",
      " 0.6491250431202864 \n",
      "\n",
      "iteration # 97\n",
      "accuracy:  0.6129537580960988\n",
      "Cost: \n",
      " 0.6487342957958331 \n",
      "\n",
      "iteration # 98\n",
      "accuracy:  0.6128935080584426\n",
      "Cost: \n",
      " 0.6482092432428278 \n",
      "\n",
      "iteration # 99\n",
      "accuracy:  0.6127428829643019\n",
      "Cost: \n",
      " 0.6477145037560549 \n",
      "\n",
      "iteration # 100\n",
      "accuracy:  0.6125320078325049\n",
      "Cost: \n",
      " 0.6470575654429281 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn1=NeuralNetwork(x_train,y_train)\n",
    "nn1.fit(epochs=100,Backpropagate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.6616096775268931\n",
      "Acc : 0.5970176231360145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_23140\\4003567598.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-Z))\n"
     ]
    }
   ],
   "source": [
    "Test_1=nn1.evaluate(x_cv,y_cv.reshape(-1,1))\n",
    "print('Loss :',Test_1[0])\n",
    "print('Acc :',Test_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.6625555547263093\n",
      "Acc : 0.596023497514686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_23140\\4003567598.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-Z))\n"
     ]
    }
   ],
   "source": [
    "Test_2=nn1.evaluate(x_test,y_test.reshape(-1,1))\n",
    "print('Loss :',Test_2[0])\n",
    "print('Acc :',Test_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayiesn Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNeuralNetwork:\n",
    "    def __init__(self, x, y,vocabulary_size=21361,nodes_in_layer1=15, nodes_in_layer2=15, nodes_in_layer3=1, l_rate=0.001):\n",
    "        embedding_dim= 60\n",
    "        vocab_size = vocabulary_size +1 # Add 1 for the out-of-vocabulary token\n",
    "        self.embedding_weights = np.random.randn(vocab_size, embedding_dim)\n",
    "        # Define x, y\n",
    "        self.inputs_in_layer0 = self.embedding(x) # Layer 0\n",
    "        self.y = y.reshape(-1,1)\n",
    "\n",
    "        self.l_rate = l_rate  # Learning rate\n",
    "\n",
    "        # Define and set the number of neurons in each layer\n",
    "        self.nodes_in_layer1 = nodes_in_layer1\n",
    "        self.nodes_in_layer2 = nodes_in_layer2\n",
    "        self.nodes_in_layer3 = nodes_in_layer3\n",
    "\n",
    "        # Initialize weights and biases with smaller values using Gaussian distributions\n",
    "        self.thetas_layer0 = initialize_weights(self.inputs_in_layer0.shape[1] + 1, self.nodes_in_layer1, mu=0.001, sigma=0.01)\n",
    "        self.thetas_layer1 = initialize_weights(self.nodes_in_layer1 + 1, self.nodes_in_layer2, mu=0.001, sigma=0.01)\n",
    "        self.thetas_layer2 = initialize_weights(self.nodes_in_layer2 + 1, self.nodes_in_layer3, mu=0.001, sigma=0.01)\n",
    "\n",
    "        # Initialize prior distributions for weights\n",
    "        self.prior_mean_theta0 = np.zeros_like(self.thetas_layer0)\n",
    "        self.prior_mean_theta1 = np.zeros_like(self.thetas_layer1)\n",
    "        self.prior_mean_theta2 = np.zeros_like(self.thetas_layer2)\n",
    "\n",
    "        self.prior_variance_theta0 = np.ones_like(self.thetas_layer0)\n",
    "        self.prior_variance_theta1 = np.ones_like(self.thetas_layer1)\n",
    "        self.prior_variance_theta2 = np.ones_like(self.thetas_layer2)\n",
    "    def embedding(self, x):\n",
    "        self.embedded_input = self.embedding_weights[x]\n",
    "        pooled_embeddings = np.mean(self.embedded_input, axis=1)\n",
    "        return pooled_embeddings    \n",
    "\n",
    "    def feedforward(self):\n",
    "        # Sample weights from their respective Gaussian distributions for each forward pass\n",
    "        self.Z1 = self.thetas_layer0[0] + np.dot(self.inputs_in_layer0, self.thetas_layer0[1:])\n",
    "        self.layer1 = sigmoid(self.Z1)\n",
    "\n",
    "        self.Z2 = self.thetas_layer1[0] + np.dot(self.layer1, self.thetas_layer1[1:])\n",
    "        self.layer2 = sigmoid(self.Z2)\n",
    "\n",
    "        self.Z3 = self.thetas_layer2[0] + np.dot(self.layer2, self.thetas_layer2[1:])\n",
    "        self.layer3 = sigmoid(self.Z3)\n",
    "\n",
    "        return self.layer3\n",
    "    \n",
    "    def calculate_accuracy(self):\n",
    "        actual_output = self.y\n",
    "        predicted_output = self.layer3\n",
    "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
    "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
    "\n",
    "        # Compare predicted classes with actual classes\n",
    "        correct_predictions = (predicted_classes == actual_output).sum()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = correct_predictions / len(actual_output)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def log_prior(self):\n",
    "        # Calculate log priors for weights using Gaussian distributions\n",
    "        log_prior_theta0 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta0) +\n",
    "                                   ((self.thetas_layer0 - self.prior_mean_theta0) ** 2) /\n",
    "                                   self.prior_variance_theta0).sum()\n",
    "\n",
    "        log_prior_theta1 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta1) +\n",
    "                                   ((self.thetas_layer1 - self.prior_mean_theta1) ** 2) /\n",
    "                                   self.prior_variance_theta1).sum()\n",
    "\n",
    "        log_prior_theta2 = -0.5 * (np.log(2 * np.pi * self.prior_variance_theta2) +\n",
    "                                   ((self.thetas_layer2 - self.prior_mean_theta2) ** 2) /\n",
    "                                   self.prior_variance_theta2).sum()\n",
    "\n",
    "        return log_prior_theta0 + log_prior_theta1 + log_prior_theta2\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        # Compute log likelihood (negative log-likelihood for binary cross-entropy)\n",
    "        self.n = self.inputs_in_layer0.shape[0]  # Number of training examples\n",
    "        log_likelihood = (1 / self.n) * np.sum(-self.y * np.log(self.layer3) - (1 - self.y) * np.log(1 - self.layer3))\n",
    "        return log_likelihood\n",
    "\n",
    "    def log_posterior(self):\n",
    "        # Compute log posterior using log prior and log likelihood\n",
    "        log_prior = self.log_prior()\n",
    "        log_likelihood = self.log_likelihood()\n",
    "        log_posterior = log_prior + log_likelihood\n",
    "        return log_posterior\n",
    "    def perform_MCMC(self,proposal_variance=0.01):\n",
    "         # Make a copy of the current weights for proposal\n",
    "        proposed_thetas_layer0 = np.copy(self.thetas_layer0)\n",
    "        proposed_thetas_layer1 = np.copy(self.thetas_layer1)\n",
    "        proposed_thetas_layer2 = np.copy(self.thetas_layer2)\n",
    "\n",
    "            # Perturb the weights for proposal (using a Gaussian random walk as an example)\n",
    "        proposed_thetas_layer0 += np.random.normal(0, proposal_variance, size=self.thetas_layer0.shape)\n",
    "        proposed_thetas_layer1 += np.random.normal(0, proposal_variance, size=self.thetas_layer1.shape)\n",
    "        proposed_thetas_layer2 += np.random.normal(0, proposal_variance, size=self.thetas_layer2.shape)\n",
    "\n",
    "            # Compute log-likelihoods for current and proposed weights\n",
    "        current_log_likelihood = self.log_likelihood()\n",
    "\n",
    "            # Compute log-posterior for the proposed weights\n",
    "        self.thetas_layer0 = proposed_thetas_layer0\n",
    "        self.thetas_layer1 = proposed_thetas_layer1\n",
    "        self.thetas_layer2 = proposed_thetas_layer2\n",
    "\n",
    "        proposed_log_likelihood = self.log_likelihood()\n",
    "        proposed_log_posterior = self.log_prior() + proposed_log_likelihood\n",
    "\n",
    "            # Accept or reject the proposal based on Metropolis-Hastings acceptance criterion\n",
    "        acceptance_ratio = np.exp(proposed_log_posterior - current_log_likelihood)\n",
    "        if np.random.uniform(0, 1) < acceptance_ratio:\n",
    "            # Accept the proposal\n",
    "            pass\n",
    "        else:\n",
    "            # Reject the proposal, revert weights to the previous state\n",
    "            self.thetas_layer0 = np.copy(proposed_thetas_layer0)\n",
    "            self.thetas_layer1 = np.copy(proposed_thetas_layer1)\n",
    "            self.thetas_layer2 = np.copy(proposed_thetas_layer2)\n",
    "    def fit(self,epochs):\n",
    "        losses=[]\n",
    "        for i in range(epochs):\n",
    "            self.feedforward()\n",
    "            error = self.log_likelihood()  # Compute log-likelihood as the error\n",
    "            losses.append(error)\n",
    "            self.perform_MCMC()\n",
    "            print(\"iteration #\",i+1)\n",
    "            print('accuracy: ',self.calculate_accuracy())\n",
    "            print(\"Cost: \\n\",error,\"\\n\")\n",
    "    def evaluate(self, x,y):\n",
    "        inputs_layer0 = self.embedding(x)\n",
    "        Z1 = self.thetas_layer0[0] + np.dot(inputs_layer0, self.thetas_layer0[1:])\n",
    "        layer1 = sigmoid(Z1)\n",
    "\n",
    "        Z2 = self.thetas_layer1[0] + np.dot(layer1, self.thetas_layer1[1:])\n",
    "        layer2 = sigmoid(Z2)\n",
    "\n",
    "        Z3 = self.thetas_layer2[0] + np.dot(layer2, self.thetas_layer2[1:])\n",
    "        layer3 = sigmoid(Z3)\n",
    "        loss= (1/inputs_layer0.shape[0]) * np.sum(-y * np.log(layer3) - (1 - y) * np.log(1 - layer3)) #cross entropy\n",
    "        actual_output=y\n",
    "        predicted_output=layer3\n",
    "        # Convert predicted probabilities to binary predictions (0 or 1)\n",
    "        predicted_classes = (predicted_output >= 0.5).astype(int)\n",
    "\n",
    "        # Compare predicted classes with actual classes\n",
    "        correct_predictions = (predicted_classes == actual_output).sum()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = correct_predictions / len(actual_output)\n",
    "        return loss,accuracy,layer3          \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration # 1\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.692676569338566 \n",
      "\n",
      "iteration # 2\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6920980133693211 \n",
      "\n",
      "iteration # 3\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.6932188845250769 \n",
      "\n",
      "iteration # 4\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.691942502964267 \n",
      "\n",
      "iteration # 5\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.6944796803112536 \n",
      "\n",
      "iteration # 6\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.6938006080882397 \n",
      "\n",
      "iteration # 7\n",
      "accuracy:  0.4561229100768188\n",
      "Cost: \n",
      " 0.6932920064988984 \n",
      "\n",
      "iteration # 8\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6926895766925598 \n",
      "\n",
      "iteration # 9\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6911608051686295 \n",
      "\n",
      "iteration # 10\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6910810703905625 \n",
      "\n",
      "iteration # 11\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6905180658362565 \n",
      "\n",
      "iteration # 12\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6901909070230453 \n",
      "\n",
      "iteration # 13\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6904073994885646 \n",
      "\n",
      "iteration # 14\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6899743648037945 \n",
      "\n",
      "iteration # 15\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.690533318100505 \n",
      "\n",
      "iteration # 16\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689919965112686 \n",
      "\n",
      "iteration # 17\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6898078743362422 \n",
      "\n",
      "iteration # 18\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6897774210057477 \n",
      "\n",
      "iteration # 19\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6898621708180904 \n",
      "\n",
      "iteration # 20\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689924254965551 \n",
      "\n",
      "iteration # 21\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6894815755924072 \n",
      "\n",
      "iteration # 22\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6893591196149674 \n",
      "\n",
      "iteration # 23\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6893123620998751 \n",
      "\n",
      "iteration # 24\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.689394562788712 \n",
      "\n",
      "iteration # 25\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6895703738146635 \n",
      "\n",
      "iteration # 26\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6893748626922265 \n",
      "\n",
      "iteration # 27\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6894799520586895 \n",
      "\n",
      "iteration # 28\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6894310393320194 \n",
      "\n",
      "iteration # 29\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6893102747353852 \n",
      "\n",
      "iteration # 30\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6893913336551648 \n",
      "\n",
      "iteration # 31\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6897133257856449 \n",
      "\n",
      "iteration # 32\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6901795574355489 \n",
      "\n",
      "iteration # 33\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6903325812336649 \n",
      "\n",
      "iteration # 34\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.690081311414227 \n",
      "\n",
      "iteration # 35\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6897064176569325 \n",
      "\n",
      "iteration # 36\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6899253759828844 \n",
      "\n",
      "iteration # 37\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6898056496576037 \n",
      "\n",
      "iteration # 38\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6897438715974472 \n",
      "\n",
      "iteration # 39\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6905462542351165 \n",
      "\n",
      "iteration # 40\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6912830615569245 \n",
      "\n",
      "iteration # 41\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6918631087615392 \n",
      "\n",
      "iteration # 42\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6934912905077233 \n",
      "\n",
      "iteration # 43\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.693711951308978 \n",
      "\n",
      "iteration # 44\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6958149246064053 \n",
      "\n",
      "iteration # 45\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6972185528117063 \n",
      "\n",
      "iteration # 46\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6975947035824985 \n",
      "\n",
      "iteration # 47\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.698940373180502 \n",
      "\n",
      "iteration # 48\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6986345351389495 \n",
      "\n",
      "iteration # 49\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6989751530249856 \n",
      "\n",
      "iteration # 50\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6995456121547 \n",
      "\n",
      "iteration # 51\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7004092700147384 \n",
      "\n",
      "iteration # 52\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7011324757385765 \n",
      "\n",
      "iteration # 53\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7025930293964312 \n",
      "\n",
      "iteration # 54\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7071179931674993 \n",
      "\n",
      "iteration # 55\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7024787572985812 \n",
      "\n",
      "iteration # 56\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7015595129502588 \n",
      "\n",
      "iteration # 57\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7022961255657822 \n",
      "\n",
      "iteration # 58\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7019947993796789 \n",
      "\n",
      "iteration # 59\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7032068376973705 \n",
      "\n",
      "iteration # 60\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7025856842602937 \n",
      "\n",
      "iteration # 61\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7009409220101457 \n",
      "\n",
      "iteration # 62\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.698437809612815 \n",
      "\n",
      "iteration # 63\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.7002288714081286 \n",
      "\n",
      "iteration # 64\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6971687658658067 \n",
      "\n",
      "iteration # 65\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6970342912238039 \n",
      "\n",
      "iteration # 66\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6967438444530142 \n",
      "\n",
      "iteration # 67\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6964032414859586 \n",
      "\n",
      "iteration # 68\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.697978091077441 \n",
      "\n",
      "iteration # 69\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.69897279519715 \n",
      "\n",
      "iteration # 70\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6999463262289942 \n",
      "\n",
      "iteration # 71\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6985234639176595 \n",
      "\n",
      "iteration # 72\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6971036754335462 \n",
      "\n",
      "iteration # 73\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6956582028142116 \n",
      "\n",
      "iteration # 74\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6960833873847052 \n",
      "\n",
      "iteration # 75\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6951974675233232 \n",
      "\n",
      "iteration # 76\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6950235835495749 \n",
      "\n",
      "iteration # 77\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6954111990421895 \n",
      "\n",
      "iteration # 78\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6937877510972615 \n",
      "\n",
      "iteration # 79\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6935011009996886 \n",
      "\n",
      "iteration # 80\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6944685563884849 \n",
      "\n",
      "iteration # 81\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6950063186596549 \n",
      "\n",
      "iteration # 82\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6925636536720603 \n",
      "\n",
      "iteration # 83\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6909557285844624 \n",
      "\n",
      "iteration # 84\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6921577993111934 \n",
      "\n",
      "iteration # 85\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6921546894349878 \n",
      "\n",
      "iteration # 86\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6923832353530068 \n",
      "\n",
      "iteration # 87\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6922419428032947 \n",
      "\n",
      "iteration # 88\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6939256569556405 \n",
      "\n",
      "iteration # 89\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6939167284938526 \n",
      "\n",
      "iteration # 90\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6950732889753335 \n",
      "\n",
      "iteration # 91\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6933266001545999 \n",
      "\n",
      "iteration # 92\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6940604539032557 \n",
      "\n",
      "iteration # 93\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6935581803977833 \n",
      "\n",
      "iteration # 94\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.693395601356879 \n",
      "\n",
      "iteration # 95\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.693201280791337 \n",
      "\n",
      "iteration # 96\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6920382609839879 \n",
      "\n",
      "iteration # 97\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6911777094668612 \n",
      "\n",
      "iteration # 98\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6907882364925394 \n",
      "\n",
      "iteration # 99\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.691442997092596 \n",
      "\n",
      "iteration # 100\n",
      "accuracy:  0.5438770899231812\n",
      "Cost: \n",
      " 0.6918954458594961 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "bnn=BayesianNeuralNetwork(x_train,y_train)\n",
    "bnn.fit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.6926265915445428\n",
      "Acc : 0.5366470854044284\n"
     ]
    }
   ],
   "source": [
    "bnn_test=bnn.evaluate(x_cv,y_cv.reshape(-1,1))\n",
    "print('Loss :',bnn_test[0])\n",
    "print('Acc :',bnn_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.6916558352708011\n",
      "Acc : 0.540081337550836\n"
     ]
    }
   ],
   "source": [
    "bnn_test_1=bnn.evaluate(x_test,y_test.reshape(-1,1))\n",
    "print('Loss :',bnn_test_1[0])\n",
    "print('Acc :',bnn_test_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
